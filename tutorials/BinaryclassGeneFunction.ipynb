{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] ME(1427609:140538876626752,MainProcess):2024-12-16-22:39:06.223.265 [mindspore/run_check/_check_version.py:230] Cuda ['10.1', '11.1', '11.6'] version(libcudart*.so need by mindspore-gpu) is not found. Please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches. Please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[ERROR] ME(1427609:140538876626752,MainProcess):2024-12-16-22:39:06.305.193 [mindspore/run_check/_check_version.py:230] Cuda ['10.1', '11.1', '11.6'] version(libcudnn*.so need by mindspore-gpu) is not found. Please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches. Please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(1427609:140538876626752,MainProcess):2024-12-16-22:39:06.307.155 [mindspore/run_check/_check_version.py:98] Can not found cuda libs. Please confirm that the correct cuda version has been installed. Refer to the installation guidelines: https://www.mindspore.cn/install\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import pickle as pk\n",
    "import mindspore as ms\n",
    "import mindspore.numpy as mnp\n",
    "import mindspore.scipy as msc\n",
    "import mindspore.dataset as ds\n",
    "from tqdm import tqdm,trange\n",
    "from mindspore import nn,ops\n",
    "from scipy.sparse import csr_matrix as csr\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.amp import FixedLossScaleManager,all_finite,DynamicLossScaleManager\n",
    "from mindspore.train import Model, CheckpointConfig, ModelCheckpoint, LossMonitor, Accuracy\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.communication import init, get_rank, get_group_size\n",
    "from mindspore.parallel._utils import _get_parallel_mode\n",
    "from mindspore.common.initializer import initializer, XavierNormal\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import Config\n",
    "from metrics import annote_metric\n",
    "\n",
    "\n",
    "from utils import Wrapper\n",
    "import warnings  \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_module(module,filter_tag=[None]):\n",
    "    for param in module.trainable_params():\n",
    "        x=False\n",
    "        for tag in filter_tag:\n",
    "            if tag and tag in param.name:\n",
    "                x=True\n",
    "                break\n",
    "        param.requires_grad = x\n",
    "        \n",
    "class MLP(nn.Cell):\n",
    "    def __init__(self,gene_emb,cfg,shard=None):\n",
    "        super().__init__()\n",
    "        self.depth=cfg.enc_nlayers\n",
    "        self.gene_emb=ms.Parameter(ms.Tensor(gene_emb))\n",
    "        emb_dims = gene_emb.shape[-1]\n",
    "        self.gene_emb.requires_grad=False\n",
    "        self.mlp=nn.SequentialCell(\n",
    "            nn.Dense(emb_dims,emb_dims//2,has_bias=False),\n",
    "            nn.Dropout(p=0.15),\n",
    "            nn.SiLU(),\n",
    "            nn.Dense(emb_dims//2,emb_dims//4,has_bias=False),\n",
    "            nn.Dropout(p=0.15),\n",
    "            nn.SiLU(),\n",
    "            nn.Dense(emb_dims//4,2,has_bias=False),\n",
    "        )\n",
    "        self.gather=P.Gather()\n",
    "        self.logsoftmax=P.LogSoftmax(-1)\n",
    "        self.nll_loss=nn.NLLLoss()\n",
    "    def construct(self,gene_id,label):\n",
    "        gene_emb=self.gather(self.gene_emb,gene_id,0).astype(ms.float32)\n",
    "        func_pred=self.mlp(gene_emb)\n",
    "        loss=self.nll_loss(self.logsoftmax(func_pred),label)\n",
    "        if self.training:\n",
    "            return loss\n",
    "        else:\n",
    "            return loss,func_pred,label\n",
    "\n",
    "class SCrna():\n",
    "    def __init__(self,path,data,fold,mode,gene_index):\n",
    "        self.mode=mode\n",
    "        adata=sc.read_h5ad(f'{path}/t123.h5ad')\n",
    "\n",
    "     \n",
    "        common_gene=np.intersect1d(list(adata.var_names), list(gene_index))\n",
    "\n",
    "        self.adata=adata[:,common_gene].copy()\n",
    "        gene=self.adata.var[self.adata.var[f'train_{data}']>-1]\n",
    "        \n",
    "        idx=gene[f'train_{data}']==fold\n",
    "    \n",
    "        self.geneset={j:i+1 for i,j in enumerate(gene_index)}\n",
    "\n",
    "        if mode=='train':\n",
    "            self.gene=np.array([self.geneset[i] for i in gene[~idx].index]).astype(np.int32)\n",
    "            self.label=gene[f'{data}'][~idx].values\n",
    "        else:\n",
    "            self.gene=np.array([self.geneset[i] for i in gene[idx].index]).astype(np.int32)\n",
    "            self.label=gene[f'{data}'][idx].values\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.gene)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.gene[idx],self.label[idx]\n",
    "\n",
    "def build_dataset(\n",
    "    data,batch,\n",
    "    mask_rate=0.2,\n",
    "    drop=True,\n",
    "    shuffle=True,\n",
    "    rank_size=None,\n",
    "    rank_id=None,\n",
    "):\n",
    "    dataset = ds.GeneratorDataset(\n",
    "        data, \n",
    "        column_names=[\"gene\",'label'],\n",
    "        shuffle=shuffle,\n",
    "        num_shards=rank_size, \n",
    "        shard_id=rank_id\n",
    "    )\n",
    "    dataset = dataset.batch(\n",
    "        batch,\n",
    "        num_parallel_workers=4, \n",
    "        drop_remainder=drop, \n",
    "    )\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['id'] = 3\n",
    "args['epoch'] = 30\n",
    "args['batch'] = 4\n",
    "args['fold'] = 2\n",
    "args['fp16'] = False\n",
    "args['data'] = \"t1\"\n",
    "args['datapath'] = \"../datasets/genefunction/\"\n",
    "args['readpath'] = '../'\n",
    "args['savepath'] = '../checkpoint/genefunction/'\n",
    "args['lr'] = 1e-4\n",
    "class config:\n",
    "    def __init__(self, args):\n",
    "        self.id = args['id']\n",
    "        self.epoch = args['epoch']\n",
    "        self.batch = args['batch']\n",
    "        self.fold = args['fold']\n",
    "        self.fp16 = args['fp16']\n",
    "        self.data = args['data']\n",
    "        self.datapath = args['datapath'] \n",
    "        self.readpath = args['readpath'] \n",
    "        self.savepath = args['savepath'] \n",
    "        self.lr = args['lr']\n",
    "\n",
    "args = config(args)\n",
    "\n",
    "ms.set_context(\n",
    "    device_target='GPU', \n",
    "    mode=ms.GRAPH_MODE,\n",
    "    device_id=args.id,\n",
    ")\n",
    "cfg=Config()\n",
    "rank_id = None\n",
    "rank_size = None\n",
    "ms.set_seed(0)\n",
    "shard=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = args.datapath\n",
    "\n",
    "gene_index = pd.read_csv(f'../../csv/gene_info.csv',index_col=0,header=0).index\n",
    "gene_emb = ms.load_checkpoint(\"../checkpoint/base_weight.ckpt\")['gene_emb'].value()\n",
    "scrna=SCrna(path,args.data,args.fold,mode='train',gene_index=gene_index)\n",
    "trainset=build_dataset(\n",
    "    scrna,args.batch,\n",
    "    rank_size=rank_size,\n",
    "    rank_id=rank_id,\n",
    "    drop=True\n",
    ")\n",
    "sctest=SCrna(path,args.data,args.fold,mode='test',gene_index=gene_index)\n",
    "testset=build_dataset(\n",
    "    sctest,\n",
    "    len(sctest),\n",
    "    rank_size=rank_size,\n",
    "    rank_id=rank_id,\n",
    "    drop=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 0.6931521892547607\n",
      "epoch: 1 step: 2, loss is 0.6931703090667725\n",
      "epoch: 1 step: 3, loss is 0.69302898645401\n",
      "epoch: 1 step: 4, loss is 0.6930884122848511\n",
      "epoch: 1 step: 5, loss is 0.6930348873138428\n",
      "epoch: 1 step: 6, loss is 0.6929506659507751\n",
      "epoch: 1 step: 7, loss is 0.6930937767028809\n",
      "epoch: 1 step: 8, loss is 0.6928531527519226\n",
      "epoch: 1 step: 9, loss is 0.6927726864814758\n",
      "epoch: 1 step: 10, loss is 0.693138837814331\n",
      "epoch: 1 step: 11, loss is 0.6926484107971191\n",
      "epoch: 1 step: 12, loss is 0.6928457021713257\n",
      "epoch: 1 step: 13, loss is 0.6926879286766052\n",
      "epoch: 1 step: 14, loss is 0.693278431892395\n",
      "epoch: 1 step: 15, loss is 0.6921830773353577\n",
      "epoch: 1 step: 16, loss is 0.6922993659973145\n",
      "epoch: 1 step: 17, loss is 0.692756175994873\n",
      "epoch: 1 step: 18, loss is 0.6924155950546265\n",
      "epoch: 1 step: 19, loss is 0.6921775341033936\n",
      "epoch: 1 step: 20, loss is 0.6924899816513062\n",
      "epoch: 1 step: 21, loss is 0.6919459104537964\n",
      "epoch: 1 step: 22, loss is 0.6922548413276672\n",
      "epoch: 1 step: 23, loss is 0.6926783919334412\n",
      "epoch: 1 step: 24, loss is 0.6920393109321594\n",
      "epoch: 1 step: 25, loss is 0.691979169845581\n",
      "epoch: 1 step: 26, loss is 0.6913694143295288\n",
      "epoch: 1 step: 27, loss is 0.6916674971580505\n",
      "epoch: 1 step: 28, loss is 0.6914777755737305\n",
      "epoch: 1 step: 29, loss is 0.6938604712486267\n",
      "epoch: 1 step: 30, loss is 0.69026780128479\n",
      "epoch: 1 step: 31, loss is 0.691576361656189\n",
      "epoch: 1 step: 32, loss is 0.6927143335342407\n",
      "epoch: 1 step: 33, loss is 0.6908726692199707\n",
      "epoch: 1 step: 34, loss is 0.6911109685897827\n",
      "epoch: 1 step: 35, loss is 0.6914981007575989\n",
      "epoch: 1 step: 36, loss is 0.6912407279014587\n",
      "epoch: 1 step: 37, loss is 0.6914416551589966\n",
      "epoch: 1 step: 38, loss is 0.6885305047035217\n",
      "epoch: 1 step: 39, loss is 0.6928460001945496\n",
      "epoch: 1 step: 40, loss is 0.6896617412567139\n",
      "epoch: 1 step: 41, loss is 0.6925154328346252\n",
      "epoch: 1 step: 42, loss is 0.6868000030517578\n",
      "epoch: 1 step: 43, loss is 0.6880227327346802\n",
      "epoch: 1 step: 44, loss is 0.6918091773986816\n",
      "epoch: 1 step: 45, loss is 0.6887274384498596\n",
      "epoch: 1 step: 46, loss is 0.6916971206665039\n",
      "epoch: 1 step: 47, loss is 0.6933781504631042\n",
      "epoch: 1 step: 48, loss is 0.684359073638916\n",
      "epoch: 1 step: 49, loss is 0.6859396696090698\n",
      "epoch: 1 step: 50, loss is 0.6862837672233582\n",
      "epoch: 1 step: 51, loss is 0.6920728087425232\n",
      "epoch: 1 step: 52, loss is 0.6921020150184631\n",
      "epoch: 1 step: 53, loss is 0.683100163936615\n",
      "epoch: 1 step: 54, loss is 0.682022213935852\n",
      "epoch: 1 step: 55, loss is 0.6826848983764648\n",
      "epoch: 1 step: 56, loss is 0.6854700446128845\n",
      "epoch: 1 step: 57, loss is 0.6834270358085632\n",
      "epoch: 1 step: 58, loss is 0.6813344955444336\n",
      "epoch: 1 step: 59, loss is 0.6858443021774292\n",
      "epoch: 1 step: 60, loss is 0.6883701086044312\n",
      "epoch: 1 step: 61, loss is 0.6792470812797546\n",
      "epoch: 1 step: 62, loss is 0.6785852313041687\n",
      "epoch: 1 step: 63, loss is 0.6849226951599121\n",
      "epoch: 1 step: 64, loss is 0.6901788115501404\n",
      "epoch: 1 step: 65, loss is 0.6926425099372864\n",
      "epoch: 1 step: 66, loss is 0.6965652108192444\n",
      "epoch: 1 step: 67, loss is 0.673714280128479\n",
      "epoch: 1 step: 68, loss is 0.6711813807487488\n",
      "epoch: 1 step: 69, loss is 0.683169960975647\n",
      "epoch: 1 step: 70, loss is 0.677936315536499\n",
      "epoch: 1 step: 71, loss is 0.6698504686355591\n",
      "epoch: 1 step: 72, loss is 0.6757514476776123\n",
      "epoch: 1 step: 73, loss is 0.6708193421363831\n",
      "epoch: 1 step: 74, loss is 0.6642711162567139\n",
      "epoch: 1 step: 75, loss is 0.6935132145881653\n",
      "epoch: 1 step: 76, loss is 0.6712051630020142\n",
      "epoch: 1 step: 77, loss is 0.6866029500961304\n",
      "epoch: 1 step: 78, loss is 0.6712639927864075\n",
      "epoch: 1 step: 79, loss is 0.6755090355873108\n",
      "epoch: 1 step: 80, loss is 0.6719632148742676\n",
      "epoch: 1 step: 81, loss is 0.6672377586364746\n",
      "epoch: 1 step: 82, loss is 0.6592153310775757\n",
      "epoch: 1 step: 83, loss is 0.6581519842147827\n",
      "epoch: 1 step: 84, loss is 0.6692860126495361\n",
      "{'accuracy': 0.7764705882352941, 'macro f1': 0.5817145817145817, 'weighted f1': 0.719386931151637, 'macro recall': 0.5829725829725829, 'macro precision': 0.7875000000000001}\n",
      "Eval result: epoch 1, metrics: {'metrics': 0.6821644382306147}\n",
      "epoch: 2 step: 1, loss is 0.6761049628257751\n",
      "epoch: 2 step: 2, loss is 0.693923830986023\n",
      "epoch: 2 step: 3, loss is 0.6442458033561707\n",
      "epoch: 2 step: 4, loss is 0.642516016960144\n",
      "epoch: 2 step: 5, loss is 0.6915878057479858\n",
      "epoch: 2 step: 6, loss is 0.6564520597457886\n",
      "epoch: 2 step: 7, loss is 0.6333945393562317\n",
      "epoch: 2 step: 8, loss is 0.6749002933502197\n",
      "epoch: 2 step: 9, loss is 0.6502658724784851\n",
      "epoch: 2 step: 10, loss is 0.650124192237854\n",
      "epoch: 2 step: 11, loss is 0.6538745164871216\n",
      "epoch: 2 step: 12, loss is 0.6586717367172241\n",
      "epoch: 2 step: 13, loss is 0.6495379209518433\n",
      "epoch: 2 step: 14, loss is 0.6688891053199768\n",
      "epoch: 2 step: 15, loss is 0.6303951740264893\n",
      "epoch: 2 step: 16, loss is 0.6634231209754944\n",
      "epoch: 2 step: 17, loss is 0.6252498626708984\n",
      "epoch: 2 step: 18, loss is 0.6504008769989014\n",
      "epoch: 2 step: 19, loss is 0.6469240784645081\n",
      "epoch: 2 step: 20, loss is 0.7041280269622803\n",
      "epoch: 2 step: 21, loss is 0.6194566488265991\n",
      "epoch: 2 step: 22, loss is 0.6870725154876709\n",
      "epoch: 2 step: 23, loss is 0.6446214318275452\n",
      "epoch: 2 step: 24, loss is 0.5909430980682373\n",
      "epoch: 2 step: 25, loss is 0.64000403881073\n",
      "epoch: 2 step: 26, loss is 0.6152905821800232\n",
      "epoch: 2 step: 27, loss is 0.6808171272277832\n",
      "epoch: 2 step: 28, loss is 0.7224270105361938\n",
      "epoch: 2 step: 29, loss is 0.632889449596405\n",
      "epoch: 2 step: 30, loss is 0.6145951151847839\n",
      "epoch: 2 step: 31, loss is 0.5933339595794678\n",
      "epoch: 2 step: 32, loss is 0.5783703327178955\n",
      "epoch: 2 step: 33, loss is 0.5994479656219482\n",
      "epoch: 2 step: 34, loss is 0.6062970161437988\n",
      "epoch: 2 step: 35, loss is 0.5990204811096191\n",
      "epoch: 2 step: 36, loss is 0.648155152797699\n",
      "epoch: 2 step: 37, loss is 0.5631178617477417\n",
      "epoch: 2 step: 38, loss is 0.5769118070602417\n",
      "epoch: 2 step: 39, loss is 0.5417014956474304\n",
      "epoch: 2 step: 40, loss is 0.6048154234886169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(1427609,7fd1c1d18740,python):2024-12-16-22:39:46.857.623 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1427609/2429778555.py]\n",
      "[ERROR] CORE(1427609,7fd1c1d18740,python):2024-12-16-22:39:46.857.665 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1427609/2429778555.py]\n",
      "[ERROR] CORE(1427609,7fd1c1d18740,python):2024-12-16-22:39:46.857.678 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1427609/2429778555.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 41, loss is 0.6324800848960876\n",
      "epoch: 2 step: 42, loss is 0.5745666027069092\n",
      "epoch: 2 step: 43, loss is 0.6076438426971436\n",
      "epoch: 2 step: 44, loss is 0.5794374942779541\n",
      "epoch: 2 step: 45, loss is 0.5442765951156616\n",
      "epoch: 2 step: 46, loss is 0.5221415758132935\n",
      "epoch: 2 step: 47, loss is 0.6672753095626831\n",
      "epoch: 2 step: 48, loss is 0.6252565383911133\n",
      "epoch: 2 step: 49, loss is 0.6101400256156921\n",
      "epoch: 2 step: 50, loss is 0.5616727471351624\n",
      "epoch: 2 step: 51, loss is 0.5852972865104675\n",
      "epoch: 2 step: 52, loss is 0.4993596076965332\n",
      "epoch: 2 step: 53, loss is 0.4719761610031128\n",
      "epoch: 2 step: 54, loss is 0.6142653822898865\n",
      "epoch: 2 step: 55, loss is 0.7408678531646729\n",
      "epoch: 2 step: 56, loss is 0.5031419992446899\n",
      "epoch: 2 step: 57, loss is 0.5690963864326477\n",
      "epoch: 2 step: 58, loss is 0.5873899459838867\n",
      "epoch: 2 step: 59, loss is 0.6310907602310181\n",
      "epoch: 2 step: 60, loss is 0.4738936424255371\n",
      "epoch: 2 step: 61, loss is 0.6972630620002747\n",
      "epoch: 2 step: 62, loss is 0.6151900291442871\n",
      "epoch: 2 step: 63, loss is 0.7274397611618042\n",
      "epoch: 2 step: 64, loss is 0.606048047542572\n",
      "epoch: 2 step: 65, loss is 0.5798851847648621\n",
      "epoch: 2 step: 66, loss is 0.6528359651565552\n",
      "epoch: 2 step: 67, loss is 0.4611310660839081\n",
      "epoch: 2 step: 68, loss is 0.6044735908508301\n",
      "epoch: 2 step: 69, loss is 0.6118583083152771\n",
      "epoch: 2 step: 70, loss is 0.4538438022136688\n",
      "epoch: 2 step: 71, loss is 0.4909643232822418\n",
      "epoch: 2 step: 72, loss is 0.5762173533439636\n",
      "epoch: 2 step: 73, loss is 0.4270183742046356\n",
      "epoch: 2 step: 74, loss is 0.5223857760429382\n",
      "epoch: 2 step: 75, loss is 0.5596689581871033\n",
      "epoch: 2 step: 76, loss is 0.4543008506298065\n",
      "epoch: 2 step: 77, loss is 0.4435155391693115\n",
      "epoch: 2 step: 78, loss is 0.5530326962471008\n",
      "epoch: 2 step: 79, loss is 0.507559061050415\n",
      "epoch: 2 step: 80, loss is 0.5356792211532593\n",
      "epoch: 2 step: 81, loss is 0.4304209053516388\n",
      "epoch: 2 step: 82, loss is 0.539091944694519\n",
      "epoch: 2 step: 83, loss is 0.6170392036437988\n",
      "epoch: 2 step: 84, loss is 0.40912795066833496\n",
      "{'accuracy': 0.7764705882352941, 'macro f1': 0.5817145817145817, 'weighted f1': 0.719386931151637, 'macro recall': 0.5829725829725829, 'macro precision': 0.7875000000000001}\n",
      "Eval result: epoch 2, metrics: {'metrics': 0.6821644382306147}\n",
      "epoch: 3 step: 1, loss is 0.649667501449585\n",
      "epoch: 3 step: 2, loss is 0.5631792545318604\n",
      "epoch: 3 step: 3, loss is 0.35949838161468506\n",
      "epoch: 3 step: 4, loss is 0.40190938115119934\n",
      "epoch: 3 step: 5, loss is 0.3868623375892639\n",
      "epoch: 3 step: 6, loss is 0.3674299120903015\n",
      "epoch: 3 step: 7, loss is 0.6980328559875488\n",
      "epoch: 3 step: 8, loss is 0.36117637157440186\n",
      "epoch: 3 step: 9, loss is 0.8958489894866943\n",
      "epoch: 3 step: 10, loss is 0.5030636191368103\n",
      "epoch: 3 step: 11, loss is 0.40846920013427734\n",
      "epoch: 3 step: 12, loss is 0.33553898334503174\n",
      "epoch: 3 step: 13, loss is 0.3524709641933441\n",
      "epoch: 3 step: 14, loss is 0.3165616989135742\n",
      "epoch: 3 step: 15, loss is 0.36440616846084595\n",
      "epoch: 3 step: 16, loss is 0.5799030661582947\n",
      "epoch: 3 step: 17, loss is 0.4627411365509033\n",
      "epoch: 3 step: 18, loss is 0.5226746797561646\n",
      "epoch: 3 step: 19, loss is 0.3107288181781769\n",
      "epoch: 3 step: 20, loss is 0.3307519853115082\n",
      "epoch: 3 step: 21, loss is 0.6114649772644043\n",
      "epoch: 3 step: 22, loss is 0.6827982068061829\n",
      "epoch: 3 step: 23, loss is 0.6350692510604858\n",
      "epoch: 3 step: 24, loss is 0.5930576324462891\n",
      "epoch: 3 step: 25, loss is 0.6121903657913208\n",
      "epoch: 3 step: 26, loss is 0.41651448607444763\n",
      "epoch: 3 step: 27, loss is 0.7951508164405823\n",
      "epoch: 3 step: 28, loss is 0.5164092183113098\n",
      "epoch: 3 step: 29, loss is 0.21937550604343414\n",
      "epoch: 3 step: 30, loss is 0.4913010597229004\n",
      "epoch: 3 step: 31, loss is 0.5399475693702698\n",
      "epoch: 3 step: 32, loss is 0.33054882287979126\n",
      "epoch: 3 step: 33, loss is 0.5630207061767578\n",
      "epoch: 3 step: 34, loss is 0.28446871042251587\n",
      "epoch: 3 step: 35, loss is 0.5035583972930908\n",
      "epoch: 3 step: 36, loss is 0.5545433163642883\n",
      "epoch: 3 step: 37, loss is 0.7009053826332092\n",
      "epoch: 3 step: 38, loss is 0.40175387263298035\n",
      "epoch: 3 step: 39, loss is 0.6101605296134949\n",
      "epoch: 3 step: 40, loss is 0.3063596487045288\n",
      "epoch: 3 step: 41, loss is 0.32252246141433716\n",
      "epoch: 3 step: 42, loss is 0.476845383644104\n",
      "epoch: 3 step: 43, loss is 0.5631518959999084\n",
      "epoch: 3 step: 44, loss is 0.31924599409103394\n",
      "epoch: 3 step: 45, loss is 0.2741474509239197\n",
      "epoch: 3 step: 46, loss is 0.5798266530036926\n",
      "epoch: 3 step: 47, loss is 0.37584352493286133\n",
      "epoch: 3 step: 48, loss is 0.20387019217014313\n",
      "epoch: 3 step: 49, loss is 0.8331936597824097\n",
      "epoch: 3 step: 50, loss is 0.23216396570205688\n",
      "epoch: 3 step: 51, loss is 0.5712659358978271\n",
      "epoch: 3 step: 52, loss is 0.4004702568054199\n",
      "epoch: 3 step: 53, loss is 0.27914494276046753\n",
      "epoch: 3 step: 54, loss is 0.7806649804115295\n",
      "epoch: 3 step: 55, loss is 0.5375545024871826\n",
      "epoch: 3 step: 56, loss is 0.30747485160827637\n",
      "epoch: 3 step: 57, loss is 0.5423333644866943\n",
      "epoch: 3 step: 58, loss is 0.3472091555595398\n",
      "epoch: 3 step: 59, loss is 0.2272946983575821\n",
      "epoch: 3 step: 60, loss is 0.7585921287536621\n",
      "epoch: 3 step: 61, loss is 0.37705695629119873\n",
      "epoch: 3 step: 62, loss is 0.37086522579193115\n",
      "epoch: 3 step: 63, loss is 0.22694718837738037\n",
      "epoch: 3 step: 64, loss is 0.21697203814983368\n",
      "epoch: 3 step: 65, loss is 0.6114380359649658\n",
      "epoch: 3 step: 66, loss is 0.2042524516582489\n",
      "epoch: 3 step: 67, loss is 0.3327261507511139\n",
      "epoch: 3 step: 68, loss is 0.7130833864212036\n",
      "epoch: 3 step: 69, loss is 0.9789244532585144\n",
      "epoch: 3 step: 70, loss is 0.21254409849643707\n",
      "epoch: 3 step: 71, loss is 0.18963545560836792\n",
      "epoch: 3 step: 72, loss is 0.3603104054927826\n",
      "epoch: 3 step: 73, loss is 0.6260603070259094\n",
      "epoch: 3 step: 74, loss is 0.24383573234081268\n",
      "epoch: 3 step: 75, loss is 0.47293007373809814\n",
      "epoch: 3 step: 76, loss is 0.532547652721405\n",
      "epoch: 3 step: 77, loss is 0.2111096829175949\n",
      "epoch: 3 step: 78, loss is 0.22367829084396362\n",
      "epoch: 3 step: 79, loss is 0.6498914957046509\n",
      "epoch: 3 step: 80, loss is 0.7105944156646729\n",
      "epoch: 3 step: 81, loss is 0.27793481945991516\n",
      "epoch: 3 step: 82, loss is 0.4213765263557434\n",
      "epoch: 3 step: 83, loss is 0.25354838371276855\n",
      "epoch: 3 step: 84, loss is 0.7647959589958191\n",
      "{'accuracy': 0.788235294117647, 'macro f1': 0.6151911468812877, 'weighted f1': 0.7396614984021778, 'macro recall': 0.6056998556998556, 'macro precision': 0.8090717299578059}\n",
      "Eval result: epoch 3, metrics: {'metrics': 0.704549506664149}\n",
      "epoch: 4 step: 1, loss is 0.3295289874076843\n",
      "epoch: 4 step: 2, loss is 0.24239614605903625\n",
      "epoch: 4 step: 3, loss is 0.7703670263290405\n",
      "epoch: 4 step: 4, loss is 0.22358456254005432\n",
      "epoch: 4 step: 5, loss is 0.8342048525810242\n",
      "epoch: 4 step: 6, loss is 0.45691362023353577\n",
      "epoch: 4 step: 7, loss is 0.9477008581161499\n",
      "epoch: 4 step: 8, loss is 0.19457218050956726\n",
      "epoch: 4 step: 9, loss is 0.1609189510345459\n",
      "epoch: 4 step: 10, loss is 0.1801142692565918\n",
      "epoch: 4 step: 11, loss is 0.14334289729595184\n",
      "epoch: 4 step: 12, loss is 0.15562015771865845\n",
      "epoch: 4 step: 13, loss is 0.19115231931209564\n",
      "epoch: 4 step: 14, loss is 0.899924635887146\n",
      "epoch: 4 step: 15, loss is 0.23154288530349731\n",
      "epoch: 4 step: 16, loss is 0.44140860438346863\n",
      "epoch: 4 step: 17, loss is 0.3522339463233948\n",
      "epoch: 4 step: 18, loss is 0.35097676515579224\n",
      "epoch: 4 step: 19, loss is 0.32283738255500793\n",
      "epoch: 4 step: 20, loss is 0.7690294981002808\n",
      "epoch: 4 step: 21, loss is 0.3481544852256775\n",
      "epoch: 4 step: 22, loss is 0.5989706516265869\n",
      "epoch: 4 step: 23, loss is 0.15593138337135315\n",
      "epoch: 4 step: 24, loss is 0.12710566818714142\n",
      "epoch: 4 step: 25, loss is 0.1920674443244934\n",
      "epoch: 4 step: 26, loss is 0.6081775426864624\n",
      "epoch: 4 step: 27, loss is 0.17036612331867218\n",
      "epoch: 4 step: 28, loss is 0.2929709255695343\n",
      "epoch: 4 step: 29, loss is 0.17148490250110626\n",
      "epoch: 4 step: 30, loss is 0.1857881247997284\n",
      "epoch: 4 step: 31, loss is 0.5496876239776611\n",
      "epoch: 4 step: 32, loss is 0.9190561771392822\n",
      "epoch: 4 step: 33, loss is 0.31968003511428833\n",
      "epoch: 4 step: 34, loss is 0.20366136729717255\n",
      "epoch: 4 step: 35, loss is 0.4682561755180359\n",
      "epoch: 4 step: 36, loss is 0.9294958114624023\n",
      "epoch: 4 step: 37, loss is 0.28115296363830566\n",
      "epoch: 4 step: 38, loss is 0.2081080675125122\n",
      "epoch: 4 step: 39, loss is 0.47811007499694824\n",
      "epoch: 4 step: 40, loss is 0.15462501347064972\n",
      "epoch: 4 step: 41, loss is 0.16068653762340546\n",
      "epoch: 4 step: 42, loss is 1.2152459621429443\n",
      "epoch: 4 step: 43, loss is 0.2700657248497009\n",
      "epoch: 4 step: 44, loss is 0.40316957235336304\n",
      "epoch: 4 step: 45, loss is 0.7567213773727417\n",
      "epoch: 4 step: 46, loss is 0.40962907671928406\n",
      "epoch: 4 step: 47, loss is 0.6014946103096008\n",
      "epoch: 4 step: 48, loss is 0.14296358823776245\n",
      "epoch: 4 step: 49, loss is 0.12084200233221054\n",
      "epoch: 4 step: 50, loss is 0.38117852807044983\n",
      "epoch: 4 step: 51, loss is 0.4554313123226166\n",
      "epoch: 4 step: 52, loss is 0.6247736811637878\n",
      "epoch: 4 step: 53, loss is 0.5635175704956055\n",
      "epoch: 4 step: 54, loss is 0.6053665280342102\n",
      "epoch: 4 step: 55, loss is 0.746662437915802\n",
      "epoch: 4 step: 56, loss is 0.2298441380262375\n",
      "epoch: 4 step: 57, loss is 0.3147153854370117\n",
      "epoch: 4 step: 58, loss is 0.452382355928421\n",
      "epoch: 4 step: 59, loss is 0.29277709126472473\n",
      "epoch: 4 step: 60, loss is 0.7418525815010071\n",
      "epoch: 4 step: 61, loss is 0.32771095633506775\n",
      "epoch: 4 step: 62, loss is 0.8519594669342041\n",
      "epoch: 4 step: 63, loss is 0.5223431587219238\n",
      "epoch: 4 step: 64, loss is 0.4469379782676697\n",
      "epoch: 4 step: 65, loss is 0.5292442440986633\n",
      "epoch: 4 step: 66, loss is 0.1917368471622467\n",
      "epoch: 4 step: 67, loss is 0.1466963291168213\n",
      "epoch: 4 step: 68, loss is 0.19469872117042542\n",
      "epoch: 4 step: 69, loss is 0.1744462102651596\n",
      "epoch: 4 step: 70, loss is 0.40257012844085693\n",
      "epoch: 4 step: 71, loss is 0.6053037047386169\n",
      "epoch: 4 step: 72, loss is 0.27865955233573914\n",
      "epoch: 4 step: 73, loss is 0.5295134782791138\n",
      "epoch: 4 step: 74, loss is 0.3140001595020294\n",
      "epoch: 4 step: 75, loss is 0.1641400307416916\n",
      "epoch: 4 step: 76, loss is 0.18866434693336487\n",
      "epoch: 4 step: 77, loss is 0.8646448850631714\n",
      "epoch: 4 step: 78, loss is 0.5276645421981812\n",
      "epoch: 4 step: 79, loss is 0.24720939993858337\n",
      "epoch: 4 step: 80, loss is 0.4264077842235565\n",
      "epoch: 4 step: 81, loss is 0.17897647619247437\n",
      "epoch: 4 step: 82, loss is 0.6275410652160645\n",
      "epoch: 4 step: 83, loss is 0.6701717972755432\n",
      "epoch: 4 step: 84, loss is 0.14937004446983337\n",
      "{'accuracy': 0.8, 'macro f1': 0.6466128637808756, 'weighted f1': 0.758914159941306, 'macro recall': 0.6284271284271283, 'macro precision': 0.826007326007326}\n",
      "Eval result: epoch 4, metrics: {'metrics': 0.7252618295538324}\n",
      "epoch: 5 step: 1, loss is 0.17293718457221985\n",
      "epoch: 5 step: 2, loss is 0.5270661115646362\n",
      "epoch: 5 step: 3, loss is 0.926858127117157\n",
      "epoch: 5 step: 4, loss is 0.6292231678962708\n",
      "epoch: 5 step: 5, loss is 0.4458775222301483\n",
      "epoch: 5 step: 6, loss is 0.22473210096359253\n",
      "epoch: 5 step: 7, loss is 0.19259124994277954\n",
      "epoch: 5 step: 8, loss is 0.5344303846359253\n",
      "epoch: 5 step: 9, loss is 0.17292994260787964\n",
      "epoch: 5 step: 10, loss is 0.5045452117919922\n",
      "epoch: 5 step: 11, loss is 0.69957435131073\n",
      "epoch: 5 step: 12, loss is 0.6805018782615662\n",
      "epoch: 5 step: 13, loss is 0.4724540710449219\n",
      "epoch: 5 step: 14, loss is 0.2997601628303528\n",
      "epoch: 5 step: 15, loss is 0.21061784029006958\n",
      "epoch: 5 step: 16, loss is 0.1115526407957077\n",
      "epoch: 5 step: 17, loss is 0.4210723638534546\n",
      "epoch: 5 step: 18, loss is 0.1903194785118103\n",
      "epoch: 5 step: 19, loss is 0.4301813244819641\n",
      "epoch: 5 step: 20, loss is 0.184716135263443\n",
      "epoch: 5 step: 21, loss is 0.2897801399230957\n",
      "epoch: 5 step: 22, loss is 0.5705196857452393\n",
      "epoch: 5 step: 23, loss is 0.21792782843112946\n",
      "epoch: 5 step: 24, loss is 0.8075020909309387\n",
      "epoch: 5 step: 25, loss is 0.19691696763038635\n",
      "epoch: 5 step: 26, loss is 0.48889875411987305\n",
      "epoch: 5 step: 27, loss is 0.1749081015586853\n",
      "epoch: 5 step: 28, loss is 0.31996721029281616\n",
      "epoch: 5 step: 29, loss is 0.13568958640098572\n",
      "epoch: 5 step: 30, loss is 0.15045025944709778\n",
      "epoch: 5 step: 31, loss is 0.4023524820804596\n",
      "epoch: 5 step: 32, loss is 0.17812958359718323\n",
      "epoch: 5 step: 33, loss is 0.1516827940940857\n",
      "epoch: 5 step: 34, loss is 0.5952997207641602\n",
      "epoch: 5 step: 35, loss is 0.14757901430130005\n",
      "epoch: 5 step: 36, loss is 0.4176234006881714\n",
      "epoch: 5 step: 37, loss is 0.19434113800525665\n",
      "epoch: 5 step: 38, loss is 0.278023362159729\n",
      "epoch: 5 step: 39, loss is 0.08806881308555603\n",
      "epoch: 5 step: 40, loss is 0.5720563530921936\n",
      "epoch: 5 step: 41, loss is 0.17939263582229614\n",
      "epoch: 5 step: 42, loss is 0.2563744783401489\n",
      "epoch: 5 step: 43, loss is 0.10957890748977661\n",
      "epoch: 5 step: 44, loss is 0.5413728356361389\n",
      "epoch: 5 step: 45, loss is 0.11972301453351974\n",
      "epoch: 5 step: 46, loss is 0.7039326429367065\n",
      "epoch: 5 step: 47, loss is 0.4838297963142395\n",
      "epoch: 5 step: 48, loss is 0.7425175309181213\n",
      "epoch: 5 step: 49, loss is 0.12331157922744751\n",
      "epoch: 5 step: 50, loss is 0.5517568588256836\n",
      "epoch: 5 step: 51, loss is 0.3707878291606903\n",
      "epoch: 5 step: 52, loss is 0.5858272314071655\n",
      "epoch: 5 step: 53, loss is 0.6766412258148193\n",
      "epoch: 5 step: 54, loss is 0.6106350421905518\n",
      "epoch: 5 step: 55, loss is 0.680671215057373\n",
      "epoch: 5 step: 56, loss is 0.12501075863838196\n",
      "epoch: 5 step: 57, loss is 0.10076844692230225\n",
      "epoch: 5 step: 58, loss is 0.1356683075428009\n",
      "epoch: 5 step: 59, loss is 0.9137892723083496\n",
      "epoch: 5 step: 60, loss is 0.4366149604320526\n",
      "epoch: 5 step: 61, loss is 0.15275394916534424\n",
      "epoch: 5 step: 62, loss is 0.42873266339302063\n",
      "epoch: 5 step: 63, loss is 0.667988121509552\n",
      "epoch: 5 step: 64, loss is 1.0641260147094727\n",
      "epoch: 5 step: 65, loss is 0.7277755737304688\n",
      "epoch: 5 step: 66, loss is 0.34645822644233704\n",
      "epoch: 5 step: 67, loss is 0.10755674540996552\n",
      "epoch: 5 step: 68, loss is 0.3231477737426758\n",
      "epoch: 5 step: 69, loss is 0.18387764692306519\n",
      "epoch: 5 step: 70, loss is 0.2133159041404724\n",
      "epoch: 5 step: 71, loss is 0.1427854746580124\n",
      "epoch: 5 step: 72, loss is 0.6439143419265747\n",
      "epoch: 5 step: 73, loss is 0.41927236318588257\n",
      "epoch: 5 step: 74, loss is 0.8702837824821472\n",
      "epoch: 5 step: 75, loss is 0.2091742753982544\n",
      "epoch: 5 step: 76, loss is 0.7595303058624268\n",
      "epoch: 5 step: 77, loss is 0.11771362274885178\n",
      "epoch: 5 step: 78, loss is 0.199672669172287\n",
      "epoch: 5 step: 79, loss is 0.09705809503793716\n",
      "epoch: 5 step: 80, loss is 0.12572051584720612\n",
      "epoch: 5 step: 81, loss is 0.15780335664749146\n",
      "epoch: 5 step: 82, loss is 0.262734591960907\n",
      "epoch: 5 step: 83, loss is 0.6456683874130249\n",
      "epoch: 5 step: 84, loss is 0.8878216743469238\n",
      "{'accuracy': 0.8235294117647058, 'macro f1': 0.7041076815966582, 'weighted f1': 0.7947797359835092, 'macro recall': 0.6738816738816739, 'macro precision': 0.8523391812865497}\n",
      "Eval result: epoch 5, metrics: {'metrics': 0.7634644871323968}\n",
      "epoch: 6 step: 1, loss is 0.254740834236145\n",
      "epoch: 6 step: 2, loss is 1.0802170038223267\n",
      "epoch: 6 step: 3, loss is 0.12005876004695892\n",
      "epoch: 6 step: 4, loss is 0.11600492894649506\n",
      "epoch: 6 step: 5, loss is 0.6654098033905029\n",
      "epoch: 6 step: 6, loss is 0.13232164084911346\n",
      "epoch: 6 step: 7, loss is 0.35610532760620117\n",
      "epoch: 6 step: 8, loss is 0.1589207947254181\n",
      "epoch: 6 step: 9, loss is 0.43567025661468506\n",
      "epoch: 6 step: 10, loss is 0.5331640839576721\n",
      "epoch: 6 step: 11, loss is 0.3925902247428894\n",
      "epoch: 6 step: 12, loss is 0.16890200972557068\n",
      "epoch: 6 step: 13, loss is 0.5939607620239258\n",
      "epoch: 6 step: 14, loss is 0.4315796494483948\n",
      "epoch: 6 step: 15, loss is 0.19979599118232727\n",
      "epoch: 6 step: 16, loss is 0.12641039490699768\n",
      "epoch: 6 step: 17, loss is 0.6908594965934753\n",
      "epoch: 6 step: 18, loss is 0.4182821810245514\n",
      "epoch: 6 step: 19, loss is 0.41001468896865845\n",
      "epoch: 6 step: 20, loss is 0.46980249881744385\n",
      "epoch: 6 step: 21, loss is 0.2552976906299591\n",
      "epoch: 6 step: 22, loss is 0.5162335634231567\n",
      "epoch: 6 step: 23, loss is 0.1120770275592804\n",
      "epoch: 6 step: 24, loss is 0.3469000458717346\n",
      "epoch: 6 step: 25, loss is 0.10817249864339828\n",
      "epoch: 6 step: 26, loss is 0.1843019723892212\n",
      "epoch: 6 step: 27, loss is 0.43944400548934937\n",
      "epoch: 6 step: 28, loss is 0.11387953162193298\n",
      "epoch: 6 step: 29, loss is 0.6431816816329956\n",
      "epoch: 6 step: 30, loss is 0.6716818809509277\n",
      "epoch: 6 step: 31, loss is 0.17257073521614075\n",
      "epoch: 6 step: 32, loss is 0.507645308971405\n",
      "epoch: 6 step: 33, loss is 0.35911184549331665\n",
      "epoch: 6 step: 34, loss is 0.13554039597511292\n",
      "epoch: 6 step: 35, loss is 0.8407259583473206\n",
      "epoch: 6 step: 36, loss is 0.3609216809272766\n",
      "epoch: 6 step: 37, loss is 0.3700447082519531\n",
      "epoch: 6 step: 38, loss is 0.1475668102502823\n",
      "epoch: 6 step: 39, loss is 0.34041082859039307\n",
      "epoch: 6 step: 40, loss is 0.5848524570465088\n",
      "epoch: 6 step: 41, loss is 0.6743125915527344\n",
      "epoch: 6 step: 42, loss is 0.13584309816360474\n",
      "epoch: 6 step: 43, loss is 0.17793621122837067\n",
      "epoch: 6 step: 44, loss is 0.09612957388162613\n",
      "epoch: 6 step: 45, loss is 1.3777973651885986\n",
      "epoch: 6 step: 46, loss is 0.44009217619895935\n",
      "epoch: 6 step: 47, loss is 0.4637162983417511\n",
      "epoch: 6 step: 48, loss is 0.09286037087440491\n",
      "epoch: 6 step: 49, loss is 0.516444981098175\n",
      "epoch: 6 step: 50, loss is 0.5617349147796631\n",
      "epoch: 6 step: 51, loss is 0.7713305950164795\n",
      "epoch: 6 step: 52, loss is 0.3785582184791565\n",
      "epoch: 6 step: 53, loss is 0.5069863796234131\n",
      "epoch: 6 step: 54, loss is 0.09605126082897186\n",
      "epoch: 6 step: 55, loss is 0.520713746547699\n",
      "epoch: 6 step: 56, loss is 0.9837616086006165\n",
      "epoch: 6 step: 57, loss is 0.11166563630104065\n",
      "epoch: 6 step: 58, loss is 0.23990726470947266\n",
      "epoch: 6 step: 59, loss is 0.630913257598877\n",
      "epoch: 6 step: 60, loss is 0.26388034224510193\n",
      "epoch: 6 step: 61, loss is 0.20052655041217804\n",
      "epoch: 6 step: 62, loss is 0.09791325777769089\n",
      "epoch: 6 step: 63, loss is 0.4883485436439514\n",
      "epoch: 6 step: 64, loss is 0.28839001059532166\n",
      "epoch: 6 step: 65, loss is 0.3463481068611145\n",
      "epoch: 6 step: 66, loss is 0.6477869749069214\n",
      "epoch: 6 step: 67, loss is 0.12158094346523285\n",
      "epoch: 6 step: 68, loss is 0.13518644869327545\n",
      "epoch: 6 step: 69, loss is 0.10909131914377213\n",
      "epoch: 6 step: 70, loss is 0.8647463917732239\n",
      "epoch: 6 step: 71, loss is 0.2056177258491516\n",
      "epoch: 6 step: 72, loss is 0.1124095693230629\n",
      "epoch: 6 step: 73, loss is 0.3223542273044586\n",
      "epoch: 6 step: 74, loss is 0.1424950361251831\n",
      "epoch: 6 step: 75, loss is 0.36963197588920593\n",
      "epoch: 6 step: 76, loss is 0.4067034125328064\n",
      "epoch: 6 step: 77, loss is 0.29077398777008057\n",
      "epoch: 6 step: 78, loss is 0.2201879918575287\n",
      "epoch: 6 step: 79, loss is 0.5070410966873169\n",
      "epoch: 6 step: 80, loss is 0.13496965169906616\n",
      "epoch: 6 step: 81, loss is 0.1689176857471466\n",
      "epoch: 6 step: 82, loss is 0.5636689066886902\n",
      "epoch: 6 step: 83, loss is 0.8792569637298584\n",
      "epoch: 6 step: 84, loss is 0.11784803122282028\n",
      "{'accuracy': 0.8470588235294118, 'macro f1': 0.7435599907171038, 'weighted f1': 0.8221424378523747, 'macro recall': 0.7045454545454546, 'macro precision': 0.9144736842105263}\n",
      "Eval result: epoch 6, metrics: {'metrics': 0.8024094882506241}\n",
      "epoch: 7 step: 1, loss is 0.2618279755115509\n",
      "epoch: 7 step: 2, loss is 0.15479984879493713\n",
      "epoch: 7 step: 3, loss is 0.4511183500289917\n",
      "epoch: 7 step: 4, loss is 0.23583367466926575\n",
      "epoch: 7 step: 5, loss is 0.09607331454753876\n",
      "epoch: 7 step: 6, loss is 0.08691269159317017\n",
      "epoch: 7 step: 7, loss is 0.49236157536506653\n",
      "epoch: 7 step: 8, loss is 1.162427306175232\n",
      "epoch: 7 step: 9, loss is 0.3554767966270447\n",
      "epoch: 7 step: 10, loss is 0.19673147797584534\n",
      "epoch: 7 step: 11, loss is 0.18468815088272095\n",
      "epoch: 7 step: 12, loss is 0.7624994516372681\n",
      "epoch: 7 step: 13, loss is 0.1782088577747345\n",
      "epoch: 7 step: 14, loss is 0.17903739213943481\n",
      "epoch: 7 step: 15, loss is 0.12770214676856995\n",
      "epoch: 7 step: 16, loss is 0.4325253665447235\n",
      "epoch: 7 step: 17, loss is 0.3972035050392151\n",
      "epoch: 7 step: 18, loss is 0.341081440448761\n",
      "epoch: 7 step: 19, loss is 0.5172374844551086\n",
      "epoch: 7 step: 20, loss is 0.5670119524002075\n",
      "epoch: 7 step: 21, loss is 0.5750375390052795\n",
      "epoch: 7 step: 22, loss is 0.11867120862007141\n",
      "epoch: 7 step: 23, loss is 0.3840293884277344\n",
      "epoch: 7 step: 24, loss is 0.37301862239837646\n",
      "epoch: 7 step: 25, loss is 0.5115447640419006\n",
      "epoch: 7 step: 26, loss is 0.11685995757579803\n",
      "epoch: 7 step: 27, loss is 0.09578285366296768\n",
      "epoch: 7 step: 28, loss is 0.2280721217393875\n",
      "epoch: 7 step: 29, loss is 0.27671509981155396\n",
      "epoch: 7 step: 30, loss is 1.293691873550415\n",
      "epoch: 7 step: 31, loss is 0.68680340051651\n",
      "epoch: 7 step: 32, loss is 0.18078455328941345\n",
      "epoch: 7 step: 33, loss is 0.5393397808074951\n",
      "epoch: 7 step: 34, loss is 0.08140479028224945\n",
      "epoch: 7 step: 35, loss is 0.42851489782333374\n",
      "epoch: 7 step: 36, loss is 0.1998203694820404\n",
      "epoch: 7 step: 37, loss is 0.9155580401420593\n",
      "epoch: 7 step: 38, loss is 0.11235547065734863\n",
      "epoch: 7 step: 39, loss is 0.2772277593612671\n",
      "epoch: 7 step: 40, loss is 0.09284218400716782\n",
      "epoch: 7 step: 41, loss is 0.4005175828933716\n",
      "epoch: 7 step: 42, loss is 0.11923610419034958\n",
      "epoch: 7 step: 43, loss is 0.19625583291053772\n",
      "epoch: 7 step: 44, loss is 0.22815890610218048\n",
      "epoch: 7 step: 45, loss is 0.23499298095703125\n",
      "epoch: 7 step: 46, loss is 0.35259905457496643\n",
      "epoch: 7 step: 47, loss is 1.1405175924301147\n",
      "epoch: 7 step: 48, loss is 0.650057852268219\n",
      "epoch: 7 step: 49, loss is 0.1417216956615448\n",
      "epoch: 7 step: 50, loss is 0.31266871094703674\n",
      "epoch: 7 step: 51, loss is 0.23517972230911255\n",
      "epoch: 7 step: 52, loss is 0.34050440788269043\n",
      "epoch: 7 step: 53, loss is 0.4529397487640381\n",
      "epoch: 7 step: 54, loss is 0.6833645105361938\n",
      "epoch: 7 step: 55, loss is 0.20373983681201935\n",
      "epoch: 7 step: 56, loss is 0.32798105478286743\n",
      "epoch: 7 step: 57, loss is 0.12421521544456482\n",
      "epoch: 7 step: 58, loss is 0.21267040073871613\n",
      "epoch: 7 step: 59, loss is 0.5160825252532959\n",
      "epoch: 7 step: 60, loss is 0.368689626455307\n",
      "epoch: 7 step: 61, loss is 0.9733657836914062\n",
      "epoch: 7 step: 62, loss is 0.13970638811588287\n",
      "epoch: 7 step: 63, loss is 0.2650110125541687\n",
      "epoch: 7 step: 64, loss is 0.4403918385505676\n",
      "epoch: 7 step: 65, loss is 0.1308274269104004\n",
      "epoch: 7 step: 66, loss is 0.5206757187843323\n",
      "epoch: 7 step: 67, loss is 0.1257425993680954\n",
      "epoch: 7 step: 68, loss is 0.34636539220809937\n",
      "epoch: 7 step: 69, loss is 0.5511866807937622\n",
      "epoch: 7 step: 70, loss is 0.1037660539150238\n",
      "epoch: 7 step: 71, loss is 0.21051187813282013\n",
      "epoch: 7 step: 72, loss is 0.14307138323783875\n",
      "epoch: 7 step: 73, loss is 0.2448507696390152\n",
      "epoch: 7 step: 74, loss is 0.1812317967414856\n",
      "epoch: 7 step: 75, loss is 0.79615318775177\n",
      "epoch: 7 step: 76, loss is 0.15888699889183044\n",
      "epoch: 7 step: 77, loss is 0.12278728187084198\n",
      "epoch: 7 step: 78, loss is 0.5688060522079468\n",
      "epoch: 7 step: 79, loss is 0.17360815405845642\n",
      "epoch: 7 step: 80, loss is 0.16628259420394897\n",
      "epoch: 7 step: 81, loss is 0.7433260679244995\n",
      "epoch: 7 step: 82, loss is 0.44393306970596313\n",
      "epoch: 7 step: 83, loss is 0.3981764614582062\n",
      "epoch: 7 step: 84, loss is 0.6336255073547363\n",
      "{'accuracy': 0.8588235294117647, 'macro f1': 0.7690217391304348, 'weighted f1': 0.8384910485933503, 'macro recall': 0.7272727272727273, 'macro precision': 0.9199999999999999}\n",
      "Eval result: epoch 7, metrics: {'metrics': 0.8187794989537316}\n",
      "epoch: 8 step: 1, loss is 0.11919009685516357\n",
      "epoch: 8 step: 2, loss is 0.3986181616783142\n",
      "epoch: 8 step: 3, loss is 0.27309396862983704\n",
      "epoch: 8 step: 4, loss is 0.21442976593971252\n",
      "epoch: 8 step: 5, loss is 0.07498902082443237\n",
      "epoch: 8 step: 6, loss is 0.7126362919807434\n",
      "epoch: 8 step: 7, loss is 0.7519913911819458\n",
      "epoch: 8 step: 8, loss is 0.2944028377532959\n",
      "epoch: 8 step: 9, loss is 0.11148594319820404\n",
      "epoch: 8 step: 10, loss is 0.3090927004814148\n",
      "epoch: 8 step: 11, loss is 0.09091684222221375\n",
      "epoch: 8 step: 12, loss is 0.2406296730041504\n",
      "epoch: 8 step: 13, loss is 0.16031387448310852\n",
      "epoch: 8 step: 14, loss is 0.8178225755691528\n",
      "epoch: 8 step: 15, loss is 0.11376953125\n",
      "epoch: 8 step: 16, loss is 0.11153580993413925\n",
      "epoch: 8 step: 17, loss is 0.5436415076255798\n",
      "epoch: 8 step: 18, loss is 0.13714388012886047\n",
      "epoch: 8 step: 19, loss is 0.4435071647167206\n",
      "epoch: 8 step: 20, loss is 0.4990960955619812\n",
      "epoch: 8 step: 21, loss is 0.1442088633775711\n",
      "epoch: 8 step: 22, loss is 0.9750029444694519\n",
      "epoch: 8 step: 23, loss is 0.3247770667076111\n",
      "epoch: 8 step: 24, loss is 0.7594722509384155\n",
      "epoch: 8 step: 25, loss is 0.535743236541748\n",
      "epoch: 8 step: 26, loss is 0.16100721061229706\n",
      "epoch: 8 step: 27, loss is 0.5253241062164307\n",
      "epoch: 8 step: 28, loss is 0.5421006679534912\n",
      "epoch: 8 step: 29, loss is 0.2546563744544983\n",
      "epoch: 8 step: 30, loss is 0.5522321462631226\n",
      "epoch: 8 step: 31, loss is 0.3442264795303345\n",
      "epoch: 8 step: 32, loss is 0.867585301399231\n",
      "epoch: 8 step: 33, loss is 0.19043809175491333\n",
      "epoch: 8 step: 34, loss is 0.4094608724117279\n",
      "epoch: 8 step: 35, loss is 0.19559568166732788\n",
      "epoch: 8 step: 36, loss is 0.3231871426105499\n",
      "epoch: 8 step: 37, loss is 0.15217213332653046\n",
      "epoch: 8 step: 38, loss is 0.4207671880722046\n",
      "epoch: 8 step: 39, loss is 0.47778981924057007\n",
      "epoch: 8 step: 40, loss is 0.12441810965538025\n",
      "epoch: 8 step: 41, loss is 0.12456803023815155\n",
      "epoch: 8 step: 42, loss is 0.4356609284877777\n",
      "epoch: 8 step: 43, loss is 0.10657903552055359\n",
      "epoch: 8 step: 44, loss is 0.09274739772081375\n",
      "epoch: 8 step: 45, loss is 0.16577813029289246\n",
      "epoch: 8 step: 46, loss is 0.3654616177082062\n",
      "epoch: 8 step: 47, loss is 0.10478372871875763\n",
      "epoch: 8 step: 48, loss is 0.17702728509902954\n",
      "epoch: 8 step: 49, loss is 0.8314684629440308\n",
      "epoch: 8 step: 50, loss is 0.43446746468544006\n",
      "epoch: 8 step: 51, loss is 0.10983489453792572\n",
      "epoch: 8 step: 52, loss is 0.34213703870773315\n",
      "epoch: 8 step: 53, loss is 0.1322629451751709\n",
      "epoch: 8 step: 54, loss is 0.15109001100063324\n",
      "epoch: 8 step: 55, loss is 0.7280063629150391\n",
      "epoch: 8 step: 56, loss is 0.44079554080963135\n",
      "epoch: 8 step: 57, loss is 0.1563718169927597\n",
      "epoch: 8 step: 58, loss is 0.389934241771698\n",
      "epoch: 8 step: 59, loss is 0.09295479953289032\n",
      "epoch: 8 step: 60, loss is 0.1919262707233429\n",
      "epoch: 8 step: 61, loss is 0.11158032715320587\n",
      "epoch: 8 step: 62, loss is 0.1855809986591339\n",
      "epoch: 8 step: 63, loss is 0.4872341752052307\n",
      "epoch: 8 step: 64, loss is 0.15395790338516235\n",
      "epoch: 8 step: 65, loss is 0.19969120621681213\n",
      "epoch: 8 step: 66, loss is 0.6490200161933899\n",
      "epoch: 8 step: 67, loss is 0.41653433442115784\n",
      "epoch: 8 step: 68, loss is 0.105492502450943\n",
      "epoch: 8 step: 69, loss is 1.030616283416748\n",
      "epoch: 8 step: 70, loss is 1.3140608072280884\n",
      "epoch: 8 step: 71, loss is 0.18106110394001007\n",
      "epoch: 8 step: 72, loss is 0.6154236793518066\n",
      "epoch: 8 step: 73, loss is 0.5131674408912659\n",
      "epoch: 8 step: 74, loss is 0.5904004573822021\n",
      "epoch: 8 step: 75, loss is 0.07444305717945099\n",
      "epoch: 8 step: 76, loss is 0.33681267499923706\n",
      "epoch: 8 step: 77, loss is 0.10909677296876907\n",
      "epoch: 8 step: 78, loss is 0.11228421330451965\n",
      "epoch: 8 step: 79, loss is 0.12760834395885468\n",
      "epoch: 8 step: 80, loss is 0.23048721253871918\n",
      "epoch: 8 step: 81, loss is 0.853566586971283\n",
      "epoch: 8 step: 82, loss is 0.4263334274291992\n",
      "epoch: 8 step: 83, loss is 0.11999687552452087\n",
      "epoch: 8 step: 84, loss is 0.19157811999320984\n",
      "{'accuracy': 0.8588235294117647, 'macro f1': 0.7690217391304348, 'weighted f1': 0.8384910485933503, 'macro recall': 0.7272727272727273, 'macro precision': 0.9199999999999999}\n",
      "Eval result: epoch 8, metrics: {'metrics': 0.8187794989537316}\n",
      "epoch: 9 step: 1, loss is 0.11774076521396637\n",
      "epoch: 9 step: 2, loss is 0.7942060828208923\n",
      "epoch: 9 step: 3, loss is 0.35663145780563354\n",
      "epoch: 9 step: 4, loss is 0.6048119068145752\n",
      "epoch: 9 step: 5, loss is 0.13503462076187134\n",
      "epoch: 9 step: 6, loss is 0.8046585321426392\n",
      "epoch: 9 step: 7, loss is 0.14633294939994812\n",
      "epoch: 9 step: 8, loss is 0.18567189574241638\n",
      "epoch: 9 step: 9, loss is 0.7891960144042969\n",
      "epoch: 9 step: 10, loss is 0.11394502222537994\n",
      "epoch: 9 step: 11, loss is 0.08658467233181\n",
      "epoch: 9 step: 12, loss is 0.7521355152130127\n",
      "epoch: 9 step: 13, loss is 0.14528557658195496\n",
      "epoch: 9 step: 14, loss is 0.1307363063097\n",
      "epoch: 9 step: 15, loss is 0.20678739249706268\n",
      "epoch: 9 step: 16, loss is 0.40849438309669495\n",
      "epoch: 9 step: 17, loss is 0.3174741566181183\n",
      "epoch: 9 step: 18, loss is 0.1283600926399231\n",
      "epoch: 9 step: 19, loss is 0.130245178937912\n",
      "epoch: 9 step: 20, loss is 0.2544487714767456\n",
      "epoch: 9 step: 21, loss is 0.6582202911376953\n",
      "epoch: 9 step: 22, loss is 0.0937352180480957\n",
      "epoch: 9 step: 23, loss is 0.12178640812635422\n",
      "epoch: 9 step: 24, loss is 0.504205584526062\n",
      "epoch: 9 step: 25, loss is 0.15968213975429535\n",
      "epoch: 9 step: 26, loss is 0.36163952946662903\n",
      "epoch: 9 step: 27, loss is 0.06471866369247437\n",
      "epoch: 9 step: 28, loss is 0.4578758478164673\n",
      "epoch: 9 step: 29, loss is 0.2566022574901581\n",
      "epoch: 9 step: 30, loss is 0.18959321081638336\n",
      "epoch: 9 step: 31, loss is 0.10867176949977875\n",
      "epoch: 9 step: 32, loss is 0.11642435193061829\n",
      "epoch: 9 step: 33, loss is 0.5965578556060791\n",
      "epoch: 9 step: 34, loss is 0.7290347814559937\n",
      "epoch: 9 step: 35, loss is 0.09752295166254044\n",
      "epoch: 9 step: 36, loss is 1.017996072769165\n",
      "epoch: 9 step: 37, loss is 0.6071832180023193\n",
      "epoch: 9 step: 38, loss is 0.09112768620252609\n",
      "epoch: 9 step: 39, loss is 0.11075863242149353\n",
      "epoch: 9 step: 40, loss is 0.6035850644111633\n",
      "epoch: 9 step: 41, loss is 0.6126100420951843\n",
      "epoch: 9 step: 42, loss is 0.1723950356245041\n",
      "epoch: 9 step: 43, loss is 0.12100106477737427\n",
      "epoch: 9 step: 44, loss is 0.25838202238082886\n",
      "epoch: 9 step: 45, loss is 0.2890736162662506\n",
      "epoch: 9 step: 46, loss is 0.1367587000131607\n",
      "epoch: 9 step: 47, loss is 0.3178274631500244\n",
      "epoch: 9 step: 48, loss is 0.48103418946266174\n",
      "epoch: 9 step: 49, loss is 0.29073360562324524\n",
      "epoch: 9 step: 50, loss is 0.7074208855628967\n",
      "epoch: 9 step: 51, loss is 0.1438644677400589\n",
      "epoch: 9 step: 52, loss is 0.4987347722053528\n",
      "epoch: 9 step: 53, loss is 0.7579297423362732\n",
      "epoch: 9 step: 54, loss is 0.25393933057785034\n",
      "epoch: 9 step: 55, loss is 0.24133914709091187\n",
      "epoch: 9 step: 56, loss is 0.1443273425102234\n",
      "epoch: 9 step: 57, loss is 0.15683728456497192\n",
      "epoch: 9 step: 58, loss is 0.29277563095092773\n",
      "epoch: 9 step: 59, loss is 0.3305966258049011\n",
      "epoch: 9 step: 60, loss is 0.10366354882717133\n",
      "epoch: 9 step: 61, loss is 0.7262434363365173\n",
      "epoch: 9 step: 62, loss is 0.08702227473258972\n",
      "epoch: 9 step: 63, loss is 0.15598785877227783\n",
      "epoch: 9 step: 64, loss is 0.16306471824645996\n",
      "epoch: 9 step: 65, loss is 0.31690582633018494\n",
      "epoch: 9 step: 66, loss is 0.5178956389427185\n",
      "epoch: 9 step: 67, loss is 0.13041676580905914\n",
      "epoch: 9 step: 68, loss is 0.8909375071525574\n",
      "epoch: 9 step: 69, loss is 0.19142109155654907\n",
      "epoch: 9 step: 70, loss is 0.4509058892726898\n",
      "epoch: 9 step: 71, loss is 0.11991441994905472\n",
      "epoch: 9 step: 72, loss is 0.9675725102424622\n",
      "epoch: 9 step: 73, loss is 0.1431710422039032\n",
      "epoch: 9 step: 74, loss is 0.07539485394954681\n",
      "epoch: 9 step: 75, loss is 0.2799389958381653\n",
      "epoch: 9 step: 76, loss is 0.3759133517742157\n",
      "epoch: 9 step: 77, loss is 0.3692057132720947\n",
      "epoch: 9 step: 78, loss is 0.6292174458503723\n",
      "epoch: 9 step: 79, loss is 0.6317406892776489\n",
      "epoch: 9 step: 80, loss is 0.23819132149219513\n",
      "epoch: 9 step: 81, loss is 0.12045444548130035\n",
      "epoch: 9 step: 82, loss is 0.17168565094470978\n",
      "epoch: 9 step: 83, loss is 0.26423484086990356\n",
      "epoch: 9 step: 84, loss is 0.11543207615613937\n",
      "{'accuracy': 0.8823529411764706, 'macro f1': 0.8161764705882353, 'weighted f1': 0.8693771626297578, 'macro recall': 0.7727272727272727, 'macro precision': 0.9315068493150684}\n",
      "Eval result: epoch 9, metrics: {'metrics': 0.8506908834517617}\n",
      "epoch: 10 step: 1, loss is 0.1731986701488495\n",
      "epoch: 10 step: 2, loss is 0.5284507274627686\n",
      "epoch: 10 step: 3, loss is 0.2797130346298218\n",
      "epoch: 10 step: 4, loss is 0.30679038166999817\n",
      "epoch: 10 step: 5, loss is 0.6143409013748169\n",
      "epoch: 10 step: 6, loss is 0.7944352626800537\n",
      "epoch: 10 step: 7, loss is 0.8716988563537598\n",
      "epoch: 10 step: 8, loss is 0.5591039061546326\n",
      "epoch: 10 step: 9, loss is 0.09682121872901917\n",
      "epoch: 10 step: 10, loss is 0.3409503102302551\n",
      "epoch: 10 step: 11, loss is 0.11245359480381012\n",
      "epoch: 10 step: 12, loss is 0.08729071915149689\n",
      "epoch: 10 step: 13, loss is 0.12568658590316772\n",
      "epoch: 10 step: 14, loss is 0.8779011964797974\n",
      "epoch: 10 step: 15, loss is 0.8098100423812866\n",
      "epoch: 10 step: 16, loss is 0.11917702108621597\n",
      "epoch: 10 step: 17, loss is 0.18191668391227722\n",
      "epoch: 10 step: 18, loss is 0.19345824420452118\n",
      "epoch: 10 step: 19, loss is 0.6442556381225586\n",
      "epoch: 10 step: 20, loss is 0.8894637823104858\n",
      "epoch: 10 step: 21, loss is 0.7255563735961914\n",
      "epoch: 10 step: 22, loss is 0.23171865940093994\n",
      "epoch: 10 step: 23, loss is 0.1384308785200119\n",
      "epoch: 10 step: 24, loss is 0.07764679193496704\n",
      "epoch: 10 step: 25, loss is 0.12033542990684509\n",
      "epoch: 10 step: 26, loss is 0.21000784635543823\n",
      "epoch: 10 step: 27, loss is 0.2756369411945343\n",
      "epoch: 10 step: 28, loss is 0.08251947164535522\n",
      "epoch: 10 step: 29, loss is 0.07778345048427582\n",
      "epoch: 10 step: 30, loss is 0.5685963034629822\n",
      "epoch: 10 step: 31, loss is 0.5360362529754639\n",
      "epoch: 10 step: 32, loss is 0.23303914070129395\n",
      "epoch: 10 step: 33, loss is 0.5399232506752014\n",
      "epoch: 10 step: 34, loss is 0.442913293838501\n",
      "epoch: 10 step: 35, loss is 0.08446082472801208\n",
      "epoch: 10 step: 36, loss is 0.3762578070163727\n",
      "epoch: 10 step: 37, loss is 0.10212831199169159\n",
      "epoch: 10 step: 38, loss is 0.42782142758369446\n",
      "epoch: 10 step: 39, loss is 0.5810631513595581\n",
      "epoch: 10 step: 40, loss is 0.3365556597709656\n",
      "epoch: 10 step: 41, loss is 0.22820624709129333\n",
      "epoch: 10 step: 42, loss is 0.21803158521652222\n",
      "epoch: 10 step: 43, loss is 0.1046774834394455\n",
      "epoch: 10 step: 44, loss is 0.5221070051193237\n",
      "epoch: 10 step: 45, loss is 0.10592080652713776\n",
      "epoch: 10 step: 46, loss is 0.12299568951129913\n",
      "epoch: 10 step: 47, loss is 0.07620711624622345\n",
      "epoch: 10 step: 48, loss is 0.35023272037506104\n",
      "epoch: 10 step: 49, loss is 0.30307602882385254\n",
      "epoch: 10 step: 50, loss is 0.4716907739639282\n",
      "epoch: 10 step: 51, loss is 0.15746533870697021\n",
      "epoch: 10 step: 52, loss is 0.1586218774318695\n",
      "epoch: 10 step: 53, loss is 0.1031913161277771\n",
      "epoch: 10 step: 54, loss is 0.11069854348897934\n",
      "epoch: 10 step: 55, loss is 0.08982539176940918\n",
      "epoch: 10 step: 56, loss is 0.4620184898376465\n",
      "epoch: 10 step: 57, loss is 0.4647907614707947\n",
      "epoch: 10 step: 58, loss is 0.13509440422058105\n",
      "epoch: 10 step: 59, loss is 0.2061435580253601\n",
      "epoch: 10 step: 60, loss is 0.1130640059709549\n",
      "epoch: 10 step: 61, loss is 0.19698379933834076\n",
      "epoch: 10 step: 62, loss is 0.8600058555603027\n",
      "epoch: 10 step: 63, loss is 0.12834960222244263\n",
      "epoch: 10 step: 64, loss is 0.1627364158630371\n",
      "epoch: 10 step: 65, loss is 0.10513065755367279\n",
      "epoch: 10 step: 66, loss is 0.30956679582595825\n",
      "epoch: 10 step: 67, loss is 0.318888783454895\n",
      "epoch: 10 step: 68, loss is 0.1104159951210022\n",
      "epoch: 10 step: 69, loss is 0.17203596234321594\n",
      "epoch: 10 step: 70, loss is 0.1578996479511261\n",
      "epoch: 10 step: 71, loss is 0.4078693687915802\n",
      "epoch: 10 step: 72, loss is 0.3737269639968872\n",
      "epoch: 10 step: 73, loss is 0.18137195706367493\n",
      "epoch: 10 step: 74, loss is 0.47566789388656616\n",
      "epoch: 10 step: 75, loss is 0.08708272129297256\n",
      "epoch: 10 step: 76, loss is 0.09226208925247192\n",
      "epoch: 10 step: 77, loss is 0.09782484173774719\n",
      "epoch: 10 step: 78, loss is 0.4395287334918976\n",
      "epoch: 10 step: 79, loss is 0.2316933274269104\n",
      "epoch: 10 step: 80, loss is 0.6442857980728149\n",
      "epoch: 10 step: 81, loss is 0.8723200559616089\n",
      "epoch: 10 step: 82, loss is 0.20425176620483398\n",
      "epoch: 10 step: 83, loss is 0.48523232340812683\n",
      "epoch: 10 step: 84, loss is 0.4875189960002899\n",
      "{'accuracy': 0.8941176470588236, 'macro f1': 0.8380952380952381, 'weighted f1': 0.8840336134453781, 'macro recall': 0.7954545454545454, 'macro precision': 0.9375}\n",
      "Eval result: epoch 10, metrics: {'metrics': 0.8662918576521518}\n",
      "epoch: 11 step: 1, loss is 0.09580141305923462\n",
      "epoch: 11 step: 2, loss is 0.07108985632658005\n",
      "epoch: 11 step: 3, loss is 0.22538474202156067\n",
      "epoch: 11 step: 4, loss is 0.817407488822937\n",
      "epoch: 11 step: 5, loss is 0.08009001612663269\n",
      "epoch: 11 step: 6, loss is 0.175113245844841\n",
      "epoch: 11 step: 7, loss is 0.3120199143886566\n",
      "epoch: 11 step: 8, loss is 0.26128363609313965\n",
      "epoch: 11 step: 9, loss is 0.7853026986122131\n",
      "epoch: 11 step: 10, loss is 0.13534919917583466\n",
      "epoch: 11 step: 11, loss is 0.09090982377529144\n",
      "epoch: 11 step: 12, loss is 0.12344235181808472\n",
      "epoch: 11 step: 13, loss is 0.22386528551578522\n",
      "epoch: 11 step: 14, loss is 0.07553666085004807\n",
      "epoch: 11 step: 15, loss is 0.4212661385536194\n",
      "epoch: 11 step: 16, loss is 0.09596090763807297\n",
      "epoch: 11 step: 17, loss is 0.9227203726768494\n",
      "epoch: 11 step: 18, loss is 0.8158015608787537\n",
      "epoch: 11 step: 19, loss is 0.07151468098163605\n",
      "epoch: 11 step: 20, loss is 0.5889735817909241\n",
      "epoch: 11 step: 21, loss is 0.295030415058136\n",
      "epoch: 11 step: 22, loss is 0.10296271741390228\n",
      "epoch: 11 step: 23, loss is 0.11739079654216766\n",
      "epoch: 11 step: 24, loss is 0.7764361500740051\n",
      "epoch: 11 step: 25, loss is 0.2546272575855255\n",
      "epoch: 11 step: 26, loss is 0.4489348530769348\n",
      "epoch: 11 step: 27, loss is 0.1258247047662735\n",
      "epoch: 11 step: 28, loss is 0.33338767290115356\n",
      "epoch: 11 step: 29, loss is 0.06404146552085876\n",
      "epoch: 11 step: 30, loss is 0.35397231578826904\n",
      "epoch: 11 step: 31, loss is 0.5311474204063416\n",
      "epoch: 11 step: 32, loss is 0.9869435429573059\n",
      "epoch: 11 step: 33, loss is 0.1051439717411995\n",
      "epoch: 11 step: 34, loss is 0.7174285054206848\n",
      "epoch: 11 step: 35, loss is 0.05652768537402153\n",
      "epoch: 11 step: 36, loss is 0.08546695858240128\n",
      "epoch: 11 step: 37, loss is 0.1905742734670639\n",
      "epoch: 11 step: 38, loss is 0.1296999156475067\n",
      "epoch: 11 step: 39, loss is 0.06423957645893097\n",
      "epoch: 11 step: 40, loss is 0.1076301634311676\n",
      "epoch: 11 step: 41, loss is 0.0776529461145401\n",
      "epoch: 11 step: 42, loss is 0.35329946875572205\n",
      "epoch: 11 step: 43, loss is 0.07030166685581207\n",
      "epoch: 11 step: 44, loss is 0.287985622882843\n",
      "epoch: 11 step: 45, loss is 0.5271645188331604\n",
      "epoch: 11 step: 46, loss is 0.6625339388847351\n",
      "epoch: 11 step: 47, loss is 0.518165647983551\n",
      "epoch: 11 step: 48, loss is 0.18505671620368958\n",
      "epoch: 11 step: 49, loss is 0.14349053800106049\n",
      "epoch: 11 step: 50, loss is 0.07551109045743942\n",
      "epoch: 11 step: 51, loss is 0.4713149070739746\n",
      "epoch: 11 step: 52, loss is 0.3428633511066437\n",
      "epoch: 11 step: 53, loss is 0.461653470993042\n",
      "epoch: 11 step: 54, loss is 0.5832903981208801\n",
      "epoch: 11 step: 55, loss is 0.21477818489074707\n",
      "epoch: 11 step: 56, loss is 0.7540724873542786\n",
      "epoch: 11 step: 57, loss is 0.10907493531703949\n",
      "epoch: 11 step: 58, loss is 0.2560720145702362\n",
      "epoch: 11 step: 59, loss is 0.20161597430706024\n",
      "epoch: 11 step: 60, loss is 0.6191210746765137\n",
      "epoch: 11 step: 61, loss is 0.22838982939720154\n",
      "epoch: 11 step: 62, loss is 0.13687673211097717\n",
      "epoch: 11 step: 63, loss is 0.4536134898662567\n",
      "epoch: 11 step: 64, loss is 0.10970105230808258\n",
      "epoch: 11 step: 65, loss is 0.534410297870636\n",
      "epoch: 11 step: 66, loss is 0.4178656041622162\n",
      "epoch: 11 step: 67, loss is 0.8339577317237854\n",
      "epoch: 11 step: 68, loss is 0.13808701932430267\n",
      "epoch: 11 step: 69, loss is 0.20906442403793335\n",
      "epoch: 11 step: 70, loss is 0.1116865947842598\n",
      "epoch: 11 step: 71, loss is 0.15755215287208557\n",
      "epoch: 11 step: 72, loss is 0.10552382469177246\n",
      "epoch: 11 step: 73, loss is 0.12139665335416794\n",
      "epoch: 11 step: 74, loss is 0.090646892786026\n",
      "epoch: 11 step: 75, loss is 0.6729149222373962\n",
      "epoch: 11 step: 76, loss is 0.33124685287475586\n",
      "epoch: 11 step: 77, loss is 0.1767423450946808\n",
      "epoch: 11 step: 78, loss is 0.10366500914096832\n",
      "epoch: 11 step: 79, loss is 0.10760326683521271\n",
      "epoch: 11 step: 80, loss is 0.38028383255004883\n",
      "epoch: 11 step: 81, loss is 0.11779239028692245\n",
      "epoch: 11 step: 82, loss is 0.14964385330677032\n",
      "epoch: 11 step: 83, loss is 0.19258460402488708\n",
      "epoch: 11 step: 84, loss is 0.07356162369251251\n",
      "{'accuracy': 0.8941176470588236, 'macro f1': 0.8380952380952381, 'weighted f1': 0.8840336134453781, 'macro recall': 0.7954545454545454, 'macro precision': 0.9375}\n",
      "Eval result: epoch 11, metrics: {'metrics': 0.8662918576521518}\n",
      "epoch: 12 step: 1, loss is 0.5544329881668091\n",
      "epoch: 12 step: 2, loss is 0.12040209025144577\n",
      "epoch: 12 step: 3, loss is 0.47260528802871704\n",
      "epoch: 12 step: 4, loss is 0.25901511311531067\n",
      "epoch: 12 step: 5, loss is 0.13722224533557892\n",
      "epoch: 12 step: 6, loss is 0.12008517980575562\n",
      "epoch: 12 step: 7, loss is 0.5550037026405334\n",
      "epoch: 12 step: 8, loss is 0.3014000952243805\n",
      "epoch: 12 step: 9, loss is 0.16920851171016693\n",
      "epoch: 12 step: 10, loss is 0.16454580426216125\n",
      "epoch: 12 step: 11, loss is 0.1426793932914734\n",
      "epoch: 12 step: 12, loss is 0.13301095366477966\n",
      "epoch: 12 step: 13, loss is 0.17682664096355438\n",
      "epoch: 12 step: 14, loss is 0.09150613099336624\n",
      "epoch: 12 step: 15, loss is 0.16045767068862915\n",
      "epoch: 12 step: 16, loss is 0.33584460616111755\n",
      "epoch: 12 step: 17, loss is 0.7915480732917786\n",
      "epoch: 12 step: 18, loss is 0.5709148645401001\n",
      "epoch: 12 step: 19, loss is 0.7269030213356018\n",
      "epoch: 12 step: 20, loss is 0.07334950566291809\n",
      "epoch: 12 step: 21, loss is 0.06643840670585632\n",
      "epoch: 12 step: 22, loss is 0.6877301335334778\n",
      "epoch: 12 step: 23, loss is 0.07468090206384659\n",
      "epoch: 12 step: 24, loss is 0.08099532127380371\n",
      "epoch: 12 step: 25, loss is 0.41191306710243225\n",
      "epoch: 12 step: 26, loss is 0.4049381911754608\n",
      "epoch: 12 step: 27, loss is 0.06018969789147377\n",
      "epoch: 12 step: 28, loss is 0.3767111599445343\n",
      "epoch: 12 step: 29, loss is 0.08459373563528061\n",
      "epoch: 12 step: 30, loss is 0.07639191299676895\n",
      "epoch: 12 step: 31, loss is 0.5970900654792786\n",
      "epoch: 12 step: 32, loss is 0.3645147979259491\n",
      "epoch: 12 step: 33, loss is 0.6045766472816467\n",
      "epoch: 12 step: 34, loss is 0.08520913124084473\n",
      "epoch: 12 step: 35, loss is 0.08177077770233154\n",
      "epoch: 12 step: 36, loss is 0.5813586115837097\n",
      "epoch: 12 step: 37, loss is 0.18093150854110718\n",
      "epoch: 12 step: 38, loss is 0.08136357367038727\n",
      "epoch: 12 step: 39, loss is 0.26778873801231384\n",
      "epoch: 12 step: 40, loss is 0.4329805374145508\n",
      "epoch: 12 step: 41, loss is 0.14706870913505554\n",
      "epoch: 12 step: 42, loss is 0.24586723744869232\n",
      "epoch: 12 step: 43, loss is 0.7232560515403748\n",
      "epoch: 12 step: 44, loss is 0.21859613060951233\n",
      "epoch: 12 step: 45, loss is 0.6129196882247925\n",
      "epoch: 12 step: 46, loss is 0.08010254055261612\n",
      "epoch: 12 step: 47, loss is 0.34062403440475464\n",
      "epoch: 12 step: 48, loss is 0.1436646431684494\n",
      "epoch: 12 step: 49, loss is 0.6783437728881836\n",
      "epoch: 12 step: 50, loss is 0.605379045009613\n",
      "epoch: 12 step: 51, loss is 0.0789528638124466\n",
      "epoch: 12 step: 52, loss is 0.1714262068271637\n",
      "epoch: 12 step: 53, loss is 0.4204568862915039\n",
      "epoch: 12 step: 54, loss is 0.18712513148784637\n",
      "epoch: 12 step: 55, loss is 0.1841462254524231\n",
      "epoch: 12 step: 56, loss is 0.08699578046798706\n",
      "epoch: 12 step: 57, loss is 0.8180946111679077\n",
      "epoch: 12 step: 58, loss is 0.11760115623474121\n",
      "epoch: 12 step: 59, loss is 0.1185702383518219\n",
      "epoch: 12 step: 60, loss is 0.06818848848342896\n",
      "epoch: 12 step: 61, loss is 0.1107972115278244\n",
      "epoch: 12 step: 62, loss is 0.4567294716835022\n",
      "epoch: 12 step: 63, loss is 0.7813670039176941\n",
      "epoch: 12 step: 64, loss is 0.07351137697696686\n",
      "epoch: 12 step: 65, loss is 0.30470213294029236\n",
      "epoch: 12 step: 66, loss is 0.38112151622772217\n",
      "epoch: 12 step: 67, loss is 0.04996357858181\n",
      "epoch: 12 step: 68, loss is 0.14678214490413666\n",
      "epoch: 12 step: 69, loss is 0.0698605477809906\n",
      "epoch: 12 step: 70, loss is 0.08404600620269775\n",
      "epoch: 12 step: 71, loss is 0.8800983428955078\n",
      "epoch: 12 step: 72, loss is 0.42022669315338135\n",
      "epoch: 12 step: 73, loss is 0.2713193893432617\n",
      "epoch: 12 step: 74, loss is 0.144100159406662\n",
      "epoch: 12 step: 75, loss is 0.4493750035762787\n",
      "epoch: 12 step: 76, loss is 0.28584080934524536\n",
      "epoch: 12 step: 77, loss is 0.11001227796077728\n",
      "epoch: 12 step: 78, loss is 0.09133567661046982\n",
      "epoch: 12 step: 79, loss is 0.08206184953451157\n",
      "epoch: 12 step: 80, loss is 0.17083515226840973\n",
      "epoch: 12 step: 81, loss is 0.09744109958410263\n",
      "epoch: 12 step: 82, loss is 0.550293505191803\n",
      "epoch: 12 step: 83, loss is 0.11215649545192719\n",
      "epoch: 12 step: 84, loss is 0.6278226971626282\n",
      "{'accuracy': 0.9058823529411765, 'macro f1': 0.8590381426202323, 'weighted f1': 0.8982343186030632, 'macro recall': 0.8181818181818181, 'macro precision': 0.943661971830986}\n",
      "Eval result: epoch 12, metrics: {'metrics': 0.8816910713935533}\n",
      "epoch: 13 step: 1, loss is 0.6804869174957275\n",
      "epoch: 13 step: 2, loss is 1.3138980865478516\n",
      "epoch: 13 step: 3, loss is 0.079823337495327\n",
      "epoch: 13 step: 4, loss is 0.21394358575344086\n",
      "epoch: 13 step: 5, loss is 0.37269771099090576\n",
      "epoch: 13 step: 6, loss is 0.18371370434761047\n",
      "epoch: 13 step: 7, loss is 0.6283949613571167\n",
      "epoch: 13 step: 8, loss is 0.32136625051498413\n",
      "epoch: 13 step: 9, loss is 0.7029976844787598\n",
      "epoch: 13 step: 10, loss is 0.1018250435590744\n",
      "epoch: 13 step: 11, loss is 0.18560397624969482\n",
      "epoch: 13 step: 12, loss is 0.10501252859830856\n",
      "epoch: 13 step: 13, loss is 0.1042739525437355\n",
      "epoch: 13 step: 14, loss is 0.10944434255361557\n",
      "epoch: 13 step: 15, loss is 0.9108657836914062\n",
      "epoch: 13 step: 16, loss is 0.10000211745500565\n",
      "epoch: 13 step: 17, loss is 0.7447704076766968\n",
      "epoch: 13 step: 18, loss is 0.30839934945106506\n",
      "epoch: 13 step: 19, loss is 0.25330954790115356\n",
      "epoch: 13 step: 20, loss is 0.10243220627307892\n",
      "epoch: 13 step: 21, loss is 0.13394327461719513\n",
      "epoch: 13 step: 22, loss is 0.16083218157291412\n",
      "epoch: 13 step: 23, loss is 0.16617953777313232\n",
      "epoch: 13 step: 24, loss is 0.09786276519298553\n",
      "epoch: 13 step: 25, loss is 0.11509517580270767\n",
      "epoch: 13 step: 26, loss is 0.5860460996627808\n",
      "epoch: 13 step: 27, loss is 0.10739803314208984\n",
      "epoch: 13 step: 28, loss is 0.3462067246437073\n",
      "epoch: 13 step: 29, loss is 0.12319819629192352\n",
      "epoch: 13 step: 30, loss is 0.051586635410785675\n",
      "epoch: 13 step: 31, loss is 0.10611990094184875\n",
      "epoch: 13 step: 32, loss is 0.12980018556118011\n",
      "epoch: 13 step: 33, loss is 0.26671579480171204\n",
      "epoch: 13 step: 34, loss is 0.04598963260650635\n",
      "epoch: 13 step: 35, loss is 0.0679602175951004\n",
      "epoch: 13 step: 36, loss is 0.14489024877548218\n",
      "epoch: 13 step: 37, loss is 0.5455469489097595\n",
      "epoch: 13 step: 38, loss is 0.707332193851471\n",
      "epoch: 13 step: 39, loss is 0.24774089455604553\n",
      "epoch: 13 step: 40, loss is 0.08743392676115036\n",
      "epoch: 13 step: 41, loss is 0.1384730041027069\n",
      "epoch: 13 step: 42, loss is 0.6355420351028442\n",
      "epoch: 13 step: 43, loss is 0.1429029405117035\n",
      "epoch: 13 step: 44, loss is 1.30971097946167\n",
      "epoch: 13 step: 45, loss is 0.1398145705461502\n",
      "epoch: 13 step: 46, loss is 0.07274235039949417\n",
      "epoch: 13 step: 47, loss is 0.14571809768676758\n",
      "epoch: 13 step: 48, loss is 0.42600056529045105\n",
      "epoch: 13 step: 49, loss is 0.25952497124671936\n",
      "epoch: 13 step: 50, loss is 0.08882090449333191\n",
      "epoch: 13 step: 51, loss is 0.14822810888290405\n",
      "epoch: 13 step: 52, loss is 0.10378546267747879\n",
      "epoch: 13 step: 53, loss is 0.27618569135665894\n",
      "epoch: 13 step: 54, loss is 0.4631905257701874\n",
      "epoch: 13 step: 55, loss is 0.12681804597377777\n",
      "epoch: 13 step: 56, loss is 0.6727256774902344\n",
      "epoch: 13 step: 57, loss is 0.33133751153945923\n",
      "epoch: 13 step: 58, loss is 0.1683509349822998\n",
      "epoch: 13 step: 59, loss is 0.08787673711776733\n",
      "epoch: 13 step: 60, loss is 0.48040804266929626\n",
      "epoch: 13 step: 61, loss is 0.14115950465202332\n",
      "epoch: 13 step: 62, loss is 0.09178175032138824\n",
      "epoch: 13 step: 63, loss is 0.5343282222747803\n",
      "epoch: 13 step: 64, loss is 0.08558125793933868\n",
      "epoch: 13 step: 65, loss is 0.27610284090042114\n",
      "epoch: 13 step: 66, loss is 0.12233015894889832\n",
      "epoch: 13 step: 67, loss is 0.21656696498394012\n",
      "epoch: 13 step: 68, loss is 0.5168125629425049\n",
      "epoch: 13 step: 69, loss is 0.3683730661869049\n",
      "epoch: 13 step: 70, loss is 0.21901552379131317\n",
      "epoch: 13 step: 71, loss is 0.12373530119657516\n",
      "epoch: 13 step: 72, loss is 0.04504840821027756\n",
      "epoch: 13 step: 73, loss is 0.6272333264350891\n",
      "epoch: 13 step: 74, loss is 0.1474514752626419\n",
      "epoch: 13 step: 75, loss is 0.1783686876296997\n",
      "epoch: 13 step: 76, loss is 0.09810608625411987\n",
      "epoch: 13 step: 77, loss is 0.600547194480896\n",
      "epoch: 13 step: 78, loss is 0.09348147362470627\n",
      "epoch: 13 step: 79, loss is 0.24597133696079254\n",
      "epoch: 13 step: 80, loss is 0.11940518766641617\n",
      "epoch: 13 step: 81, loss is 0.156749427318573\n",
      "epoch: 13 step: 82, loss is 0.07735321670770645\n",
      "epoch: 13 step: 83, loss is 0.11110846698284149\n",
      "epoch: 13 step: 84, loss is 0.08799031376838684\n",
      "{'accuracy': 0.9176470588235294, 'macro f1': 0.8790896159317212, 'weighted f1': 0.9120240984018073, 'macro recall': 0.8409090909090908, 'macro precision': 0.95}\n",
      "Eval result: epoch 13, metrics: {'metrics': 0.8969114414160853}\n",
      "epoch: 14 step: 1, loss is 0.409445583820343\n",
      "epoch: 14 step: 2, loss is 0.6957958936691284\n",
      "epoch: 14 step: 3, loss is 0.1646290421485901\n",
      "epoch: 14 step: 4, loss is 0.08755674213171005\n",
      "epoch: 14 step: 5, loss is 0.08731734752655029\n",
      "epoch: 14 step: 6, loss is 0.47460031509399414\n",
      "epoch: 14 step: 7, loss is 0.05108353868126869\n",
      "epoch: 14 step: 8, loss is 0.12065389007329941\n",
      "epoch: 14 step: 9, loss is 0.04431391879916191\n",
      "epoch: 14 step: 10, loss is 0.23124197125434875\n",
      "epoch: 14 step: 11, loss is 0.6333438158035278\n",
      "epoch: 14 step: 12, loss is 0.08321395516395569\n",
      "epoch: 14 step: 13, loss is 0.12352432310581207\n",
      "epoch: 14 step: 14, loss is 0.71046382188797\n",
      "epoch: 14 step: 15, loss is 0.11222586035728455\n",
      "epoch: 14 step: 16, loss is 0.06397773325443268\n",
      "epoch: 14 step: 17, loss is 0.6292895078659058\n",
      "epoch: 14 step: 18, loss is 0.36935025453567505\n",
      "epoch: 14 step: 19, loss is 0.07312905043363571\n",
      "epoch: 14 step: 20, loss is 0.3007436692714691\n",
      "epoch: 14 step: 21, loss is 0.07163958996534348\n",
      "epoch: 14 step: 22, loss is 0.498717725276947\n",
      "epoch: 14 step: 23, loss is 0.08389440923929214\n",
      "epoch: 14 step: 24, loss is 0.16633039712905884\n",
      "epoch: 14 step: 25, loss is 0.19438187777996063\n",
      "epoch: 14 step: 26, loss is 0.0825500339269638\n",
      "epoch: 14 step: 27, loss is 0.09031308442354202\n",
      "epoch: 14 step: 28, loss is 0.3373892903327942\n",
      "epoch: 14 step: 29, loss is 0.2092629373073578\n",
      "epoch: 14 step: 30, loss is 0.6065321564674377\n",
      "epoch: 14 step: 31, loss is 0.08766020834445953\n",
      "epoch: 14 step: 32, loss is 0.12975332140922546\n",
      "epoch: 14 step: 33, loss is 0.17759351432323456\n",
      "epoch: 14 step: 34, loss is 0.09208691865205765\n",
      "epoch: 14 step: 35, loss is 0.07419463992118835\n",
      "epoch: 14 step: 36, loss is 0.06472167372703552\n",
      "epoch: 14 step: 37, loss is 0.11865922808647156\n",
      "epoch: 14 step: 38, loss is 0.0807800143957138\n",
      "epoch: 14 step: 39, loss is 0.23589009046554565\n",
      "epoch: 14 step: 40, loss is 0.0995212122797966\n",
      "epoch: 14 step: 41, loss is 0.09388592839241028\n",
      "epoch: 14 step: 42, loss is 0.08512839674949646\n",
      "epoch: 14 step: 43, loss is 0.8452126979827881\n",
      "epoch: 14 step: 44, loss is 0.09033549576997757\n",
      "epoch: 14 step: 45, loss is 0.23914191126823425\n",
      "epoch: 14 step: 46, loss is 0.1522463858127594\n",
      "epoch: 14 step: 47, loss is 0.04826386272907257\n",
      "epoch: 14 step: 48, loss is 0.12913677096366882\n",
      "epoch: 14 step: 49, loss is 0.548026442527771\n",
      "epoch: 14 step: 50, loss is 0.1288297176361084\n",
      "epoch: 14 step: 51, loss is 0.39121517539024353\n",
      "epoch: 14 step: 52, loss is 0.14447654783725739\n",
      "epoch: 14 step: 53, loss is 0.06193971633911133\n",
      "epoch: 14 step: 54, loss is 0.4393647313117981\n",
      "epoch: 14 step: 55, loss is 0.13842354714870453\n",
      "epoch: 14 step: 56, loss is 0.49916723370552063\n",
      "epoch: 14 step: 57, loss is 0.4389781951904297\n",
      "epoch: 14 step: 58, loss is 0.06777217239141464\n",
      "epoch: 14 step: 59, loss is 0.44747796654701233\n",
      "epoch: 14 step: 60, loss is 0.10549402236938477\n",
      "epoch: 14 step: 61, loss is 0.39295434951782227\n",
      "epoch: 14 step: 62, loss is 0.39894384145736694\n",
      "epoch: 14 step: 63, loss is 0.0876844972372055\n",
      "epoch: 14 step: 64, loss is 0.05272301286458969\n",
      "epoch: 14 step: 65, loss is 0.4128277897834778\n",
      "epoch: 14 step: 66, loss is 0.05883733555674553\n",
      "epoch: 14 step: 67, loss is 0.1711321920156479\n",
      "epoch: 14 step: 68, loss is 0.07462916523218155\n",
      "epoch: 14 step: 69, loss is 0.48943600058555603\n",
      "epoch: 14 step: 70, loss is 0.9865766763687134\n",
      "epoch: 14 step: 71, loss is 0.6915154457092285\n",
      "epoch: 14 step: 72, loss is 0.24755820631980896\n",
      "epoch: 14 step: 73, loss is 0.20669031143188477\n",
      "epoch: 14 step: 74, loss is 1.144239902496338\n",
      "epoch: 14 step: 75, loss is 0.19062379002571106\n",
      "epoch: 14 step: 76, loss is 0.06386156380176544\n",
      "epoch: 14 step: 77, loss is 0.07867036759853363\n",
      "epoch: 14 step: 78, loss is 0.05457257851958275\n",
      "epoch: 14 step: 79, loss is 0.1681203842163086\n",
      "epoch: 14 step: 80, loss is 0.43655142188072205\n",
      "epoch: 14 step: 81, loss is 0.49318426847457886\n",
      "epoch: 14 step: 82, loss is 0.10015662014484406\n",
      "epoch: 14 step: 83, loss is 0.5210393071174622\n",
      "epoch: 14 step: 84, loss is 0.09275121986865997\n",
      "{'accuracy': 0.9294117647058824, 'macro f1': 0.8983253588516746, 'weighted f1': 0.925443287362792, 'macro recall': 0.8636363636363636, 'macro precision': 0.9565217391304348}\n",
      "Eval result: epoch 14, metrics: {'metrics': 0.911973806581089}\n",
      "epoch: 15 step: 1, loss is 0.6286171674728394\n",
      "epoch: 15 step: 2, loss is 0.4482727646827698\n",
      "epoch: 15 step: 3, loss is 0.0586722306907177\n",
      "epoch: 15 step: 4, loss is 0.1250554323196411\n",
      "epoch: 15 step: 5, loss is 0.5592932105064392\n",
      "epoch: 15 step: 6, loss is 0.14276838302612305\n",
      "epoch: 15 step: 7, loss is 0.1251293122768402\n",
      "epoch: 15 step: 8, loss is 0.07739364355802536\n",
      "epoch: 15 step: 9, loss is 0.08693279325962067\n",
      "epoch: 15 step: 10, loss is 0.18929964303970337\n",
      "epoch: 15 step: 11, loss is 0.06494864821434021\n",
      "epoch: 15 step: 12, loss is 0.0707399845123291\n",
      "epoch: 15 step: 13, loss is 0.05279678851366043\n",
      "epoch: 15 step: 14, loss is 0.07048860937356949\n",
      "epoch: 15 step: 15, loss is 0.3180306851863861\n",
      "epoch: 15 step: 16, loss is 0.7104904651641846\n",
      "epoch: 15 step: 17, loss is 0.07611028850078583\n",
      "epoch: 15 step: 18, loss is 0.4366900324821472\n",
      "epoch: 15 step: 19, loss is 0.32951855659484863\n",
      "epoch: 15 step: 20, loss is 0.1029367595911026\n",
      "epoch: 15 step: 21, loss is 0.11390907317399979\n",
      "epoch: 15 step: 22, loss is 0.10785193741321564\n",
      "epoch: 15 step: 23, loss is 0.22881166636943817\n",
      "epoch: 15 step: 24, loss is 0.0912826806306839\n",
      "epoch: 15 step: 25, loss is 0.0723356381058693\n",
      "epoch: 15 step: 26, loss is 0.08443709462881088\n",
      "epoch: 15 step: 27, loss is 0.981090784072876\n",
      "epoch: 15 step: 28, loss is 0.39558738470077515\n",
      "epoch: 15 step: 29, loss is 0.1300869882106781\n",
      "epoch: 15 step: 30, loss is 0.11458303779363632\n",
      "epoch: 15 step: 31, loss is 0.07062704861164093\n",
      "epoch: 15 step: 32, loss is 0.18206708133220673\n",
      "epoch: 15 step: 33, loss is 0.2157476842403412\n",
      "epoch: 15 step: 34, loss is 0.07095350325107574\n",
      "epoch: 15 step: 35, loss is 0.10625288635492325\n",
      "epoch: 15 step: 36, loss is 0.7208446860313416\n",
      "epoch: 15 step: 37, loss is 0.07997843623161316\n",
      "epoch: 15 step: 38, loss is 0.0436694473028183\n",
      "epoch: 15 step: 39, loss is 0.4114300608634949\n",
      "epoch: 15 step: 40, loss is 0.7630172967910767\n",
      "epoch: 15 step: 41, loss is 0.0970594584941864\n",
      "epoch: 15 step: 42, loss is 0.2749674916267395\n",
      "epoch: 15 step: 43, loss is 0.9621201753616333\n",
      "epoch: 15 step: 44, loss is 0.38885557651519775\n",
      "epoch: 15 step: 45, loss is 0.09930969029664993\n",
      "epoch: 15 step: 46, loss is 0.08491098135709763\n",
      "epoch: 15 step: 47, loss is 0.2511165738105774\n",
      "epoch: 15 step: 48, loss is 0.30339980125427246\n",
      "epoch: 15 step: 49, loss is 0.10073652118444443\n",
      "epoch: 15 step: 50, loss is 0.17843127250671387\n",
      "epoch: 15 step: 51, loss is 0.8467756509780884\n",
      "epoch: 15 step: 52, loss is 0.12896552681922913\n",
      "epoch: 15 step: 53, loss is 0.4338836967945099\n",
      "epoch: 15 step: 54, loss is 0.6508981585502625\n",
      "epoch: 15 step: 55, loss is 0.11818093806505203\n",
      "epoch: 15 step: 56, loss is 0.07965992391109467\n",
      "epoch: 15 step: 57, loss is 0.5492087602615356\n",
      "epoch: 15 step: 58, loss is 0.07866661250591278\n",
      "epoch: 15 step: 59, loss is 0.04088599607348442\n",
      "epoch: 15 step: 60, loss is 0.10609344393014908\n",
      "epoch: 15 step: 61, loss is 0.07883568108081818\n",
      "epoch: 15 step: 62, loss is 0.08258657902479172\n",
      "epoch: 15 step: 63, loss is 0.30204474925994873\n",
      "epoch: 15 step: 64, loss is 0.02446606755256653\n",
      "epoch: 15 step: 65, loss is 0.06730678677558899\n",
      "epoch: 15 step: 66, loss is 0.382461816072464\n",
      "epoch: 15 step: 67, loss is 0.7258060574531555\n",
      "epoch: 15 step: 68, loss is 0.05673497915267944\n",
      "epoch: 15 step: 69, loss is 0.09640424698591232\n",
      "epoch: 15 step: 70, loss is 0.053337462246418\n",
      "epoch: 15 step: 71, loss is 0.07441577315330505\n",
      "epoch: 15 step: 72, loss is 0.43334001302719116\n",
      "epoch: 15 step: 73, loss is 0.13158676028251648\n",
      "epoch: 15 step: 74, loss is 0.6375436186790466\n",
      "epoch: 15 step: 75, loss is 0.7053618431091309\n",
      "epoch: 15 step: 76, loss is 0.16421721875667572\n",
      "epoch: 15 step: 77, loss is 0.33708247542381287\n",
      "epoch: 15 step: 78, loss is 0.1049606204032898\n",
      "epoch: 15 step: 79, loss is 0.06663884967565536\n",
      "epoch: 15 step: 80, loss is 0.26889365911483765\n",
      "epoch: 15 step: 81, loss is 0.09723073244094849\n",
      "epoch: 15 step: 82, loss is 0.3137408196926117\n",
      "epoch: 15 step: 83, loss is 0.0604073703289032\n",
      "epoch: 15 step: 84, loss is 0.11839704215526581\n",
      "{'accuracy': 0.9294117647058824, 'macro f1': 0.8983253588516746, 'weighted f1': 0.925443287362792, 'macro recall': 0.8636363636363636, 'macro precision': 0.9565217391304348}\n",
      "Eval result: epoch 15, metrics: {'metrics': 0.911973806581089}\n",
      "epoch: 16 step: 1, loss is 0.12065917998552322\n",
      "epoch: 16 step: 2, loss is 0.13658255338668823\n",
      "epoch: 16 step: 3, loss is 0.7784969806671143\n",
      "epoch: 16 step: 4, loss is 0.6197400093078613\n",
      "epoch: 16 step: 5, loss is 0.5038754343986511\n",
      "epoch: 16 step: 6, loss is 0.3716934323310852\n",
      "epoch: 16 step: 7, loss is 0.1437518149614334\n",
      "epoch: 16 step: 8, loss is 0.13562418520450592\n",
      "epoch: 16 step: 9, loss is 0.17981070280075073\n",
      "epoch: 16 step: 10, loss is 0.07364071905612946\n",
      "epoch: 16 step: 11, loss is 0.20088398456573486\n",
      "epoch: 16 step: 12, loss is 0.10311780869960785\n",
      "epoch: 16 step: 13, loss is 0.18095988035202026\n",
      "epoch: 16 step: 14, loss is 0.6009398102760315\n",
      "epoch: 16 step: 15, loss is 0.09161882102489471\n",
      "epoch: 16 step: 16, loss is 0.3336220383644104\n",
      "epoch: 16 step: 17, loss is 0.2923274636268616\n",
      "epoch: 16 step: 18, loss is 0.5817067623138428\n",
      "epoch: 16 step: 19, loss is 0.055756110697984695\n",
      "epoch: 16 step: 20, loss is 0.3518128991127014\n",
      "epoch: 16 step: 21, loss is 0.12663373351097107\n",
      "epoch: 16 step: 22, loss is 0.08464893698692322\n",
      "epoch: 16 step: 23, loss is 0.05787861347198486\n",
      "epoch: 16 step: 24, loss is 0.06660231202840805\n",
      "epoch: 16 step: 25, loss is 0.08552366495132446\n",
      "epoch: 16 step: 26, loss is 0.20389950275421143\n",
      "epoch: 16 step: 27, loss is 0.07108928263187408\n",
      "epoch: 16 step: 28, loss is 0.36718934774398804\n",
      "epoch: 16 step: 29, loss is 0.12463624030351639\n",
      "epoch: 16 step: 30, loss is 0.09397439658641815\n",
      "epoch: 16 step: 31, loss is 0.365253746509552\n",
      "epoch: 16 step: 32, loss is 0.058657266199588776\n",
      "epoch: 16 step: 33, loss is 0.08245028555393219\n",
      "epoch: 16 step: 34, loss is 0.47089797258377075\n",
      "epoch: 16 step: 35, loss is 0.40433093905448914\n",
      "epoch: 16 step: 36, loss is 0.5283358097076416\n",
      "epoch: 16 step: 37, loss is 0.33783671259880066\n",
      "epoch: 16 step: 38, loss is 0.04610947519540787\n",
      "epoch: 16 step: 39, loss is 0.07155412435531616\n",
      "epoch: 16 step: 40, loss is 0.058062221854925156\n",
      "epoch: 16 step: 41, loss is 0.11659826338291168\n",
      "epoch: 16 step: 42, loss is 0.6704970598220825\n",
      "epoch: 16 step: 43, loss is 0.09569190442562103\n",
      "epoch: 16 step: 44, loss is 0.08288025856018066\n",
      "epoch: 16 step: 45, loss is 0.3476223945617676\n",
      "epoch: 16 step: 46, loss is 0.3158499002456665\n",
      "epoch: 16 step: 47, loss is 0.43362957239151\n",
      "epoch: 16 step: 48, loss is 0.09553477168083191\n",
      "epoch: 16 step: 49, loss is 0.3132355809211731\n",
      "epoch: 16 step: 50, loss is 0.06394046545028687\n",
      "epoch: 16 step: 51, loss is 0.2020464390516281\n",
      "epoch: 16 step: 52, loss is 0.09619414061307907\n",
      "epoch: 16 step: 53, loss is 0.33422333002090454\n",
      "epoch: 16 step: 54, loss is 0.19883568584918976\n",
      "epoch: 16 step: 55, loss is 0.07061636447906494\n",
      "epoch: 16 step: 56, loss is 0.20933464169502258\n",
      "epoch: 16 step: 57, loss is 0.08196558058261871\n",
      "epoch: 16 step: 58, loss is 0.6412988901138306\n",
      "epoch: 16 step: 59, loss is 0.04199882969260216\n",
      "epoch: 16 step: 60, loss is 0.058923088014125824\n",
      "epoch: 16 step: 61, loss is 0.09932061284780502\n",
      "epoch: 16 step: 62, loss is 0.48231834173202515\n",
      "epoch: 16 step: 63, loss is 0.08634883165359497\n",
      "epoch: 16 step: 64, loss is 0.06489459425210953\n",
      "epoch: 16 step: 65, loss is 0.07457244396209717\n",
      "epoch: 16 step: 66, loss is 0.08565515279769897\n",
      "epoch: 16 step: 67, loss is 0.07606050372123718\n",
      "epoch: 16 step: 68, loss is 0.08047027885913849\n",
      "epoch: 16 step: 69, loss is 1.005063772201538\n",
      "epoch: 16 step: 70, loss is 0.10049517452716827\n",
      "epoch: 16 step: 71, loss is 0.8154882788658142\n",
      "epoch: 16 step: 72, loss is 0.09016847610473633\n",
      "epoch: 16 step: 73, loss is 0.4528813660144806\n",
      "epoch: 16 step: 74, loss is 0.07824616134166718\n",
      "epoch: 16 step: 75, loss is 0.5063597559928894\n",
      "epoch: 16 step: 76, loss is 0.11523431539535522\n",
      "epoch: 16 step: 77, loss is 0.14842058718204498\n",
      "epoch: 16 step: 78, loss is 0.05358646437525749\n",
      "epoch: 16 step: 79, loss is 0.10569442808628082\n",
      "epoch: 16 step: 80, loss is 0.0767284408211708\n",
      "epoch: 16 step: 81, loss is 0.4296058416366577\n",
      "epoch: 16 step: 82, loss is 0.05070493742823601\n",
      "epoch: 16 step: 83, loss is 0.4368629455566406\n",
      "epoch: 16 step: 84, loss is 0.06329262256622314\n",
      "{'accuracy': 0.9411764705882353, 'macro f1': 0.9168134664317871, 'weighted f1': 0.9385283179625346, 'macro recall': 0.8863636363636364, 'macro precision': 0.9632352941176471}\n",
      "Eval result: epoch 16, metrics: {'metrics': 0.9268972168753264}\n",
      "epoch: 17 step: 1, loss is 0.09306363016366959\n",
      "epoch: 17 step: 2, loss is 0.36196550726890564\n",
      "epoch: 17 step: 3, loss is 0.05308280512690544\n",
      "epoch: 17 step: 4, loss is 0.38264939188957214\n",
      "epoch: 17 step: 5, loss is 0.0674610286951065\n",
      "epoch: 17 step: 6, loss is 0.12423039972782135\n",
      "epoch: 17 step: 7, loss is 0.17249487340450287\n",
      "epoch: 17 step: 8, loss is 0.3429170250892639\n",
      "epoch: 17 step: 9, loss is 0.09569960832595825\n",
      "epoch: 17 step: 10, loss is 0.23761671781539917\n",
      "epoch: 17 step: 11, loss is 0.04686441645026207\n",
      "epoch: 17 step: 12, loss is 0.2795203626155853\n",
      "epoch: 17 step: 13, loss is 0.3574472665786743\n",
      "epoch: 17 step: 14, loss is 0.488058477640152\n",
      "epoch: 17 step: 15, loss is 0.1419130563735962\n",
      "epoch: 17 step: 16, loss is 0.36780205368995667\n",
      "epoch: 17 step: 17, loss is 0.2815359830856323\n",
      "epoch: 17 step: 18, loss is 0.07518348097801208\n",
      "epoch: 17 step: 19, loss is 0.6898825168609619\n",
      "epoch: 17 step: 20, loss is 0.06823752820491791\n",
      "epoch: 17 step: 21, loss is 0.08809156715869904\n",
      "epoch: 17 step: 22, loss is 0.10310517251491547\n",
      "epoch: 17 step: 23, loss is 0.6556727886199951\n",
      "epoch: 17 step: 24, loss is 0.1864020824432373\n",
      "epoch: 17 step: 25, loss is 0.38768747448921204\n",
      "epoch: 17 step: 26, loss is 0.07448277622461319\n",
      "epoch: 17 step: 27, loss is 0.17162391543388367\n",
      "epoch: 17 step: 28, loss is 0.07812604308128357\n",
      "epoch: 17 step: 29, loss is 0.09092838317155838\n",
      "epoch: 17 step: 30, loss is 0.14365139603614807\n",
      "epoch: 17 step: 31, loss is 0.06274345517158508\n",
      "epoch: 17 step: 32, loss is 0.06106790155172348\n",
      "epoch: 17 step: 33, loss is 0.06703115999698639\n",
      "epoch: 17 step: 34, loss is 0.12177407741546631\n",
      "epoch: 17 step: 35, loss is 0.08958259224891663\n",
      "epoch: 17 step: 36, loss is 0.4853747487068176\n",
      "epoch: 17 step: 37, loss is 0.2763563096523285\n",
      "epoch: 17 step: 38, loss is 0.4430873990058899\n",
      "epoch: 17 step: 39, loss is 0.0950930118560791\n",
      "epoch: 17 step: 40, loss is 0.205888569355011\n",
      "epoch: 17 step: 41, loss is 0.653015673160553\n",
      "epoch: 17 step: 42, loss is 0.074448361992836\n",
      "epoch: 17 step: 43, loss is 0.05416102707386017\n",
      "epoch: 17 step: 44, loss is 0.07550644874572754\n",
      "epoch: 17 step: 45, loss is 0.920963704586029\n",
      "epoch: 17 step: 46, loss is 0.09863366931676865\n",
      "epoch: 17 step: 47, loss is 0.7080764770507812\n",
      "epoch: 17 step: 48, loss is 0.10207735002040863\n",
      "epoch: 17 step: 49, loss is 0.07545995712280273\n",
      "epoch: 17 step: 50, loss is 0.09573865681886673\n",
      "epoch: 17 step: 51, loss is 0.7571471333503723\n",
      "epoch: 17 step: 52, loss is 0.3639319837093353\n",
      "epoch: 17 step: 53, loss is 0.09437920153141022\n",
      "epoch: 17 step: 54, loss is 0.053845085203647614\n",
      "epoch: 17 step: 55, loss is 0.0462118424475193\n",
      "epoch: 17 step: 56, loss is 0.11383998394012451\n",
      "epoch: 17 step: 57, loss is 0.0886639803647995\n",
      "epoch: 17 step: 58, loss is 0.06089566648006439\n",
      "epoch: 17 step: 59, loss is 0.5825889110565186\n",
      "epoch: 17 step: 60, loss is 0.06369803845882416\n",
      "epoch: 17 step: 61, loss is 0.5711574554443359\n",
      "epoch: 17 step: 62, loss is 0.07242737710475922\n",
      "epoch: 17 step: 63, loss is 0.08293488621711731\n",
      "epoch: 17 step: 64, loss is 0.06147968769073486\n",
      "epoch: 17 step: 65, loss is 0.04815270006656647\n",
      "epoch: 17 step: 66, loss is 0.09063278138637543\n",
      "epoch: 17 step: 67, loss is 0.5139919519424438\n",
      "epoch: 17 step: 68, loss is 0.2277771234512329\n",
      "epoch: 17 step: 69, loss is 0.08299165964126587\n",
      "epoch: 17 step: 70, loss is 0.17906537652015686\n",
      "epoch: 17 step: 71, loss is 0.43365418910980225\n",
      "epoch: 17 step: 72, loss is 0.17681056261062622\n",
      "epoch: 17 step: 73, loss is 0.2999655604362488\n",
      "epoch: 17 step: 74, loss is 0.07564171403646469\n",
      "epoch: 17 step: 75, loss is 0.47605907917022705\n",
      "epoch: 17 step: 76, loss is 0.050855498760938644\n",
      "epoch: 17 step: 77, loss is 0.03155212849378586\n",
      "epoch: 17 step: 78, loss is 0.0938601866364479\n",
      "epoch: 17 step: 79, loss is 0.42014482617378235\n",
      "epoch: 17 step: 80, loss is 0.29787683486938477\n",
      "epoch: 17 step: 81, loss is 0.03847229480743408\n",
      "epoch: 17 step: 82, loss is 0.10511492192745209\n",
      "epoch: 17 step: 83, loss is 0.16413846611976624\n",
      "epoch: 17 step: 84, loss is 0.03553958237171173\n",
      "{'accuracy': 0.9411764705882353, 'macro f1': 0.9168134664317871, 'weighted f1': 0.9385283179625346, 'macro recall': 0.8863636363636364, 'macro precision': 0.9632352941176471}\n",
      "Eval result: epoch 17, metrics: {'metrics': 0.9268972168753264}\n",
      "epoch: 18 step: 1, loss is 0.1420820951461792\n",
      "epoch: 18 step: 2, loss is 0.09367533028125763\n",
      "epoch: 18 step: 3, loss is 0.038368772715330124\n",
      "epoch: 18 step: 4, loss is 0.1489771157503128\n",
      "epoch: 18 step: 5, loss is 0.06069573014974594\n",
      "epoch: 18 step: 6, loss is 0.5065467357635498\n",
      "epoch: 18 step: 7, loss is 0.05759873241186142\n",
      "epoch: 18 step: 8, loss is 0.04319271445274353\n",
      "epoch: 18 step: 9, loss is 0.07644794881343842\n",
      "epoch: 18 step: 10, loss is 0.6665074825286865\n",
      "epoch: 18 step: 11, loss is 0.043661583214998245\n",
      "epoch: 18 step: 12, loss is 0.15620559453964233\n",
      "epoch: 18 step: 13, loss is 0.05200158432126045\n",
      "epoch: 18 step: 14, loss is 0.5798274278640747\n",
      "epoch: 18 step: 15, loss is 0.0573461689054966\n",
      "epoch: 18 step: 16, loss is 0.08162622898817062\n",
      "epoch: 18 step: 17, loss is 0.3470938801765442\n",
      "epoch: 18 step: 18, loss is 0.08362865447998047\n",
      "epoch: 18 step: 19, loss is 0.06979623436927795\n",
      "epoch: 18 step: 20, loss is 0.33626115322113037\n",
      "epoch: 18 step: 21, loss is 0.0661582350730896\n",
      "epoch: 18 step: 22, loss is 0.19261270761489868\n",
      "epoch: 18 step: 23, loss is 0.15898557007312775\n",
      "epoch: 18 step: 24, loss is 0.11783599853515625\n",
      "epoch: 18 step: 25, loss is 0.11153428256511688\n",
      "epoch: 18 step: 26, loss is 0.06288592517375946\n",
      "epoch: 18 step: 27, loss is 0.16954222321510315\n",
      "epoch: 18 step: 28, loss is 0.7915102243423462\n",
      "epoch: 18 step: 29, loss is 0.8611544966697693\n",
      "epoch: 18 step: 30, loss is 0.03712235018610954\n",
      "epoch: 18 step: 31, loss is 0.07668415457010269\n",
      "epoch: 18 step: 32, loss is 0.8620855212211609\n",
      "epoch: 18 step: 33, loss is 0.05292993783950806\n",
      "epoch: 18 step: 34, loss is 0.1526382565498352\n",
      "epoch: 18 step: 35, loss is 0.6387492418289185\n",
      "epoch: 18 step: 36, loss is 0.2986729145050049\n",
      "epoch: 18 step: 37, loss is 0.3897037208080292\n",
      "epoch: 18 step: 38, loss is 0.05370265245437622\n",
      "epoch: 18 step: 39, loss is 0.2062600702047348\n",
      "epoch: 18 step: 40, loss is 0.44471460580825806\n",
      "epoch: 18 step: 41, loss is 0.09747160971164703\n",
      "epoch: 18 step: 42, loss is 0.5390490293502808\n",
      "epoch: 18 step: 43, loss is 0.055609602481126785\n",
      "epoch: 18 step: 44, loss is 0.17933520674705505\n",
      "epoch: 18 step: 45, loss is 0.05240572616457939\n",
      "epoch: 18 step: 46, loss is 0.06169595196843147\n",
      "epoch: 18 step: 47, loss is 0.2981822192668915\n",
      "epoch: 18 step: 48, loss is 0.16589628159999847\n",
      "epoch: 18 step: 49, loss is 0.0627300888299942\n",
      "epoch: 18 step: 50, loss is 0.25337937474250793\n",
      "epoch: 18 step: 51, loss is 0.42983898520469666\n",
      "epoch: 18 step: 52, loss is 0.16075235605239868\n",
      "epoch: 18 step: 53, loss is 0.13598622381687164\n",
      "epoch: 18 step: 54, loss is 0.08083299547433853\n",
      "epoch: 18 step: 55, loss is 0.06633326411247253\n",
      "epoch: 18 step: 56, loss is 0.09366307407617569\n",
      "epoch: 18 step: 57, loss is 0.38379165530204773\n",
      "epoch: 18 step: 58, loss is 0.26904064416885376\n",
      "epoch: 18 step: 59, loss is 0.0688934326171875\n",
      "epoch: 18 step: 60, loss is 0.07125677168369293\n",
      "epoch: 18 step: 61, loss is 0.05441373586654663\n",
      "epoch: 18 step: 62, loss is 0.07498840242624283\n",
      "epoch: 18 step: 63, loss is 0.275913268327713\n",
      "epoch: 18 step: 64, loss is 0.08037102222442627\n",
      "epoch: 18 step: 65, loss is 0.20058301091194153\n",
      "epoch: 18 step: 66, loss is 0.07422565668821335\n",
      "epoch: 18 step: 67, loss is 0.28635233640670776\n",
      "epoch: 18 step: 68, loss is 0.35965317487716675\n",
      "epoch: 18 step: 69, loss is 0.33215588331222534\n",
      "epoch: 18 step: 70, loss is 0.35384249687194824\n",
      "epoch: 18 step: 71, loss is 0.42960241436958313\n",
      "epoch: 18 step: 72, loss is 0.2387051284313202\n",
      "epoch: 18 step: 73, loss is 0.0590842068195343\n",
      "epoch: 18 step: 74, loss is 0.3201741576194763\n",
      "epoch: 18 step: 75, loss is 0.04627919942140579\n",
      "epoch: 18 step: 76, loss is 0.366443395614624\n",
      "epoch: 18 step: 77, loss is 0.17137566208839417\n",
      "epoch: 18 step: 78, loss is 0.17389708757400513\n",
      "epoch: 18 step: 79, loss is 0.08585642278194427\n",
      "epoch: 18 step: 80, loss is 0.30942678451538086\n",
      "epoch: 18 step: 81, loss is 0.09193506836891174\n",
      "epoch: 18 step: 82, loss is 0.054076142609119415\n",
      "epoch: 18 step: 83, loss is 0.05982661619782448\n",
      "epoch: 18 step: 84, loss is 0.08513998985290527\n",
      "{'accuracy': 0.9411764705882353, 'macro f1': 0.9168134664317871, 'weighted f1': 0.9385283179625346, 'macro recall': 0.8863636363636364, 'macro precision': 0.9632352941176471}\n",
      "Eval result: epoch 18, metrics: {'metrics': 0.9268972168753264}\n",
      "epoch: 19 step: 1, loss is 0.06800232827663422\n",
      "epoch: 19 step: 2, loss is 0.0471145436167717\n",
      "epoch: 19 step: 3, loss is 0.1575050950050354\n",
      "epoch: 19 step: 4, loss is 0.04935113340616226\n",
      "epoch: 19 step: 5, loss is 0.08988846838474274\n",
      "epoch: 19 step: 6, loss is 0.4707748293876648\n",
      "epoch: 19 step: 7, loss is 0.03887486085295677\n",
      "epoch: 19 step: 8, loss is 0.08995512872934341\n",
      "epoch: 19 step: 9, loss is 0.11412041634321213\n",
      "epoch: 19 step: 10, loss is 0.07953595370054245\n",
      "epoch: 19 step: 11, loss is 0.12701678276062012\n",
      "epoch: 19 step: 12, loss is 0.08355842530727386\n",
      "epoch: 19 step: 13, loss is 0.08602587878704071\n",
      "epoch: 19 step: 14, loss is 0.05953827127814293\n",
      "epoch: 19 step: 15, loss is 0.04781884700059891\n",
      "epoch: 19 step: 16, loss is 0.04608381539583206\n",
      "epoch: 19 step: 17, loss is 0.07578808814287186\n",
      "epoch: 19 step: 18, loss is 0.09141618013381958\n",
      "epoch: 19 step: 19, loss is 0.398455947637558\n",
      "epoch: 19 step: 20, loss is 0.05277227610349655\n",
      "epoch: 19 step: 21, loss is 0.23567615449428558\n",
      "epoch: 19 step: 22, loss is 0.059779904782772064\n",
      "epoch: 19 step: 23, loss is 0.09757888317108154\n",
      "epoch: 19 step: 24, loss is 0.8809534311294556\n",
      "epoch: 19 step: 25, loss is 0.050802282989025116\n",
      "epoch: 19 step: 26, loss is 0.1682790070772171\n",
      "epoch: 19 step: 27, loss is 0.06410673260688782\n",
      "epoch: 19 step: 28, loss is 0.047767672687768936\n",
      "epoch: 19 step: 29, loss is 0.044441159814596176\n",
      "epoch: 19 step: 30, loss is 0.04011791571974754\n",
      "epoch: 19 step: 31, loss is 0.09670089930295944\n",
      "epoch: 19 step: 32, loss is 0.07915756106376648\n",
      "epoch: 19 step: 33, loss is 0.04686220362782478\n",
      "epoch: 19 step: 34, loss is 0.33876660466194153\n",
      "epoch: 19 step: 35, loss is 0.151249960064888\n",
      "epoch: 19 step: 36, loss is 0.04826650023460388\n",
      "epoch: 19 step: 37, loss is 0.5527853965759277\n",
      "epoch: 19 step: 38, loss is 0.054800860583782196\n",
      "epoch: 19 step: 39, loss is 0.04979758709669113\n",
      "epoch: 19 step: 40, loss is 0.18937508761882782\n",
      "epoch: 19 step: 41, loss is 0.3380231559276581\n",
      "epoch: 19 step: 42, loss is 0.37666547298431396\n",
      "epoch: 19 step: 43, loss is 0.052699849009513855\n",
      "epoch: 19 step: 44, loss is 0.006963743828237057\n",
      "epoch: 19 step: 45, loss is 0.06188291311264038\n",
      "epoch: 19 step: 46, loss is 0.6570534706115723\n",
      "epoch: 19 step: 47, loss is 0.9313241839408875\n",
      "epoch: 19 step: 48, loss is 0.05218459665775299\n",
      "epoch: 19 step: 49, loss is 0.07537578791379929\n",
      "epoch: 19 step: 50, loss is 0.14963144063949585\n",
      "epoch: 19 step: 51, loss is 0.26851868629455566\n",
      "epoch: 19 step: 52, loss is 0.23622079193592072\n",
      "epoch: 19 step: 53, loss is 0.32356932759284973\n",
      "epoch: 19 step: 54, loss is 0.06530061364173889\n",
      "epoch: 19 step: 55, loss is 0.2533226013183594\n",
      "epoch: 19 step: 56, loss is 0.48805737495422363\n",
      "epoch: 19 step: 57, loss is 0.061319500207901\n",
      "epoch: 19 step: 58, loss is 0.3456127643585205\n",
      "epoch: 19 step: 59, loss is 0.1409277617931366\n",
      "epoch: 19 step: 60, loss is 0.6988163590431213\n",
      "epoch: 19 step: 61, loss is 0.0700070708990097\n",
      "epoch: 19 step: 62, loss is 0.3039398491382599\n",
      "epoch: 19 step: 63, loss is 0.11004188656806946\n",
      "epoch: 19 step: 64, loss is 0.29859858751296997\n",
      "epoch: 19 step: 65, loss is 0.07134663313627243\n",
      "epoch: 19 step: 66, loss is 0.16079220175743103\n",
      "epoch: 19 step: 67, loss is 0.574492871761322\n",
      "epoch: 19 step: 68, loss is 0.04985348507761955\n",
      "epoch: 19 step: 69, loss is 0.030027929693460464\n",
      "epoch: 19 step: 70, loss is 0.5501735806465149\n",
      "epoch: 19 step: 71, loss is 0.49441033601760864\n",
      "epoch: 19 step: 72, loss is 0.08555532991886139\n",
      "epoch: 19 step: 73, loss is 0.07342725247144699\n",
      "epoch: 19 step: 74, loss is 0.03429185226559639\n",
      "epoch: 19 step: 75, loss is 0.05682096257805824\n",
      "epoch: 19 step: 76, loss is 0.053475938737392426\n",
      "epoch: 19 step: 77, loss is 0.328802227973938\n",
      "epoch: 19 step: 78, loss is 0.5390827059745789\n",
      "epoch: 19 step: 79, loss is 0.17707911133766174\n",
      "epoch: 19 step: 80, loss is 0.05080726742744446\n",
      "epoch: 19 step: 81, loss is 0.05530514568090439\n",
      "epoch: 19 step: 82, loss is 0.05924231559038162\n",
      "epoch: 19 step: 83, loss is 0.3053210973739624\n",
      "epoch: 19 step: 84, loss is 0.566171407699585\n",
      "{'accuracy': 0.9411764705882353, 'macro f1': 0.9168134664317871, 'weighted f1': 0.9385283179625346, 'macro recall': 0.8863636363636364, 'macro precision': 0.9632352941176471}\n",
      "Eval result: epoch 19, metrics: {'metrics': 0.9268972168753264}\n",
      "epoch: 20 step: 1, loss is 0.05641314759850502\n",
      "epoch: 20 step: 2, loss is 0.18165266513824463\n",
      "epoch: 20 step: 3, loss is 0.06828524172306061\n",
      "epoch: 20 step: 4, loss is 0.2667914032936096\n",
      "epoch: 20 step: 5, loss is 0.36242562532424927\n",
      "epoch: 20 step: 6, loss is 0.06814286857843399\n",
      "epoch: 20 step: 7, loss is 0.12791812419891357\n",
      "epoch: 20 step: 8, loss is 0.035803623497486115\n",
      "epoch: 20 step: 9, loss is 0.0720396488904953\n",
      "epoch: 20 step: 10, loss is 0.07657994329929352\n",
      "epoch: 20 step: 11, loss is 0.433067262172699\n",
      "epoch: 20 step: 12, loss is 0.34007522463798523\n",
      "epoch: 20 step: 13, loss is 0.34525054693222046\n",
      "epoch: 20 step: 14, loss is 0.14017006754875183\n",
      "epoch: 20 step: 15, loss is 0.054381534457206726\n",
      "epoch: 20 step: 16, loss is 0.03709212318062782\n",
      "epoch: 20 step: 17, loss is 0.42106541991233826\n",
      "epoch: 20 step: 18, loss is 0.09620895981788635\n",
      "epoch: 20 step: 19, loss is 0.09922385215759277\n",
      "epoch: 20 step: 20, loss is 0.707665205001831\n",
      "epoch: 20 step: 21, loss is 0.05555104836821556\n",
      "epoch: 20 step: 22, loss is 0.06817841529846191\n",
      "epoch: 20 step: 23, loss is 0.30846700072288513\n",
      "epoch: 20 step: 24, loss is 0.06380175799131393\n",
      "epoch: 20 step: 25, loss is 0.06096244975924492\n",
      "epoch: 20 step: 26, loss is 0.05105293542146683\n",
      "epoch: 20 step: 27, loss is 0.041178252547979355\n",
      "epoch: 20 step: 28, loss is 0.11804471909999847\n",
      "epoch: 20 step: 29, loss is 0.039569657295942307\n",
      "epoch: 20 step: 30, loss is 0.42056357860565186\n",
      "epoch: 20 step: 31, loss is 0.5226056575775146\n",
      "epoch: 20 step: 32, loss is 0.06286855041980743\n",
      "epoch: 20 step: 33, loss is 0.6401862502098083\n",
      "epoch: 20 step: 34, loss is 0.08654386550188065\n",
      "epoch: 20 step: 35, loss is 0.0517745167016983\n",
      "epoch: 20 step: 36, loss is 0.2932533323764801\n",
      "epoch: 20 step: 37, loss is 0.039700787514448166\n",
      "epoch: 20 step: 38, loss is 0.06647711247205734\n",
      "epoch: 20 step: 39, loss is 0.0499008372426033\n",
      "epoch: 20 step: 40, loss is 0.049924612045288086\n",
      "epoch: 20 step: 41, loss is 0.06357782334089279\n",
      "epoch: 20 step: 42, loss is 0.06530267000198364\n",
      "epoch: 20 step: 43, loss is 0.30689096450805664\n",
      "epoch: 20 step: 44, loss is 0.0643284022808075\n",
      "epoch: 20 step: 45, loss is 0.037395600229501724\n",
      "epoch: 20 step: 46, loss is 0.07211200892925262\n",
      "epoch: 20 step: 47, loss is 0.3050614893436432\n",
      "epoch: 20 step: 48, loss is 0.033387236297130585\n",
      "epoch: 20 step: 49, loss is 0.0625164583325386\n",
      "epoch: 20 step: 50, loss is 0.0314650759100914\n",
      "epoch: 20 step: 51, loss is 0.07560260593891144\n",
      "epoch: 20 step: 52, loss is 0.19062724709510803\n",
      "epoch: 20 step: 53, loss is 0.20177289843559265\n",
      "epoch: 20 step: 54, loss is 0.10938379168510437\n",
      "epoch: 20 step: 55, loss is 0.12691086530685425\n",
      "epoch: 20 step: 56, loss is 0.07427513599395752\n",
      "epoch: 20 step: 57, loss is 0.15185648202896118\n",
      "epoch: 20 step: 58, loss is 0.03387438878417015\n",
      "epoch: 20 step: 59, loss is 0.3669099509716034\n",
      "epoch: 20 step: 60, loss is 0.05512997508049011\n",
      "epoch: 20 step: 61, loss is 0.07687919586896896\n",
      "epoch: 20 step: 62, loss is 0.056982334703207016\n",
      "epoch: 20 step: 63, loss is 0.07160691916942596\n",
      "epoch: 20 step: 64, loss is 0.04398765414953232\n",
      "epoch: 20 step: 65, loss is 0.4077639579772949\n",
      "epoch: 20 step: 66, loss is 0.7992641925811768\n",
      "epoch: 20 step: 67, loss is 0.25928938388824463\n",
      "epoch: 20 step: 68, loss is 0.04792740195989609\n",
      "epoch: 20 step: 69, loss is 0.2355174571275711\n",
      "epoch: 20 step: 70, loss is 0.09051547944545746\n",
      "epoch: 20 step: 71, loss is 0.18229907751083374\n",
      "epoch: 20 step: 72, loss is 0.027071304619312286\n",
      "epoch: 20 step: 73, loss is 0.11849833279848099\n",
      "epoch: 20 step: 74, loss is 0.06196955218911171\n",
      "epoch: 20 step: 75, loss is 0.07551562041044235\n",
      "epoch: 20 step: 76, loss is 0.2834630608558655\n",
      "epoch: 20 step: 77, loss is 0.42299142479896545\n",
      "epoch: 20 step: 78, loss is 0.37768101692199707\n",
      "epoch: 20 step: 79, loss is 0.229213148355484\n",
      "epoch: 20 step: 80, loss is 0.05794423446059227\n",
      "epoch: 20 step: 81, loss is 0.1009574830532074\n",
      "epoch: 20 step: 82, loss is 0.6853679418563843\n",
      "epoch: 20 step: 83, loss is 0.04642774909734726\n",
      "epoch: 20 step: 84, loss is 0.5137180089950562\n",
      "{'accuracy': 0.9411764705882353, 'macro f1': 0.9168134664317871, 'weighted f1': 0.9385283179625346, 'macro recall': 0.8863636363636364, 'macro precision': 0.9632352941176471}\n",
      "Eval result: epoch 20, metrics: {'metrics': 0.9268972168753264}\n",
      "epoch: 21 step: 1, loss is 0.046988219022750854\n",
      "epoch: 21 step: 2, loss is 0.3855847418308258\n",
      "epoch: 21 step: 3, loss is 0.10780066251754761\n",
      "epoch: 21 step: 4, loss is 0.12168492376804352\n",
      "epoch: 21 step: 5, loss is 0.05250680446624756\n",
      "epoch: 21 step: 6, loss is 0.11664867401123047\n",
      "epoch: 21 step: 7, loss is 0.3558252453804016\n",
      "epoch: 21 step: 8, loss is 0.43265071511268616\n",
      "epoch: 21 step: 9, loss is 0.04401618242263794\n",
      "epoch: 21 step: 10, loss is 0.06689286977052689\n",
      "epoch: 21 step: 11, loss is 0.09727286547422409\n",
      "epoch: 21 step: 12, loss is 0.06311748921871185\n",
      "epoch: 21 step: 13, loss is 0.05363541468977928\n",
      "epoch: 21 step: 14, loss is 0.4638383090496063\n",
      "epoch: 21 step: 15, loss is 0.03619580715894699\n",
      "epoch: 21 step: 16, loss is 0.24019822478294373\n",
      "epoch: 21 step: 17, loss is 0.2139706015586853\n",
      "epoch: 21 step: 18, loss is 0.23562118411064148\n",
      "epoch: 21 step: 19, loss is 0.03250379487872124\n",
      "epoch: 21 step: 20, loss is 0.07264000177383423\n",
      "epoch: 21 step: 21, loss is 0.11144143342971802\n",
      "epoch: 21 step: 22, loss is 0.32414790987968445\n",
      "epoch: 21 step: 23, loss is 0.08043082058429718\n",
      "epoch: 21 step: 24, loss is 0.13433203101158142\n",
      "epoch: 21 step: 25, loss is 0.13868887722492218\n",
      "epoch: 21 step: 26, loss is 0.031650085002183914\n",
      "epoch: 21 step: 27, loss is 0.37201982736587524\n",
      "epoch: 21 step: 28, loss is 0.6590037941932678\n",
      "epoch: 21 step: 29, loss is 0.08511616289615631\n",
      "epoch: 21 step: 30, loss is 0.24096333980560303\n",
      "epoch: 21 step: 31, loss is 0.15520939230918884\n",
      "epoch: 21 step: 32, loss is 0.07781435549259186\n",
      "epoch: 21 step: 33, loss is 0.08549191057682037\n",
      "epoch: 21 step: 34, loss is 0.09779852628707886\n",
      "epoch: 21 step: 35, loss is 0.06324930489063263\n",
      "epoch: 21 step: 36, loss is 0.29772859811782837\n",
      "epoch: 21 step: 37, loss is 0.05343044549226761\n",
      "epoch: 21 step: 38, loss is 0.023702340200543404\n",
      "epoch: 21 step: 39, loss is 0.1664167195558548\n",
      "epoch: 21 step: 40, loss is 0.19715848565101624\n",
      "epoch: 21 step: 41, loss is 0.5568782091140747\n",
      "epoch: 21 step: 42, loss is 0.309923380613327\n",
      "epoch: 21 step: 43, loss is 0.06767738610506058\n",
      "epoch: 21 step: 44, loss is 0.1281900256872177\n",
      "epoch: 21 step: 45, loss is 0.09947369247674942\n",
      "epoch: 21 step: 46, loss is 0.09240289777517319\n",
      "epoch: 21 step: 47, loss is 0.05015859380364418\n",
      "epoch: 21 step: 48, loss is 0.13914698362350464\n",
      "epoch: 21 step: 49, loss is 0.08390019834041595\n",
      "epoch: 21 step: 50, loss is 0.03648898005485535\n",
      "epoch: 21 step: 51, loss is 0.09517385065555573\n",
      "epoch: 21 step: 52, loss is 0.09001817554235458\n",
      "epoch: 21 step: 53, loss is 0.622248113155365\n",
      "epoch: 21 step: 54, loss is 0.07839706540107727\n",
      "epoch: 21 step: 55, loss is 0.46905750036239624\n",
      "epoch: 21 step: 56, loss is 0.0796576738357544\n",
      "epoch: 21 step: 57, loss is 0.3334497809410095\n",
      "epoch: 21 step: 58, loss is 0.0462351068854332\n",
      "epoch: 21 step: 59, loss is 0.028778742998838425\n",
      "epoch: 21 step: 60, loss is 0.16788506507873535\n",
      "epoch: 21 step: 61, loss is 0.05277707800269127\n",
      "epoch: 21 step: 62, loss is 0.5220506191253662\n",
      "epoch: 21 step: 63, loss is 0.22830919921398163\n",
      "epoch: 21 step: 64, loss is 0.14148244261741638\n",
      "epoch: 21 step: 65, loss is 0.050417814403772354\n",
      "epoch: 21 step: 66, loss is 0.03425528481602669\n",
      "epoch: 21 step: 67, loss is 0.03744944930076599\n",
      "epoch: 21 step: 68, loss is 0.0998820811510086\n",
      "epoch: 21 step: 69, loss is 0.36631232500076294\n",
      "epoch: 21 step: 70, loss is 0.5698080062866211\n",
      "epoch: 21 step: 71, loss is 0.03998216986656189\n",
      "epoch: 21 step: 72, loss is 0.0624111071228981\n",
      "epoch: 21 step: 73, loss is 0.03574619069695473\n",
      "epoch: 21 step: 74, loss is 0.12199302017688751\n",
      "epoch: 21 step: 75, loss is 0.1276882141828537\n",
      "epoch: 21 step: 76, loss is 0.49787044525146484\n",
      "epoch: 21 step: 77, loss is 0.08323092013597488\n",
      "epoch: 21 step: 78, loss is 0.1782800704240799\n",
      "epoch: 21 step: 79, loss is 0.08441482484340668\n",
      "epoch: 21 step: 80, loss is 0.44987255334854126\n",
      "epoch: 21 step: 81, loss is 0.033722713589668274\n",
      "epoch: 21 step: 82, loss is 0.23110640048980713\n",
      "epoch: 21 step: 83, loss is 0.07340194284915924\n",
      "epoch: 21 step: 84, loss is 0.09192806482315063\n",
      "{'accuracy': 0.9294117647058824, 'macro f1': 0.8983253588516746, 'weighted f1': 0.925443287362792, 'macro recall': 0.8636363636363636, 'macro precision': 0.9565217391304348}\n",
      "Eval result: epoch 21, metrics: {'metrics': 0.911973806581089}\n",
      "epoch: 22 step: 1, loss is 0.11793054640293121\n",
      "epoch: 22 step: 2, loss is 0.21956613659858704\n",
      "epoch: 22 step: 3, loss is 0.05797693505883217\n",
      "epoch: 22 step: 4, loss is 0.2027239203453064\n",
      "epoch: 22 step: 5, loss is 0.14113359153270721\n",
      "epoch: 22 step: 6, loss is 0.023960836231708527\n",
      "epoch: 22 step: 7, loss is 0.08349528908729553\n",
      "epoch: 22 step: 8, loss is 0.5215706825256348\n",
      "epoch: 22 step: 9, loss is 0.04894439876079559\n",
      "epoch: 22 step: 10, loss is 0.042974844574928284\n",
      "epoch: 22 step: 11, loss is 0.12205207347869873\n",
      "epoch: 22 step: 12, loss is 0.23499326407909393\n",
      "epoch: 22 step: 13, loss is 0.04580563306808472\n",
      "epoch: 22 step: 14, loss is 0.4128777086734772\n",
      "epoch: 22 step: 15, loss is 0.6052358746528625\n",
      "epoch: 22 step: 16, loss is 0.2585168182849884\n",
      "epoch: 22 step: 17, loss is 0.3130507469177246\n",
      "epoch: 22 step: 18, loss is 0.05692918598651886\n",
      "epoch: 22 step: 19, loss is 0.05037355422973633\n",
      "epoch: 22 step: 20, loss is 0.032508838921785355\n",
      "epoch: 22 step: 21, loss is 0.11522487550973892\n",
      "epoch: 22 step: 22, loss is 0.07020653784275055\n",
      "epoch: 22 step: 23, loss is 0.08456937968730927\n",
      "epoch: 22 step: 24, loss is 0.49299249053001404\n",
      "epoch: 22 step: 25, loss is 0.06913267076015472\n",
      "epoch: 22 step: 26, loss is 0.3086302876472473\n",
      "epoch: 22 step: 27, loss is 0.1191846951842308\n",
      "epoch: 22 step: 28, loss is 0.04933789372444153\n",
      "epoch: 22 step: 29, loss is 0.2198280692100525\n",
      "epoch: 22 step: 30, loss is 0.055078305304050446\n",
      "epoch: 22 step: 31, loss is 0.058110810816287994\n",
      "epoch: 22 step: 32, loss is 0.11067565530538559\n",
      "epoch: 22 step: 33, loss is 0.027417844161391258\n",
      "epoch: 22 step: 34, loss is 0.033424533903598785\n",
      "epoch: 22 step: 35, loss is 0.93868488073349\n",
      "epoch: 22 step: 36, loss is 0.12062055617570877\n",
      "epoch: 22 step: 37, loss is 0.05619127303361893\n",
      "epoch: 22 step: 38, loss is 0.05445079505443573\n",
      "epoch: 22 step: 39, loss is 0.15973323583602905\n",
      "epoch: 22 step: 40, loss is 0.15826943516731262\n",
      "epoch: 22 step: 41, loss is 0.17681129276752472\n",
      "epoch: 22 step: 42, loss is 0.11841587722301483\n",
      "epoch: 22 step: 43, loss is 0.07573153078556061\n",
      "epoch: 22 step: 44, loss is 0.12674856185913086\n",
      "epoch: 22 step: 45, loss is 0.04889344051480293\n",
      "epoch: 22 step: 46, loss is 0.13022136688232422\n",
      "epoch: 22 step: 47, loss is 0.17801906168460846\n",
      "epoch: 22 step: 48, loss is 0.04797650873661041\n",
      "epoch: 22 step: 49, loss is 0.01780589297413826\n",
      "epoch: 22 step: 50, loss is 0.40866202116012573\n",
      "epoch: 22 step: 51, loss is 0.2698078155517578\n",
      "epoch: 22 step: 52, loss is 0.04678251966834068\n",
      "epoch: 22 step: 53, loss is 0.12339921295642853\n",
      "epoch: 22 step: 54, loss is 0.04102538526058197\n",
      "epoch: 22 step: 55, loss is 0.0329185426235199\n",
      "epoch: 22 step: 56, loss is 0.0694817379117012\n",
      "epoch: 22 step: 57, loss is 0.3065100610256195\n",
      "epoch: 22 step: 58, loss is 0.08711014688014984\n",
      "epoch: 22 step: 59, loss is 0.2035776525735855\n",
      "epoch: 22 step: 60, loss is 0.28215739130973816\n",
      "epoch: 22 step: 61, loss is 0.04296119511127472\n",
      "epoch: 22 step: 62, loss is 0.044160425662994385\n",
      "epoch: 22 step: 63, loss is 0.6021413803100586\n",
      "epoch: 22 step: 64, loss is 0.019056323915719986\n",
      "epoch: 22 step: 65, loss is 0.050859786570072174\n",
      "epoch: 22 step: 66, loss is 0.07569961249828339\n",
      "epoch: 22 step: 67, loss is 0.19461408257484436\n",
      "epoch: 22 step: 68, loss is 0.07107006013393402\n",
      "epoch: 22 step: 69, loss is 0.03735680505633354\n",
      "epoch: 22 step: 70, loss is 0.016158122569322586\n",
      "epoch: 22 step: 71, loss is 0.07456439733505249\n",
      "epoch: 22 step: 72, loss is 0.4532458484172821\n",
      "epoch: 22 step: 73, loss is 0.06040000542998314\n",
      "epoch: 22 step: 74, loss is 0.03181202709674835\n",
      "epoch: 22 step: 75, loss is 0.6828274130821228\n",
      "epoch: 22 step: 76, loss is 0.040823981165885925\n",
      "epoch: 22 step: 77, loss is 0.06987524032592773\n",
      "epoch: 22 step: 78, loss is 0.0344080775976181\n",
      "epoch: 22 step: 79, loss is 0.06164627894759178\n",
      "epoch: 22 step: 80, loss is 0.03914002701640129\n",
      "epoch: 22 step: 81, loss is 0.0877520889043808\n",
      "epoch: 22 step: 82, loss is 0.1613079160451889\n",
      "epoch: 22 step: 83, loss is 0.22509287297725677\n",
      "epoch: 22 step: 84, loss is 0.056969478726387024\n",
      "{'accuracy': 0.9294117647058824, 'macro f1': 0.8983253588516746, 'weighted f1': 0.925443287362792, 'macro recall': 0.8636363636363636, 'macro precision': 0.9565217391304348}\n",
      "Eval result: epoch 22, metrics: {'metrics': 0.911973806581089}\n",
      "epoch: 23 step: 1, loss is 0.012118160724639893\n",
      "epoch: 23 step: 2, loss is 0.09960082918405533\n",
      "epoch: 23 step: 3, loss is 0.038655590265989304\n",
      "epoch: 23 step: 4, loss is 0.02523678168654442\n",
      "epoch: 23 step: 5, loss is 0.037843771278858185\n",
      "epoch: 23 step: 6, loss is 0.08494368195533752\n",
      "epoch: 23 step: 7, loss is 0.06361875683069229\n",
      "epoch: 23 step: 8, loss is 0.04158935695886612\n",
      "epoch: 23 step: 9, loss is 0.015398301184177399\n",
      "epoch: 23 step: 10, loss is 0.2660851776599884\n",
      "epoch: 23 step: 11, loss is 0.4199133515357971\n",
      "epoch: 23 step: 12, loss is 0.030443290248513222\n",
      "epoch: 23 step: 13, loss is 0.029055677354335785\n",
      "epoch: 23 step: 14, loss is 0.056871455162763596\n",
      "epoch: 23 step: 15, loss is 0.5154836177825928\n",
      "epoch: 23 step: 16, loss is 0.4560094475746155\n",
      "epoch: 23 step: 17, loss is 0.31724292039871216\n",
      "epoch: 23 step: 18, loss is 0.04506359621882439\n",
      "epoch: 23 step: 19, loss is 0.02366090565919876\n",
      "epoch: 23 step: 20, loss is 0.05822663754224777\n",
      "epoch: 23 step: 21, loss is 0.46376222372055054\n",
      "epoch: 23 step: 22, loss is 0.0724254697561264\n",
      "epoch: 23 step: 23, loss is 0.08954381942749023\n",
      "epoch: 23 step: 24, loss is 0.04457184672355652\n",
      "epoch: 23 step: 25, loss is 0.0353570356965065\n",
      "epoch: 23 step: 26, loss is 0.07488316297531128\n",
      "epoch: 23 step: 27, loss is 0.07166062295436859\n",
      "epoch: 23 step: 28, loss is 0.034775447100400925\n",
      "epoch: 23 step: 29, loss is 0.28343141078948975\n",
      "epoch: 23 step: 30, loss is 0.13281098008155823\n",
      "epoch: 23 step: 31, loss is 0.025465266779065132\n",
      "epoch: 23 step: 32, loss is 0.11838072538375854\n",
      "epoch: 23 step: 33, loss is 0.02916250377893448\n",
      "epoch: 23 step: 34, loss is 0.06492994725704193\n",
      "epoch: 23 step: 35, loss is 0.050863754004240036\n",
      "epoch: 23 step: 36, loss is 0.11898940801620483\n",
      "epoch: 23 step: 37, loss is 0.03487470746040344\n",
      "epoch: 23 step: 38, loss is 0.09367366135120392\n",
      "epoch: 23 step: 39, loss is 0.8272783756256104\n",
      "epoch: 23 step: 40, loss is 0.127528116106987\n",
      "epoch: 23 step: 41, loss is 0.17005586624145508\n",
      "epoch: 23 step: 42, loss is 0.07379229366779327\n",
      "epoch: 23 step: 43, loss is 0.03169378265738487\n",
      "epoch: 23 step: 44, loss is 0.3143927752971649\n",
      "epoch: 23 step: 45, loss is 0.046900488436222076\n",
      "epoch: 23 step: 46, loss is 0.028383485972881317\n",
      "epoch: 23 step: 47, loss is 0.05951729416847229\n",
      "epoch: 23 step: 48, loss is 0.02861359901726246\n",
      "epoch: 23 step: 49, loss is 0.05945770442485809\n",
      "epoch: 23 step: 50, loss is 0.03382166475057602\n",
      "epoch: 23 step: 51, loss is 0.3167743682861328\n",
      "epoch: 23 step: 52, loss is 0.44825443625450134\n",
      "epoch: 23 step: 53, loss is 0.5886802673339844\n",
      "epoch: 23 step: 54, loss is 0.025428930297493935\n",
      "epoch: 23 step: 55, loss is 0.059615813195705414\n",
      "epoch: 23 step: 56, loss is 0.02088775858283043\n",
      "epoch: 23 step: 57, loss is 0.04700138419866562\n",
      "epoch: 23 step: 58, loss is 0.04723164439201355\n",
      "epoch: 23 step: 59, loss is 0.0490865521132946\n",
      "epoch: 23 step: 60, loss is 0.12641943991184235\n",
      "epoch: 23 step: 61, loss is 0.030858345329761505\n",
      "epoch: 23 step: 62, loss is 0.041697707027196884\n",
      "epoch: 23 step: 63, loss is 0.2178889364004135\n",
      "epoch: 23 step: 64, loss is 0.06773309409618378\n",
      "epoch: 23 step: 65, loss is 0.012906007468700409\n",
      "epoch: 23 step: 66, loss is 0.0659179762005806\n",
      "epoch: 23 step: 67, loss is 0.697092592716217\n",
      "epoch: 23 step: 68, loss is 0.047384195029735565\n",
      "epoch: 23 step: 69, loss is 0.46622881293296814\n",
      "epoch: 23 step: 70, loss is 0.08437713235616684\n",
      "epoch: 23 step: 71, loss is 0.048726122826337814\n",
      "epoch: 23 step: 72, loss is 0.06430486589670181\n",
      "epoch: 23 step: 73, loss is 0.6362443566322327\n",
      "epoch: 23 step: 74, loss is 0.0610959455370903\n",
      "epoch: 23 step: 75, loss is 0.6421163082122803\n",
      "epoch: 23 step: 76, loss is 0.47449907660484314\n",
      "epoch: 23 step: 77, loss is 0.042485740035772324\n",
      "epoch: 23 step: 78, loss is 0.29532650113105774\n",
      "epoch: 23 step: 79, loss is 0.09124766290187836\n",
      "epoch: 23 step: 80, loss is 0.332833468914032\n",
      "epoch: 23 step: 81, loss is 0.0261138454079628\n",
      "epoch: 23 step: 82, loss is 0.14098620414733887\n",
      "epoch: 23 step: 83, loss is 0.04949960857629776\n",
      "epoch: 23 step: 84, loss is 0.1509229987859726\n",
      "{'accuracy': 0.9176470588235294, 'macro f1': 0.8835388530045019, 'weighted f1': 0.9139396451475482, 'macro recall': 0.8556998556998556, 'macro precision': 0.9264705882352942}\n",
      "Eval result: epoch 23, metrics: {'metrics': 0.8958390889407952}\n",
      "epoch: 24 step: 1, loss is 0.0545925498008728\n",
      "epoch: 24 step: 2, loss is 0.019219975918531418\n",
      "epoch: 24 step: 3, loss is 0.029157623648643494\n",
      "epoch: 24 step: 4, loss is 0.24689608812332153\n",
      "epoch: 24 step: 5, loss is 0.015316582284867764\n",
      "epoch: 24 step: 6, loss is 0.07428619265556335\n",
      "epoch: 24 step: 7, loss is 0.09233331680297852\n",
      "epoch: 24 step: 8, loss is 0.03652198240160942\n",
      "epoch: 24 step: 9, loss is 0.45809346437454224\n",
      "epoch: 24 step: 10, loss is 0.11677321791648865\n",
      "epoch: 24 step: 11, loss is 0.04689674824476242\n",
      "epoch: 24 step: 12, loss is 0.034148529171943665\n",
      "epoch: 24 step: 13, loss is 0.03748685121536255\n",
      "epoch: 24 step: 14, loss is 0.0444069430232048\n",
      "epoch: 24 step: 15, loss is 0.14470313489437103\n",
      "epoch: 24 step: 16, loss is 0.48470473289489746\n",
      "epoch: 24 step: 17, loss is 0.8060437440872192\n",
      "epoch: 24 step: 18, loss is 0.04637365788221359\n",
      "epoch: 24 step: 19, loss is 0.022128887474536896\n",
      "epoch: 24 step: 20, loss is 0.04480552673339844\n",
      "epoch: 24 step: 21, loss is 0.23648099601268768\n",
      "epoch: 24 step: 22, loss is 0.08115512877702713\n",
      "epoch: 24 step: 23, loss is 0.054144278168678284\n",
      "epoch: 24 step: 24, loss is 0.23062725365161896\n",
      "epoch: 24 step: 25, loss is 0.13478565216064453\n",
      "epoch: 24 step: 26, loss is 0.03789281100034714\n",
      "epoch: 24 step: 27, loss is 0.056445665657520294\n",
      "epoch: 24 step: 28, loss is 0.5243420004844666\n",
      "epoch: 24 step: 29, loss is 0.0597120076417923\n",
      "epoch: 24 step: 30, loss is 0.038216426968574524\n",
      "epoch: 24 step: 31, loss is 0.1715158224105835\n",
      "epoch: 24 step: 32, loss is 0.04123976081609726\n",
      "epoch: 24 step: 33, loss is 0.1385808289051056\n",
      "epoch: 24 step: 34, loss is 0.04987073689699173\n",
      "epoch: 24 step: 35, loss is 0.045717980712652206\n",
      "epoch: 24 step: 36, loss is 0.3929027020931244\n",
      "epoch: 24 step: 37, loss is 0.3093790113925934\n",
      "epoch: 24 step: 38, loss is 0.1001272201538086\n",
      "epoch: 24 step: 39, loss is 0.02700582891702652\n",
      "epoch: 24 step: 40, loss is 0.08547599613666534\n",
      "epoch: 24 step: 41, loss is 0.027276618406176567\n",
      "epoch: 24 step: 42, loss is 0.05261441320180893\n",
      "epoch: 24 step: 43, loss is 0.18614423274993896\n",
      "epoch: 24 step: 44, loss is 0.05102448910474777\n",
      "epoch: 24 step: 45, loss is 0.19691185653209686\n",
      "epoch: 24 step: 46, loss is 0.15913358330726624\n",
      "epoch: 24 step: 47, loss is 0.21596798300743103\n",
      "epoch: 24 step: 48, loss is 0.0598912388086319\n",
      "epoch: 24 step: 49, loss is 0.07490323483943939\n",
      "epoch: 24 step: 50, loss is 0.1300024390220642\n",
      "epoch: 24 step: 51, loss is 0.059117093682289124\n",
      "epoch: 24 step: 52, loss is 0.08599444478750229\n",
      "epoch: 24 step: 53, loss is 0.03791050612926483\n",
      "epoch: 24 step: 54, loss is 0.0723678320646286\n",
      "epoch: 24 step: 55, loss is 0.02420772612094879\n",
      "epoch: 24 step: 56, loss is 0.03914419561624527\n",
      "epoch: 24 step: 57, loss is 0.05033647269010544\n",
      "epoch: 24 step: 58, loss is 0.27576565742492676\n",
      "epoch: 24 step: 59, loss is 0.5683017373085022\n",
      "epoch: 24 step: 60, loss is 0.07549974322319031\n",
      "epoch: 24 step: 61, loss is 0.140693798661232\n",
      "epoch: 24 step: 62, loss is 0.05378337204456329\n",
      "epoch: 24 step: 63, loss is 0.03923759236931801\n",
      "epoch: 24 step: 64, loss is 0.6174688339233398\n",
      "epoch: 24 step: 65, loss is 0.01720384508371353\n",
      "epoch: 24 step: 66, loss is 0.0442781001329422\n",
      "epoch: 24 step: 67, loss is 0.3476123809814453\n",
      "epoch: 24 step: 68, loss is 0.3671875\n",
      "epoch: 24 step: 69, loss is 0.037206992506980896\n",
      "epoch: 24 step: 70, loss is 0.034861572086811066\n",
      "epoch: 24 step: 71, loss is 0.062478207051754\n",
      "epoch: 24 step: 72, loss is 0.3593071699142456\n",
      "epoch: 24 step: 73, loss is 0.03736630827188492\n",
      "epoch: 24 step: 74, loss is 0.04496961086988449\n",
      "epoch: 24 step: 75, loss is 0.04109597206115723\n",
      "epoch: 24 step: 76, loss is 0.08686194568872452\n",
      "epoch: 24 step: 77, loss is 0.45582765340805054\n",
      "epoch: 24 step: 78, loss is 0.5152109861373901\n",
      "epoch: 24 step: 79, loss is 0.08289939165115356\n",
      "epoch: 24 step: 80, loss is 0.12258070707321167\n",
      "epoch: 24 step: 81, loss is 0.060566626489162445\n",
      "epoch: 24 step: 82, loss is 0.06603311002254486\n",
      "epoch: 24 step: 83, loss is 0.05558348447084427\n",
      "epoch: 24 step: 84, loss is 0.06212277710437775\n",
      "{'accuracy': 0.9058823529411765, 'macro f1': 0.8692307692307693, 'weighted f1': 0.9026244343891403, 'macro recall': 0.8477633477633477, 'macro precision': 0.8996683250414593}\n",
      "Eval result: epoch 24, metrics: {'metrics': 0.8806361987441881}\n",
      "epoch: 25 step: 1, loss is 0.05656220763921738\n",
      "epoch: 25 step: 2, loss is 0.23453986644744873\n",
      "epoch: 25 step: 3, loss is 0.03792472556233406\n",
      "epoch: 25 step: 4, loss is 0.4519475996494293\n",
      "epoch: 25 step: 5, loss is 0.16411235928535461\n",
      "epoch: 25 step: 6, loss is 0.0431334525346756\n",
      "epoch: 25 step: 7, loss is 0.05418401584029198\n",
      "epoch: 25 step: 8, loss is 0.20345154404640198\n",
      "epoch: 25 step: 9, loss is 0.251787930727005\n",
      "epoch: 25 step: 10, loss is 0.04697130620479584\n",
      "epoch: 25 step: 11, loss is 0.05869058519601822\n",
      "epoch: 25 step: 12, loss is 0.04849408566951752\n",
      "epoch: 25 step: 13, loss is 0.031662557274103165\n",
      "epoch: 25 step: 14, loss is 0.01926386170089245\n",
      "epoch: 25 step: 15, loss is 0.10688664764165878\n",
      "epoch: 25 step: 16, loss is 0.03184410184621811\n",
      "epoch: 25 step: 17, loss is 0.1218336969614029\n",
      "epoch: 25 step: 18, loss is 0.20170901715755463\n",
      "epoch: 25 step: 19, loss is 0.02628401666879654\n",
      "epoch: 25 step: 20, loss is 0.0597519725561142\n",
      "epoch: 25 step: 21, loss is 0.04288637638092041\n",
      "epoch: 25 step: 22, loss is 0.06004990637302399\n",
      "epoch: 25 step: 23, loss is 0.3126376271247864\n",
      "epoch: 25 step: 24, loss is 0.07974062860012054\n",
      "epoch: 25 step: 25, loss is 0.02620677836239338\n",
      "epoch: 25 step: 26, loss is 0.01754230633378029\n",
      "epoch: 25 step: 27, loss is 0.551711916923523\n",
      "epoch: 25 step: 28, loss is 0.6227908730506897\n",
      "epoch: 25 step: 29, loss is 0.1760224997997284\n",
      "epoch: 25 step: 30, loss is 0.026737524196505547\n",
      "epoch: 25 step: 31, loss is 0.3064441680908203\n",
      "epoch: 25 step: 32, loss is 0.4740552604198456\n",
      "epoch: 25 step: 33, loss is 0.03888919949531555\n",
      "epoch: 25 step: 34, loss is 0.2701103985309601\n",
      "epoch: 25 step: 35, loss is 0.12931089103221893\n",
      "epoch: 25 step: 36, loss is 0.032860636711120605\n",
      "epoch: 25 step: 37, loss is 0.023059431463479996\n",
      "epoch: 25 step: 38, loss is 0.10130957514047623\n",
      "epoch: 25 step: 39, loss is 0.36792027950286865\n",
      "epoch: 25 step: 40, loss is 0.2305036336183548\n",
      "epoch: 25 step: 41, loss is 0.033361174166202545\n",
      "epoch: 25 step: 42, loss is 0.03966176137328148\n",
      "epoch: 25 step: 43, loss is 0.1151966005563736\n",
      "epoch: 25 step: 44, loss is 0.1447247862815857\n",
      "epoch: 25 step: 45, loss is 0.18113714456558228\n",
      "epoch: 25 step: 46, loss is 0.11959768086671829\n",
      "epoch: 25 step: 47, loss is 0.16904136538505554\n",
      "epoch: 25 step: 48, loss is 0.09616157412528992\n",
      "epoch: 25 step: 49, loss is 0.028681594878435135\n",
      "epoch: 25 step: 50, loss is 0.06272698938846588\n",
      "epoch: 25 step: 51, loss is 0.16650089621543884\n",
      "epoch: 25 step: 52, loss is 0.08927051723003387\n",
      "epoch: 25 step: 53, loss is 0.061203282326459885\n",
      "epoch: 25 step: 54, loss is 0.09222746640443802\n",
      "epoch: 25 step: 55, loss is 0.04765623062849045\n",
      "epoch: 25 step: 56, loss is 0.06265167891979218\n",
      "epoch: 25 step: 57, loss is 0.038747940212488174\n",
      "epoch: 25 step: 58, loss is 0.03094387799501419\n",
      "epoch: 25 step: 59, loss is 0.07936618477106094\n",
      "epoch: 25 step: 60, loss is 0.03489832207560539\n",
      "epoch: 25 step: 61, loss is 0.03708386421203613\n",
      "epoch: 25 step: 62, loss is 0.07715529203414917\n",
      "epoch: 25 step: 63, loss is 0.04519747197628021\n",
      "epoch: 25 step: 64, loss is 0.09841842949390411\n",
      "epoch: 25 step: 65, loss is 0.050994016230106354\n",
      "epoch: 25 step: 66, loss is 0.03716856613755226\n",
      "epoch: 25 step: 67, loss is 0.09268014132976532\n",
      "epoch: 25 step: 68, loss is 0.03375020623207092\n",
      "epoch: 25 step: 69, loss is 0.24877825379371643\n",
      "epoch: 25 step: 70, loss is 0.26131731271743774\n",
      "epoch: 25 step: 71, loss is 0.02350093238055706\n",
      "epoch: 25 step: 72, loss is 0.009778196923434734\n",
      "epoch: 25 step: 73, loss is 0.2635396718978882\n",
      "epoch: 25 step: 74, loss is 0.07355818897485733\n",
      "epoch: 25 step: 75, loss is 0.27085837721824646\n",
      "epoch: 25 step: 76, loss is 0.05209328234195709\n",
      "epoch: 25 step: 77, loss is 0.0705924779176712\n",
      "epoch: 25 step: 78, loss is 0.056341931223869324\n",
      "epoch: 25 step: 79, loss is 0.7001729607582092\n",
      "epoch: 25 step: 80, loss is 0.06820705533027649\n",
      "epoch: 25 step: 81, loss is 0.19901521503925323\n",
      "epoch: 25 step: 82, loss is 0.23648935556411743\n",
      "epoch: 25 step: 83, loss is 0.034942492842674255\n",
      "epoch: 25 step: 84, loss is 0.01711004227399826\n",
      "{'accuracy': 0.9176470588235294, 'macro f1': 0.8835388530045019, 'weighted f1': 0.9139396451475482, 'macro recall': 0.8556998556998556, 'macro precision': 0.9264705882352942}\n",
      "Eval result: epoch 25, metrics: {'metrics': 0.8958390889407952}\n",
      "epoch: 26 step: 1, loss is 0.05391682684421539\n",
      "epoch: 26 step: 2, loss is 0.5074506402015686\n",
      "epoch: 26 step: 3, loss is 0.07098038494586945\n",
      "epoch: 26 step: 4, loss is 0.03455110639333725\n",
      "epoch: 26 step: 5, loss is 0.03583836555480957\n",
      "epoch: 26 step: 6, loss is 0.024620220065116882\n",
      "epoch: 26 step: 7, loss is 0.032021697610616684\n",
      "epoch: 26 step: 8, loss is 0.45801547169685364\n",
      "epoch: 26 step: 9, loss is 0.05330072343349457\n",
      "epoch: 26 step: 10, loss is 0.038906343281269073\n",
      "epoch: 26 step: 11, loss is 0.0216910969465971\n",
      "epoch: 26 step: 12, loss is 0.2300228625535965\n",
      "epoch: 26 step: 13, loss is 0.08998651802539825\n",
      "epoch: 26 step: 14, loss is 0.014702712185680866\n",
      "epoch: 26 step: 15, loss is 0.033925801515579224\n",
      "epoch: 26 step: 16, loss is 0.03978670760989189\n",
      "epoch: 26 step: 17, loss is 0.03435267508029938\n",
      "epoch: 26 step: 18, loss is 0.026371954008936882\n",
      "epoch: 26 step: 19, loss is 0.020520074293017387\n",
      "epoch: 26 step: 20, loss is 0.017733480781316757\n",
      "epoch: 26 step: 21, loss is 0.09980501234531403\n",
      "epoch: 26 step: 22, loss is 0.3754146993160248\n",
      "epoch: 26 step: 23, loss is 0.018503626808524132\n",
      "epoch: 26 step: 24, loss is 0.012613480910658836\n",
      "epoch: 26 step: 25, loss is 0.049218811094760895\n",
      "epoch: 26 step: 26, loss is 0.1936258226633072\n",
      "epoch: 26 step: 27, loss is 0.03198908641934395\n",
      "epoch: 26 step: 28, loss is 0.044994767755270004\n",
      "epoch: 26 step: 29, loss is 0.03493693470954895\n",
      "epoch: 26 step: 30, loss is 0.060215409845113754\n",
      "epoch: 26 step: 31, loss is 0.01428486779332161\n",
      "epoch: 26 step: 32, loss is 0.08656146377325058\n",
      "epoch: 26 step: 33, loss is 0.034758903086185455\n",
      "epoch: 26 step: 34, loss is 0.028410879895091057\n",
      "epoch: 26 step: 35, loss is 0.020333683118224144\n",
      "epoch: 26 step: 36, loss is 0.1694546490907669\n",
      "epoch: 26 step: 37, loss is 0.5613733530044556\n",
      "epoch: 26 step: 38, loss is 0.2784901559352875\n",
      "epoch: 26 step: 39, loss is 0.5532427430152893\n",
      "epoch: 26 step: 40, loss is 0.06859244406223297\n",
      "epoch: 26 step: 41, loss is 0.2440900057554245\n",
      "epoch: 26 step: 42, loss is 0.041197195649147034\n",
      "epoch: 26 step: 43, loss is 0.19394934177398682\n",
      "epoch: 26 step: 44, loss is 0.1479722559452057\n",
      "epoch: 26 step: 45, loss is 0.05224043130874634\n",
      "epoch: 26 step: 46, loss is 0.12498221546411514\n",
      "epoch: 26 step: 47, loss is 0.03864773362874985\n",
      "epoch: 26 step: 48, loss is 0.023680787533521652\n",
      "epoch: 26 step: 49, loss is 0.16968420147895813\n",
      "epoch: 26 step: 50, loss is 0.057952627539634705\n",
      "epoch: 26 step: 51, loss is 0.055289968848228455\n",
      "epoch: 26 step: 52, loss is 0.05516710877418518\n",
      "epoch: 26 step: 53, loss is 0.21592892706394196\n",
      "epoch: 26 step: 54, loss is 0.03657117486000061\n",
      "epoch: 26 step: 55, loss is 0.1265978366136551\n",
      "epoch: 26 step: 56, loss is 0.15670661628246307\n",
      "epoch: 26 step: 57, loss is 0.47303247451782227\n",
      "epoch: 26 step: 58, loss is 0.14119532704353333\n",
      "epoch: 26 step: 59, loss is 0.2344132661819458\n",
      "epoch: 26 step: 60, loss is 0.23776401579380035\n",
      "epoch: 26 step: 61, loss is 0.040366560220718384\n",
      "epoch: 26 step: 62, loss is 0.07195641100406647\n",
      "epoch: 26 step: 63, loss is 0.06766409426927567\n",
      "epoch: 26 step: 64, loss is 0.06468379497528076\n",
      "epoch: 26 step: 65, loss is 0.04553712159395218\n",
      "epoch: 26 step: 66, loss is 0.036879729479551315\n",
      "epoch: 26 step: 67, loss is 0.014537184499204159\n",
      "epoch: 26 step: 68, loss is 0.06965753436088562\n",
      "epoch: 26 step: 69, loss is 0.2782556116580963\n",
      "epoch: 26 step: 70, loss is 0.07612454891204834\n",
      "epoch: 26 step: 71, loss is 0.08665875345468521\n",
      "epoch: 26 step: 72, loss is 0.34032365679740906\n",
      "epoch: 26 step: 73, loss is 0.02231743559241295\n",
      "epoch: 26 step: 74, loss is 0.3336487412452698\n",
      "epoch: 26 step: 75, loss is 0.21803846955299377\n",
      "epoch: 26 step: 76, loss is 0.13937166333198547\n",
      "epoch: 26 step: 77, loss is 0.6995740532875061\n",
      "epoch: 26 step: 78, loss is 0.09769078344106674\n",
      "epoch: 26 step: 79, loss is 0.019195303320884705\n",
      "epoch: 26 step: 80, loss is 0.03122866526246071\n",
      "epoch: 26 step: 81, loss is 0.24538375437259674\n",
      "epoch: 26 step: 82, loss is 0.03251514583826065\n",
      "epoch: 26 step: 83, loss is 0.10748361796140671\n",
      "epoch: 26 step: 84, loss is 0.08146406710147858\n",
      "{'accuracy': 0.9058823529411765, 'macro f1': 0.8692307692307693, 'weighted f1': 0.9026244343891403, 'macro recall': 0.8477633477633477, 'macro precision': 0.8996683250414593}\n",
      "Eval result: epoch 26, metrics: {'metrics': 0.8806361987441881}\n",
      "epoch: 27 step: 1, loss is 0.11080479621887207\n",
      "epoch: 27 step: 2, loss is 0.02887694351375103\n",
      "epoch: 27 step: 3, loss is 0.23077017068862915\n",
      "epoch: 27 step: 4, loss is 0.07372438907623291\n",
      "epoch: 27 step: 5, loss is 0.09367594122886658\n",
      "epoch: 27 step: 6, loss is 0.0446930006146431\n",
      "epoch: 27 step: 7, loss is 0.032223477959632874\n",
      "epoch: 27 step: 8, loss is 0.5139289498329163\n",
      "epoch: 27 step: 9, loss is 0.17486731708049774\n",
      "epoch: 27 step: 10, loss is 0.022871945053339005\n",
      "epoch: 27 step: 11, loss is 0.02967272326350212\n",
      "epoch: 27 step: 12, loss is 0.06144552305340767\n",
      "epoch: 27 step: 13, loss is 0.05363389104604721\n",
      "epoch: 27 step: 14, loss is 0.021338511258363724\n",
      "epoch: 27 step: 15, loss is 0.4449021518230438\n",
      "epoch: 27 step: 16, loss is 0.17539489269256592\n",
      "epoch: 27 step: 17, loss is 0.039577774703502655\n",
      "epoch: 27 step: 18, loss is 0.05596290901303291\n",
      "epoch: 27 step: 19, loss is 0.044900912791490555\n",
      "epoch: 27 step: 20, loss is 0.008925352245569229\n",
      "epoch: 27 step: 21, loss is 0.07830571383237839\n",
      "epoch: 27 step: 22, loss is 0.8758387565612793\n",
      "epoch: 27 step: 23, loss is 0.12546411156654358\n",
      "epoch: 27 step: 24, loss is 0.05673649162054062\n",
      "epoch: 27 step: 25, loss is 0.08084042370319366\n",
      "epoch: 27 step: 26, loss is 0.07297152280807495\n",
      "epoch: 27 step: 27, loss is 0.13529428839683533\n",
      "epoch: 27 step: 28, loss is 0.03934607282280922\n",
      "epoch: 27 step: 29, loss is 0.03483700752258301\n",
      "epoch: 27 step: 30, loss is 0.11010612547397614\n",
      "epoch: 27 step: 31, loss is 0.04511933773756027\n",
      "epoch: 27 step: 32, loss is 0.029929671436548233\n",
      "epoch: 27 step: 33, loss is 0.06632789969444275\n",
      "epoch: 27 step: 34, loss is 0.06795907765626907\n",
      "epoch: 27 step: 35, loss is 0.019520370289683342\n",
      "epoch: 27 step: 36, loss is 0.5863229632377625\n",
      "epoch: 27 step: 37, loss is 0.009299764409661293\n",
      "epoch: 27 step: 38, loss is 0.030263660475611687\n",
      "epoch: 27 step: 39, loss is 0.06920772045850754\n",
      "epoch: 27 step: 40, loss is 0.015332018956542015\n",
      "epoch: 27 step: 41, loss is 0.06666912883520126\n",
      "epoch: 27 step: 42, loss is 0.02805083431303501\n",
      "epoch: 27 step: 43, loss is 0.22601406276226044\n",
      "epoch: 27 step: 44, loss is 0.3716544210910797\n",
      "epoch: 27 step: 45, loss is 0.10791143774986267\n",
      "epoch: 27 step: 46, loss is 0.044932592660188675\n",
      "epoch: 27 step: 47, loss is 0.020595291629433632\n",
      "epoch: 27 step: 48, loss is 0.17946656048297882\n",
      "epoch: 27 step: 49, loss is 0.022052954882383347\n",
      "epoch: 27 step: 50, loss is 0.16919074952602386\n",
      "epoch: 27 step: 51, loss is 0.15621592104434967\n",
      "epoch: 27 step: 52, loss is 0.40791040658950806\n",
      "epoch: 27 step: 53, loss is 0.09979956597089767\n",
      "epoch: 27 step: 54, loss is 0.04482109472155571\n",
      "epoch: 27 step: 55, loss is 0.03227906674146652\n",
      "epoch: 27 step: 56, loss is 0.06543975323438644\n",
      "epoch: 27 step: 57, loss is 0.12516511976718903\n",
      "epoch: 27 step: 58, loss is 0.0362609401345253\n",
      "epoch: 27 step: 59, loss is 0.2173500508069992\n",
      "epoch: 27 step: 60, loss is 0.32036665081977844\n",
      "epoch: 27 step: 61, loss is 0.1907808780670166\n",
      "epoch: 27 step: 62, loss is 0.04876543954014778\n",
      "epoch: 27 step: 63, loss is 0.027478910982608795\n",
      "epoch: 27 step: 64, loss is 0.06625537574291229\n",
      "epoch: 27 step: 65, loss is 0.05675017833709717\n",
      "epoch: 27 step: 66, loss is 0.04549552872776985\n",
      "epoch: 27 step: 67, loss is 0.0290928166359663\n",
      "epoch: 27 step: 68, loss is 0.14321337640285492\n",
      "epoch: 27 step: 69, loss is 0.028960883617401123\n",
      "epoch: 27 step: 70, loss is 0.3874569535255432\n",
      "epoch: 27 step: 71, loss is 0.09437304735183716\n",
      "epoch: 27 step: 72, loss is 0.08085843920707703\n",
      "epoch: 27 step: 73, loss is 0.036658935248851776\n",
      "epoch: 27 step: 74, loss is 0.022120486944913864\n",
      "epoch: 27 step: 75, loss is 0.008323650807142258\n",
      "epoch: 27 step: 76, loss is 0.154918372631073\n",
      "epoch: 27 step: 77, loss is 0.050872690975666046\n",
      "epoch: 27 step: 78, loss is 0.04135391116142273\n",
      "epoch: 27 step: 79, loss is 0.01755688339471817\n",
      "epoch: 27 step: 80, loss is 0.28624504804611206\n",
      "epoch: 27 step: 81, loss is 0.4460570514202118\n",
      "epoch: 27 step: 82, loss is 0.04542380943894386\n",
      "epoch: 27 step: 83, loss is 0.060258135199546814\n",
      "epoch: 27 step: 84, loss is 0.08151090890169144\n",
      "{'accuracy': 0.9058823529411765, 'macro f1': 0.8692307692307693, 'weighted f1': 0.9026244343891403, 'macro recall': 0.8477633477633477, 'macro precision': 0.8996683250414593}\n",
      "Eval result: epoch 27, metrics: {'metrics': 0.8806361987441881}\n",
      "epoch: 28 step: 1, loss is 0.18547751009464264\n",
      "epoch: 28 step: 2, loss is 0.06085702031850815\n",
      "epoch: 28 step: 3, loss is 0.2782524824142456\n",
      "epoch: 28 step: 4, loss is 0.04442591592669487\n",
      "epoch: 28 step: 5, loss is 0.04566396027803421\n",
      "epoch: 28 step: 6, loss is 0.0652986466884613\n",
      "epoch: 28 step: 7, loss is 0.6566293239593506\n",
      "epoch: 28 step: 8, loss is 0.05430873855948448\n",
      "epoch: 28 step: 9, loss is 0.019029591232538223\n",
      "epoch: 28 step: 10, loss is 0.013140623457729816\n",
      "epoch: 28 step: 11, loss is 0.1423642933368683\n",
      "epoch: 28 step: 12, loss is 0.17455297708511353\n",
      "epoch: 28 step: 13, loss is 0.017211752012372017\n",
      "epoch: 28 step: 14, loss is 0.05917010456323624\n",
      "epoch: 28 step: 15, loss is 0.11509200930595398\n",
      "epoch: 28 step: 16, loss is 0.13167308270931244\n",
      "epoch: 28 step: 17, loss is 0.42986321449279785\n",
      "epoch: 28 step: 18, loss is 0.04691991209983826\n",
      "epoch: 28 step: 19, loss is 0.1919400990009308\n",
      "epoch: 28 step: 20, loss is 0.022119896486401558\n",
      "epoch: 28 step: 21, loss is 0.058652278035879135\n",
      "epoch: 28 step: 22, loss is 0.17207515239715576\n",
      "epoch: 28 step: 23, loss is 0.026589494198560715\n",
      "epoch: 28 step: 24, loss is 0.5910829901695251\n",
      "epoch: 28 step: 25, loss is 0.021991871297359467\n",
      "epoch: 28 step: 26, loss is 0.3898341655731201\n",
      "epoch: 28 step: 27, loss is 0.023058967664837837\n",
      "epoch: 28 step: 28, loss is 0.03290657699108124\n",
      "epoch: 28 step: 29, loss is 0.02008201740682125\n",
      "epoch: 28 step: 30, loss is 0.4948057234287262\n",
      "epoch: 28 step: 31, loss is 0.05075624957680702\n",
      "epoch: 28 step: 32, loss is 0.038051411509513855\n",
      "epoch: 28 step: 33, loss is 0.032580502331256866\n",
      "epoch: 28 step: 34, loss is 0.3072584569454193\n",
      "epoch: 28 step: 35, loss is 0.028208449482917786\n",
      "epoch: 28 step: 36, loss is 0.19260333478450775\n",
      "epoch: 28 step: 37, loss is 0.04174218326807022\n",
      "epoch: 28 step: 38, loss is 0.12973839044570923\n",
      "epoch: 28 step: 39, loss is 0.08859177678823471\n",
      "epoch: 28 step: 40, loss is 0.0552402101457119\n",
      "epoch: 28 step: 41, loss is 0.01463119313120842\n",
      "epoch: 28 step: 42, loss is 0.08205454051494598\n",
      "epoch: 28 step: 43, loss is 0.07218170166015625\n",
      "epoch: 28 step: 44, loss is 0.043218620121479034\n",
      "epoch: 28 step: 45, loss is 0.024193227291107178\n",
      "epoch: 28 step: 46, loss is 0.028154626488685608\n",
      "epoch: 28 step: 47, loss is 0.02743750996887684\n",
      "epoch: 28 step: 48, loss is 0.16124911606311798\n",
      "epoch: 28 step: 49, loss is 0.060814183205366135\n",
      "epoch: 28 step: 50, loss is 0.018802253529429436\n",
      "epoch: 28 step: 51, loss is 0.017115745693445206\n",
      "epoch: 28 step: 52, loss is 0.2880447208881378\n",
      "epoch: 28 step: 53, loss is 0.04014930501580238\n",
      "epoch: 28 step: 54, loss is 0.028972849249839783\n",
      "epoch: 28 step: 55, loss is 0.0852368101477623\n",
      "epoch: 28 step: 56, loss is 0.015237607061862946\n",
      "epoch: 28 step: 57, loss is 0.03242138773202896\n",
      "epoch: 28 step: 58, loss is 0.24320723116397858\n",
      "epoch: 28 step: 59, loss is 0.0463394857943058\n",
      "epoch: 28 step: 60, loss is 0.030363230034708977\n",
      "epoch: 28 step: 61, loss is 0.01935664191842079\n",
      "epoch: 28 step: 62, loss is 0.08161667734384537\n",
      "epoch: 28 step: 63, loss is 0.2090034782886505\n",
      "epoch: 28 step: 64, loss is 0.01644931174814701\n",
      "epoch: 28 step: 65, loss is 0.024263396859169006\n",
      "epoch: 28 step: 66, loss is 0.012129408307373524\n",
      "epoch: 28 step: 67, loss is 0.16770373284816742\n",
      "epoch: 28 step: 68, loss is 0.2335638850927353\n",
      "epoch: 28 step: 69, loss is 0.01715576834976673\n",
      "epoch: 28 step: 70, loss is 0.023977188393473625\n",
      "epoch: 28 step: 71, loss is 0.37386271357536316\n",
      "epoch: 28 step: 72, loss is 0.02219046838581562\n",
      "epoch: 28 step: 73, loss is 0.03980835899710655\n",
      "epoch: 28 step: 74, loss is 0.052667729556560516\n",
      "epoch: 28 step: 75, loss is 0.043138355016708374\n",
      "epoch: 28 step: 76, loss is 0.016892878338694572\n",
      "epoch: 28 step: 77, loss is 0.26525744795799255\n",
      "epoch: 28 step: 78, loss is 0.5488561391830444\n",
      "epoch: 28 step: 79, loss is 0.11388161033391953\n",
      "epoch: 28 step: 80, loss is 0.03334634751081467\n",
      "epoch: 28 step: 81, loss is 0.0640212818980217\n",
      "epoch: 28 step: 82, loss is 0.024629361927509308\n",
      "epoch: 28 step: 83, loss is 0.110881507396698\n",
      "epoch: 28 step: 84, loss is 0.07075785845518112\n",
      "{'accuracy': 0.9058823529411765, 'macro f1': 0.8692307692307693, 'weighted f1': 0.9026244343891403, 'macro recall': 0.8477633477633477, 'macro precision': 0.8996683250414593}\n",
      "Eval result: epoch 28, metrics: {'metrics': 0.8806361987441881}\n",
      "epoch: 29 step: 1, loss is 0.024085115641355515\n",
      "epoch: 29 step: 2, loss is 0.05206293612718582\n",
      "epoch: 29 step: 3, loss is 0.011222448199987411\n",
      "epoch: 29 step: 4, loss is 0.6232201457023621\n",
      "epoch: 29 step: 5, loss is 0.4405805468559265\n",
      "epoch: 29 step: 6, loss is 0.12642072141170502\n",
      "epoch: 29 step: 7, loss is 0.24816881120204926\n",
      "epoch: 29 step: 8, loss is 0.09359703958034515\n",
      "epoch: 29 step: 9, loss is 0.16259509325027466\n",
      "epoch: 29 step: 10, loss is 0.4180329442024231\n",
      "epoch: 29 step: 11, loss is 0.16427603363990784\n",
      "epoch: 29 step: 12, loss is 0.025810349732637405\n",
      "epoch: 29 step: 13, loss is 0.017909524962306023\n",
      "epoch: 29 step: 14, loss is 0.02349391207098961\n",
      "epoch: 29 step: 15, loss is 0.6768044829368591\n",
      "epoch: 29 step: 16, loss is 0.0531574971973896\n",
      "epoch: 29 step: 17, loss is 0.01655246689915657\n",
      "epoch: 29 step: 18, loss is 0.13225170969963074\n",
      "epoch: 29 step: 19, loss is 0.03308850899338722\n",
      "epoch: 29 step: 20, loss is 0.08061408996582031\n",
      "epoch: 29 step: 21, loss is 0.01885085366666317\n",
      "epoch: 29 step: 22, loss is 0.04725145548582077\n",
      "epoch: 29 step: 23, loss is 0.36896535754203796\n",
      "epoch: 29 step: 24, loss is 0.058016445487737656\n",
      "epoch: 29 step: 25, loss is 0.05622275173664093\n",
      "epoch: 29 step: 26, loss is 0.030362822115421295\n",
      "epoch: 29 step: 27, loss is 0.040575627237558365\n",
      "epoch: 29 step: 28, loss is 0.044038623571395874\n",
      "epoch: 29 step: 29, loss is 0.028227023780345917\n",
      "epoch: 29 step: 30, loss is 0.16131652891635895\n",
      "epoch: 29 step: 31, loss is 0.014723059721291065\n",
      "epoch: 29 step: 32, loss is 0.08306019753217697\n",
      "epoch: 29 step: 33, loss is 0.09973989427089691\n",
      "epoch: 29 step: 34, loss is 0.030057255178689957\n",
      "epoch: 29 step: 35, loss is 0.0317341685295105\n",
      "epoch: 29 step: 36, loss is 0.04085847735404968\n",
      "epoch: 29 step: 37, loss is 0.06892817467451096\n",
      "epoch: 29 step: 38, loss is 0.03857588768005371\n",
      "epoch: 29 step: 39, loss is 0.03748100996017456\n",
      "epoch: 29 step: 40, loss is 0.07396478950977325\n",
      "epoch: 29 step: 41, loss is 0.04941143840551376\n",
      "epoch: 29 step: 42, loss is 0.03494342416524887\n",
      "epoch: 29 step: 43, loss is 0.0470847487449646\n",
      "epoch: 29 step: 44, loss is 0.08321060240268707\n",
      "epoch: 29 step: 45, loss is 0.46831485629081726\n",
      "epoch: 29 step: 46, loss is 0.03485652431845665\n",
      "epoch: 29 step: 47, loss is 0.02782275341451168\n",
      "epoch: 29 step: 48, loss is 0.06291733682155609\n",
      "epoch: 29 step: 49, loss is 0.06642045080661774\n",
      "epoch: 29 step: 50, loss is 0.024440094828605652\n",
      "epoch: 29 step: 51, loss is 0.030969392508268356\n",
      "epoch: 29 step: 52, loss is 0.0716625452041626\n",
      "epoch: 29 step: 53, loss is 0.0353548601269722\n",
      "epoch: 29 step: 54, loss is 0.03549029305577278\n",
      "epoch: 29 step: 55, loss is 0.022051924839615822\n",
      "epoch: 29 step: 56, loss is 0.05680497735738754\n",
      "epoch: 29 step: 57, loss is 0.06562557816505432\n",
      "epoch: 29 step: 58, loss is 0.5817649960517883\n",
      "epoch: 29 step: 59, loss is 0.05274615436792374\n",
      "epoch: 29 step: 60, loss is 0.035791292786598206\n",
      "epoch: 29 step: 61, loss is 0.3797253966331482\n",
      "epoch: 29 step: 62, loss is 0.008588546887040138\n",
      "epoch: 29 step: 63, loss is 0.05081813409924507\n",
      "epoch: 29 step: 64, loss is 0.1398462951183319\n",
      "epoch: 29 step: 65, loss is 0.024527110159397125\n",
      "epoch: 29 step: 66, loss is 0.27464497089385986\n",
      "epoch: 29 step: 67, loss is 0.1088009774684906\n",
      "epoch: 29 step: 68, loss is 0.04257642477750778\n",
      "epoch: 29 step: 69, loss is 0.07952772080898285\n",
      "epoch: 29 step: 70, loss is 0.011720482259988785\n",
      "epoch: 29 step: 71, loss is 0.04805751144886017\n",
      "epoch: 29 step: 72, loss is 0.04785637557506561\n",
      "epoch: 29 step: 73, loss is 0.33816200494766235\n",
      "epoch: 29 step: 74, loss is 0.18318244814872742\n",
      "epoch: 29 step: 75, loss is 0.016019277274608612\n",
      "epoch: 29 step: 76, loss is 0.09565839171409607\n",
      "epoch: 29 step: 77, loss is 0.0879909098148346\n",
      "epoch: 29 step: 78, loss is 0.01753966324031353\n",
      "epoch: 29 step: 79, loss is 0.052545905113220215\n",
      "epoch: 29 step: 80, loss is 0.13994352519512177\n",
      "epoch: 29 step: 81, loss is 0.025551246479153633\n",
      "epoch: 29 step: 82, loss is 0.022672174498438835\n",
      "epoch: 29 step: 83, loss is 0.05031367391347885\n",
      "epoch: 29 step: 84, loss is 0.03169672563672066\n",
      "{'accuracy': 0.9058823529411765, 'macro f1': 0.8692307692307693, 'weighted f1': 0.9026244343891403, 'macro recall': 0.8477633477633477, 'macro precision': 0.8996683250414593}\n",
      "Eval result: epoch 29, metrics: {'metrics': 0.8806361987441881}\n",
      "epoch: 30 step: 1, loss is 0.008531633764505386\n",
      "epoch: 30 step: 2, loss is 0.029393326491117477\n",
      "epoch: 30 step: 3, loss is 0.035921186208724976\n",
      "epoch: 30 step: 4, loss is 0.026466963812708855\n",
      "epoch: 30 step: 5, loss is 0.6821479797363281\n",
      "epoch: 30 step: 6, loss is 0.4719359278678894\n",
      "epoch: 30 step: 7, loss is 0.04811058193445206\n",
      "epoch: 30 step: 8, loss is 0.0659012719988823\n",
      "epoch: 30 step: 9, loss is 0.5688629746437073\n",
      "epoch: 30 step: 10, loss is 0.04490179568529129\n",
      "epoch: 30 step: 11, loss is 0.017628498375415802\n",
      "epoch: 30 step: 12, loss is 0.10187821090221405\n",
      "epoch: 30 step: 13, loss is 0.14278608560562134\n",
      "epoch: 30 step: 14, loss is 0.22630296647548676\n",
      "epoch: 30 step: 15, loss is 0.03688710182905197\n",
      "epoch: 30 step: 16, loss is 0.0429699644446373\n",
      "epoch: 30 step: 17, loss is 0.020004741847515106\n",
      "epoch: 30 step: 18, loss is 0.05950792133808136\n",
      "epoch: 30 step: 19, loss is 0.060772038996219635\n",
      "epoch: 30 step: 20, loss is 0.03125622868537903\n",
      "epoch: 30 step: 21, loss is 0.026577908545732498\n",
      "epoch: 30 step: 22, loss is 0.06760202348232269\n",
      "epoch: 30 step: 23, loss is 0.3812268078327179\n",
      "epoch: 30 step: 24, loss is 0.01481604389846325\n",
      "epoch: 30 step: 25, loss is 0.021886980161070824\n",
      "epoch: 30 step: 26, loss is 0.008998014032840729\n",
      "epoch: 30 step: 27, loss is 0.012340463697910309\n",
      "epoch: 30 step: 28, loss is 0.08919505774974823\n",
      "epoch: 30 step: 29, loss is 0.39073067903518677\n",
      "epoch: 30 step: 30, loss is 0.04956957697868347\n",
      "epoch: 30 step: 31, loss is 0.06843509525060654\n",
      "epoch: 30 step: 32, loss is 0.20772066712379456\n",
      "epoch: 30 step: 33, loss is 0.09634159505367279\n",
      "epoch: 30 step: 34, loss is 0.06505261361598969\n",
      "epoch: 30 step: 35, loss is 0.007079421076923609\n",
      "epoch: 30 step: 36, loss is 0.018941590562462807\n",
      "epoch: 30 step: 37, loss is 0.1142544373869896\n",
      "epoch: 30 step: 38, loss is 0.5461204051971436\n",
      "epoch: 30 step: 39, loss is 0.021497488021850586\n",
      "epoch: 30 step: 40, loss is 0.008423920720815659\n",
      "epoch: 30 step: 41, loss is 0.03140968084335327\n",
      "epoch: 30 step: 42, loss is 0.033942289650440216\n",
      "epoch: 30 step: 43, loss is 0.0760476216673851\n",
      "epoch: 30 step: 44, loss is 0.010017435066401958\n",
      "epoch: 30 step: 45, loss is 0.028833743184804916\n",
      "epoch: 30 step: 46, loss is 0.020580004900693893\n",
      "epoch: 30 step: 47, loss is 0.10214359313249588\n",
      "epoch: 30 step: 48, loss is 0.5464358329772949\n",
      "epoch: 30 step: 49, loss is 0.011752933263778687\n",
      "epoch: 30 step: 50, loss is 0.04006717726588249\n",
      "epoch: 30 step: 51, loss is 0.04985450580716133\n",
      "epoch: 30 step: 52, loss is 0.11428122967481613\n",
      "epoch: 30 step: 53, loss is 0.026903031393885612\n",
      "epoch: 30 step: 54, loss is 0.02028159610927105\n",
      "epoch: 30 step: 55, loss is 0.12646585702896118\n",
      "epoch: 30 step: 56, loss is 0.15333330631256104\n",
      "epoch: 30 step: 57, loss is 0.16654519736766815\n",
      "epoch: 30 step: 58, loss is 0.1627427637577057\n",
      "epoch: 30 step: 59, loss is 0.03706340491771698\n",
      "epoch: 30 step: 60, loss is 0.03988891839981079\n",
      "epoch: 30 step: 61, loss is 0.038654495030641556\n",
      "epoch: 30 step: 62, loss is 0.023937705904245377\n",
      "epoch: 30 step: 63, loss is 0.2010272741317749\n",
      "epoch: 30 step: 64, loss is 0.012467973865568638\n",
      "epoch: 30 step: 65, loss is 0.1307659149169922\n",
      "epoch: 30 step: 66, loss is 0.21164138615131378\n",
      "epoch: 30 step: 67, loss is 0.1674787402153015\n",
      "epoch: 30 step: 68, loss is 0.021680455654859543\n",
      "epoch: 30 step: 69, loss is 0.030750419944524765\n",
      "epoch: 30 step: 70, loss is 0.19411514699459076\n",
      "epoch: 30 step: 71, loss is 0.06631215661764145\n",
      "epoch: 30 step: 72, loss is 0.041647203266620636\n",
      "epoch: 30 step: 73, loss is 0.35862234234809875\n",
      "epoch: 30 step: 74, loss is 0.033804964274168015\n",
      "epoch: 30 step: 75, loss is 0.034398145973682404\n",
      "epoch: 30 step: 76, loss is 0.07176229357719421\n",
      "epoch: 30 step: 77, loss is 0.04659883677959442\n",
      "epoch: 30 step: 78, loss is 0.034441761672496796\n",
      "epoch: 30 step: 79, loss is 0.03924914821982384\n",
      "epoch: 30 step: 80, loss is 0.012402142398059368\n",
      "epoch: 30 step: 81, loss is 0.028805464506149292\n",
      "epoch: 30 step: 82, loss is 0.01271582581102848\n",
      "epoch: 30 step: 83, loss is 0.01649806648492813\n",
      "epoch: 30 step: 84, loss is 0.010266147553920746\n",
      "{'accuracy': 0.8941176470588236, 'macro f1': 0.8553601815087919, 'weighted f1': 0.8914750925895032, 'macro recall': 0.8398268398268398, 'macro precision': 0.8755980861244019}\n",
      "Eval result: epoch 30, metrics: {'metrics': 0.8662256886297144}\n",
      "dataset t1: 423\n"
     ]
    }
   ],
   "source": [
    "model=MLP(gene_emb, cfg)\n",
    "lr=args.lr\n",
    "optimizer=nn.Adam(model.trainable_params(),lr,weight_decay=1e-5)\n",
    "wrapper=Wrapper(model,optimizer)\n",
    "trainer=Model(\n",
    "    wrapper,\n",
    "    amp_level='O0',\n",
    "    eval_network=model,\n",
    "    metrics={\n",
    "        'metrics':annote_metric(2),\n",
    "    },\n",
    "    eval_indexes=[0,1,2]\n",
    ")\n",
    "ckpt_config = CheckpointConfig(\n",
    "    save_checkpoint_steps=args.epoch*len(trainset),\n",
    "    keep_checkpoint_max=1,\n",
    "    integrated_save=False,\n",
    "    async_save=True,\n",
    ")\n",
    "\n",
    "loss_cb = LossMonitor(1)\n",
    "now=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "# print(f'Begin training {len(trainset)*args.epoch} steps at {now}')\n",
    "cbs = [loss_cb]\n",
    "trainer.fit(args.epoch,trainset,testset,callbacks=cbs)\n",
    "print(f'dataset {args.data}:', len(scrna) + len(sctest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
