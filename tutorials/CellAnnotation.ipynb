{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1da129-51bb-40f0-9cf0-141ccef906e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:08:32.667840Z",
     "iopub.status.busy": "2024-06-26T12:08:32.667174Z",
     "iopub.status.idle": "2024-06-26T12:09:08.201757Z",
     "shell.execute_reply": "2024-06-26T12:09:08.200759Z",
     "shell.execute_reply.started": "2024-06-26T12:08:32.667786Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import mindspore as ms\n",
    "import mindspore.numpy as mnp\n",
    "import mindspore.scipy as msc\n",
    "import mindspore.dataset as ds\n",
    "from tqdm import tqdm,trange\n",
    "from mindspore import nn,ops\n",
    "from scipy.sparse import csr_matrix as csm\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.amp import FixedLossScaleManager,all_finite,DynamicLossScaleManager\n",
    "from mindspore.train import Model, CheckpointConfig, ModelCheckpoint, LossMonitor\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.communication import init, get_rank, get_group_size\n",
    "from mindspore.parallel._utils import _get_parallel_mode\n",
    "from mindspore.common.initializer import initializer, XavierNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340697f0-f9fa-4126-a66a-edffdaf7f859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:08.203652Z",
     "iopub.status.busy": "2024-06-26T12:09:08.203184Z",
     "iopub.status.idle": "2024-06-26T12:09:08.207207Z",
     "shell.execute_reply": "2024-06-26T12:09:08.206693Z",
     "shell.execute_reply.started": "2024-06-26T12:09:08.203628Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5737d88d-dce4-4149-8217-3b8a8bc77324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:08.208344Z",
     "iopub.status.busy": "2024-06-26T12:09:08.207940Z",
     "iopub.status.idle": "2024-06-26T12:09:08.645116Z",
     "shell.execute_reply": "2024-06-26T12:09:08.644250Z",
     "shell.execute_reply.started": "2024-06-26T12:09:08.208325Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from config import Config\n",
    "from annotation_model import *\n",
    "from metrics import annote_metric\n",
    "from utils import Wrapper\n",
    "from data_process import Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20d430d-1f4d-4b18-b386-612f5b381534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:08.647024Z",
     "iopub.status.busy": "2024-06-26T12:09:08.646800Z",
     "iopub.status.idle": "2024-06-26T12:09:08.651621Z",
     "shell.execute_reply": "2024-06-26T12:09:08.650890Z",
     "shell.execute_reply.started": "2024-06-26T12:09:08.647004Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def freeze_module(module,filter_tag=[None]):\n",
    "    for param in module.trainable_params():\n",
    "        x=False\n",
    "        for tag in filter_tag:\n",
    "            if tag and tag in param.name:\n",
    "                x=True\n",
    "                break\n",
    "        param.requires_grad = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e787f5d5-db9f-4473-a3cf-3ec159a31b3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:08.652490Z",
     "iopub.status.busy": "2024-06-26T12:09:08.652296Z",
     "iopub.status.idle": "2024-06-26T12:09:08.661007Z",
     "shell.execute_reply": "2024-06-26T12:09:08.660286Z",
     "shell.execute_reply.started": "2024-06-26T12:09:08.652473Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_h5ad(path,var_rate=0,test_rate=0.3):\n",
    "    suffix=path.split('.')[-1]\n",
    "    if suffix=='h5ad':\n",
    "        adata=sc.read_h5ad(path)\n",
    "    else:\n",
    "        adata=sc.read_10x_h5(path)\n",
    "    print('origin shape:',adata.shape,len(adata.obs['cell_type'].unique()))\n",
    "    adata.obs['train']=0\n",
    "    adatas=[adata[adata.obs['batch']==i].copy() for i in adata.obs['batch'].unique()]\n",
    "    for adatai in adatas:\n",
    "        for i in adatai.obs['cell_type'].unique():\n",
    "            idx=adatai.obs['cell_type']==i\n",
    "            size=idx.sum()\n",
    "            order=np.random.permutation(size)\n",
    "            num1=int(np.ceil(size*test_rate))\n",
    "            num2=int(np.ceil(size*var_rate))\n",
    "            test=order[:num1]\n",
    "            val=order[num1:num1+num2]\n",
    "            test=idx.values.nonzero()[0][test]\n",
    "            val=idx.values.nonzero()[0][val]\n",
    "            adatai.obs['train'][test]=2\n",
    "            adatai.obs['train'][val]=1\n",
    "    adata=sc.concat(adatas,merge='same')\n",
    "        \n",
    "    data=adata.X.astype(np.float32)\n",
    "    T=adata.X.sum(1)\n",
    "    data=csm(np.round(data/np.maximum(1,T/1e5,dtype=np.float32)))\n",
    "    data.eliminate_zeros()\n",
    "    adata.X=data\n",
    "    \n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c542bfb-5919-4a28-926f-07e72bb53448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:08.661906Z",
     "iopub.status.busy": "2024-06-26T12:09:08.661714Z",
     "iopub.status.idle": "2024-06-26T12:09:08.680291Z",
     "shell.execute_reply": "2024-06-26T12:09:08.679394Z",
     "shell.execute_reply.started": "2024-06-26T12:09:08.661889Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SCrna():\n",
    "    def __init__(self,adata,mode='train',prep=True):\n",
    "        self.cls=len(adata.obs['cell_type'].unique())\n",
    "        if mode==\"train\":\n",
    "            adata=adata[adata.obs.train==0]\n",
    "        elif mode=='val':\n",
    "            adata=adata[adata.obs.train==1]\n",
    "        else:\n",
    "            adata=adata[adata.obs.train==2]\n",
    "        self.gene_info=pd.read_csv(f'../csv/gene_info.csv',index_col=0,header=0)\n",
    "        self.geneset={j:i+1 for i,j in enumerate(self.gene_info.index)}\n",
    "        \n",
    "        gene=np.intersect1d(adata.var_names,self.gene_info.index)\n",
    "        adata=adata[:,gene].copy()\n",
    "        adata.obs['cell_type']=adata.obs['cell_type'].astype('category')\n",
    "        label=adata.obs['cell_type'].cat.codes.values\n",
    "        adata.obs['label']=label\n",
    "        if prep:\n",
    "            adata.layers['x_normed']=sc.pp.normalize_total(adata,target_sum=1e4,inplace=False)['X']\n",
    "            adata.layers['x_log1p']=adata.layers['x_normed']\n",
    "            sc.pp.log1p(adata,layer='x_log1p')\n",
    "        self.adata=adata\n",
    "        self.id2label=adata.obs['cell_type'].cat.categories.values\n",
    "        self.gene=np.array([self.geneset[i] for i in self.adata.var_names]).astype(np.int32)\n",
    "        self.cls=len(adata.obs['cell_type'].unique())\n",
    "        self.label=self.adata.obs['label'].values.astype(np.int32)\n",
    "        print(f'{mode} adata:',adata.shape,self.cls)\n",
    "        if prep:\n",
    "            self.data=self.adata.layers['x_log1p'].A.astype(np.float32)\n",
    "        else:\n",
    "            self.data=self.adata.X.astype(np.int32)\n",
    "    def __len__(self):\n",
    "        return len(self.adata)\n",
    "    def __getitem__(self,idx):\n",
    "        data=self.data[idx].reshape(-1)\n",
    "        label=self.label[idx]\n",
    "        return data,self.gene,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70766405-a73f-46ba-a918-e3d88f2c9eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:08.681848Z",
     "iopub.status.busy": "2024-06-26T12:09:08.681404Z",
     "iopub.status.idle": "2024-06-26T12:09:08.692598Z",
     "shell.execute_reply": "2024-06-26T12:09:08.691704Z",
     "shell.execute_reply.started": "2024-06-26T12:09:08.681819Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    data,prep,batch,\n",
    "    rank_size=None,\n",
    "    rank_id=None,\n",
    "    drop=True,\n",
    "    shuffle=True\n",
    "):\n",
    "    dataset = ds.GeneratorDataset(\n",
    "        data, \n",
    "        column_names=['data','gene','label'],\n",
    "        shuffle=shuffle,\n",
    "        num_shards=rank_size, \n",
    "        shard_id=rank_id\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        prep.seperate, input_columns=['data'],\n",
    "        output_columns=['data', 'nonz','zero']\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        prep.sample, input_columns=['data','nonz','zero'],\n",
    "        output_columns=['data','nonz','cuted','z_sample','seq_len']\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        prep.compress, input_columns=['data','nonz'],\n",
    "        output_columns=['data','nonz_data', 'nonz']\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        prep.compress, input_columns=['gene','nonz'],\n",
    "        output_columns=['gene','nonz_gene', 'nonz']\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        prep.attn_mask, input_columns=['seq_len'],\n",
    "        output_columns=['zero_idx']\n",
    "    )\n",
    "    dataset = dataset.map(prep.pad_zero, input_columns=['nonz_data'])\n",
    "    dataset = dataset.map(prep.pad_zero, input_columns=['nonz_gene'])\n",
    "    dataset=dataset.project(\n",
    "        columns=['nonz_data','nonz_gene','zero_idx','label']\n",
    "    )\n",
    "    dataset = dataset.batch(\n",
    "        batch,\n",
    "        num_parallel_workers=4, \n",
    "        drop_remainder=drop, \n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d22789-889c-4db4-b828-63bdb94afbdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:08.694008Z",
     "iopub.status.busy": "2024-06-26T12:09:08.693682Z",
     "iopub.status.idle": "2024-06-26T12:09:08.698960Z",
     "shell.execute_reply": "2024-06-26T12:09:08.698089Z",
     "shell.execute_reply.started": "2024-06-26T12:09:08.693981Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ms.set_context(\n",
    "    device_target='Ascend', \n",
    "    mode=ms.GRAPH_MODE,\n",
    "    device_id=0,\n",
    ")\n",
    "ms.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa54c88-b012-47a9-a17e-05efbcabb8c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:08.700779Z",
     "iopub.status.busy": "2024-06-26T12:09:08.700033Z",
     "iopub.status.idle": "2024-06-26T12:09:17.485125Z",
     "shell.execute_reply": "2024-06-26T12:09:17.484216Z",
     "shell.execute_reply.started": "2024-06-26T12:09:08.700750Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin shape: (14767, 15558) 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105122/220353223.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  adatai.obs['train'][test]=2\n",
      "/tmp/ipykernel_105122/220353223.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  adatai.obs['train'][val]=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train adata: (10316, 15285) 15\n",
      "test adata: (4451, 15285) 15\n"
     ]
    }
   ],
   "source": [
    "adata=read_h5ad(f\"../datasets/processed/Pancrm.h5ad\")\n",
    "trainset=SCrna(adata,mode='train')\n",
    "testset=SCrna(adata,mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6eb299-0726-4004-a064-a0f1ad264bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:17.487085Z",
     "iopub.status.busy": "2024-06-26T12:09:17.486898Z",
     "iopub.status.idle": "2024-06-26T12:09:17.490907Z",
     "shell.execute_reply": "2024-06-26T12:09:17.490091Z",
     "shell.execute_reply.started": "2024-06-26T12:09:17.487067Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg=Config()\n",
    "cfg.num_cls=trainset.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e0e09b3-474f-4c24-a64e-2945b91afeb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:17.491635Z",
     "iopub.status.busy": "2024-06-26T12:09:17.491475Z",
     "iopub.status.idle": "2024-06-26T12:09:17.501577Z",
     "shell.execute_reply": "2024-06-26T12:09:17.500873Z",
     "shell.execute_reply.started": "2024-06-26T12:09:17.491621Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prep=Prepare(\n",
    "    cfg.nonz_len,pad=1,mask_ratio=0,random=False\n",
    ")\n",
    "train_loader=build_dataset(\n",
    "    trainset,\n",
    "    prep,\n",
    "    16,\n",
    "    drop=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader=build_dataset(\n",
    "    testset,\n",
    "    prep,\n",
    "    1,\n",
    "    drop=False,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377701df-4390-49d3-8b57-c1f838cff281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:17.502287Z",
     "iopub.status.busy": "2024-06-26T12:09:17.502132Z",
     "iopub.status.idle": "2024-06-26T12:09:46.211822Z",
     "shell.execute_reply": "2024-06-26T12:09:46.210586Z",
     "shell.execute_reply.started": "2024-06-26T12:09:17.502274Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "para=ms.load_checkpoint(\"../weights/base_weight.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76df8be4-2701-4915-9d01-5ff091ac885f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:09:46.213415Z",
     "iopub.status.busy": "2024-06-26T12:09:46.213146Z",
     "iopub.status.idle": "2024-06-26T12:10:03.557228Z",
     "shell.execute_reply": "2024-06-26T12:10:03.555859Z",
     "shell.execute_reply.started": "2024-06-26T12:09:46.213387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone=Backbone(len(trainset.geneset),cfg)\n",
    "ms.load_param_into_net(backbone, para)\n",
    "model=Net(backbone,cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c55a536d-4ce3-4bac-901d-1a3e3906779b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:10:03.559227Z",
     "iopub.status.busy": "2024-06-26T12:10:03.558552Z",
     "iopub.status.idle": "2024-06-26T12:10:03.569179Z",
     "shell.execute_reply": "2024-06-26T12:10:03.568104Z",
     "shell.execute_reply.started": "2024-06-26T12:10:03.559198Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "freeze_module(model.extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b3b63d7-46a4-45ea-9307-823e71e545ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:10:03.570857Z",
     "iopub.status.busy": "2024-06-26T12:10:03.570229Z",
     "iopub.status.idle": "2024-06-26T12:10:04.039259Z",
     "shell.execute_reply": "2024-06-26T12:10:04.038205Z",
     "shell.execute_reply.started": "2024-06-26T12:10:03.570830Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer=nn.Adam(model.trainable_params(),1e-4,weight_decay=1e-5)\n",
    "update_cell=nn.DynamicLossScaleUpdateCell(1,2,1000)\n",
    "wrapper=Wrapper(model,optimizer)\n",
    "trainer=Model(\n",
    "    wrapper,\n",
    "    eval_network=model,\n",
    "    amp_level='O0',\n",
    "    metrics={\n",
    "        'accuracy':annote_metric(trainset.cls,key='accuracy'),\n",
    "    },\n",
    "    eval_indexes=[0,1,2]\n",
    ")\n",
    "loss_cb = LossMonitor(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9cceaea-0044-4f11-9249-9766746294c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:10:51.761637Z",
     "iopub.status.busy": "2024-06-26T12:10:51.761078Z",
     "iopub.status.idle": "2024-06-26T12:10:51.767692Z",
     "shell.execute_reply": "2024-06-26T12:10:51.766676Z",
     "shell.execute_reply.started": "2024-06-26T12:10:51.761612Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_cb = LossMonitor(20)\n",
    "ckpt_config = CheckpointConfig(\n",
    "    save_checkpoint_steps=len(train_loader),\n",
    "    keep_checkpoint_max=1,\n",
    "    integrated_save=False,\n",
    "    async_save=False\n",
    ")\n",
    "ckpt_cb = ModelCheckpoint(\n",
    "    prefix=f'Pancrm_intra', \n",
    "    directory=f\"../checkpoint/CellAnnotation/\", \n",
    "    config=ckpt_config\n",
    ")\n",
    "cbs=[loss_cb,ckpt_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afdc6c97-c541-4210-8688-672bafe5b058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T01:32:55.178632Z",
     "iopub.status.busy": "2024-06-26T01:32:55.178474Z",
     "iopub.status.idle": "2024-06-26T11:57:00.626089Z",
     "shell.execute_reply": "2024-06-26T11:57:00.617142Z",
     "shell.execute_reply.started": "2024-06-26T01:32:55.178619Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 20, loss is 3.9814298152923584\n",
      "epoch: 1 step: 40, loss is 4.81403923034668\n",
      "epoch: 1 step: 60, loss is 4.15134334564209\n",
      "epoch: 1 step: 80, loss is 3.271254539489746\n",
      "epoch: 1 step: 100, loss is 3.0359766483306885\n",
      "epoch: 1 step: 120, loss is 4.056429862976074\n",
      "epoch: 1 step: 140, loss is 3.7438266277313232\n",
      "epoch: 1 step: 160, loss is 4.359557151794434\n",
      "epoch: 1 step: 180, loss is 3.1246750354766846\n",
      "epoch: 1 step: 200, loss is 4.2687530517578125\n",
      "epoch: 1 step: 220, loss is 3.5927300453186035\n",
      "epoch: 1 step: 240, loss is 3.8465421199798584\n",
      "epoch: 1 step: 260, loss is 4.6303606033325195\n",
      "epoch: 1 step: 280, loss is 3.776381015777588\n",
      "epoch: 1 step: 300, loss is 4.1960954666137695\n",
      "epoch: 1 step: 320, loss is 3.611701011657715\n",
      "epoch: 1 step: 340, loss is 3.3810300827026367\n",
      "epoch: 1 step: 360, loss is 3.7454464435577393\n",
      "epoch: 1 step: 380, loss is 3.231947183609009\n",
      "epoch: 1 step: 400, loss is 3.565889358520508\n",
      "epoch: 1 step: 420, loss is 3.692998170852661\n",
      "epoch: 1 step: 440, loss is 2.7244713306427\n",
      "epoch: 1 step: 460, loss is 3.742706537246704\n",
      "epoch: 1 step: 480, loss is 3.815617799758911\n",
      "epoch: 1 step: 500, loss is 4.408685684204102\n",
      "epoch: 1 step: 520, loss is 3.7742371559143066\n",
      "epoch: 1 step: 540, loss is 2.9479117393493652\n",
      "epoch: 1 step: 560, loss is 3.603443145751953\n",
      "epoch: 1 step: 580, loss is 3.591859817504883\n",
      "epoch: 1 step: 600, loss is 2.454008102416992\n",
      "epoch: 1 step: 620, loss is 2.843766689300537\n",
      "epoch: 1 step: 640, loss is 3.4025492668151855\n",
      "epoch: 2 step: 16, loss is 4.509145736694336\n",
      "epoch: 2 step: 36, loss is 2.1028599739074707\n",
      "epoch: 2 step: 56, loss is 2.5803475379943848\n",
      "epoch: 2 step: 76, loss is 3.8821537494659424\n",
      "epoch: 2 step: 96, loss is 3.501756191253662\n",
      "epoch: 2 step: 116, loss is 3.5629711151123047\n",
      "epoch: 2 step: 136, loss is 3.7730817794799805\n",
      "epoch: 2 step: 156, loss is 3.3385300636291504\n",
      "epoch: 2 step: 176, loss is 3.7282166481018066\n",
      "epoch: 2 step: 196, loss is 3.362848997116089\n",
      "epoch: 2 step: 216, loss is 3.3682854175567627\n",
      "epoch: 2 step: 236, loss is 4.575628280639648\n",
      "epoch: 2 step: 256, loss is 3.954462766647339\n",
      "epoch: 2 step: 276, loss is 2.9329514503479004\n",
      "epoch: 2 step: 296, loss is 3.260279655456543\n",
      "epoch: 2 step: 316, loss is 4.018902778625488\n",
      "epoch: 2 step: 336, loss is 3.120652914047241\n",
      "epoch: 2 step: 356, loss is 4.095616817474365\n",
      "epoch: 2 step: 376, loss is 3.219597101211548\n",
      "epoch: 2 step: 396, loss is 3.187103748321533\n",
      "epoch: 2 step: 416, loss is 3.5330145359039307\n",
      "epoch: 2 step: 436, loss is 3.17167067527771\n",
      "epoch: 2 step: 456, loss is 2.843355178833008\n",
      "epoch: 2 step: 476, loss is 2.987724542617798\n",
      "epoch: 2 step: 496, loss is 3.479135751724243\n",
      "epoch: 2 step: 516, loss is 3.3477747440338135\n",
      "epoch: 2 step: 536, loss is 2.7210474014282227\n",
      "epoch: 2 step: 556, loss is 3.037017345428467\n",
      "epoch: 2 step: 576, loss is 2.8584060668945312\n",
      "epoch: 2 step: 596, loss is 3.4867594242095947\n",
      "epoch: 2 step: 616, loss is 2.7355635166168213\n",
      "epoch: 2 step: 636, loss is 3.0379366874694824\n",
      "epoch: 3 step: 12, loss is 2.9977810382843018\n",
      "epoch: 3 step: 32, loss is 2.585012912750244\n",
      "epoch: 3 step: 52, loss is 3.248656749725342\n",
      "epoch: 3 step: 72, loss is 3.3389058113098145\n",
      "epoch: 3 step: 92, loss is 3.4307661056518555\n",
      "epoch: 3 step: 112, loss is 3.7306952476501465\n",
      "epoch: 3 step: 132, loss is 2.332711935043335\n",
      "epoch: 3 step: 152, loss is 3.3370888233184814\n",
      "epoch: 3 step: 172, loss is 3.420182704925537\n",
      "epoch: 3 step: 192, loss is 3.6135876178741455\n",
      "epoch: 3 step: 212, loss is 3.2541818618774414\n",
      "epoch: 3 step: 232, loss is 3.0261669158935547\n",
      "epoch: 3 step: 252, loss is 2.869586944580078\n",
      "epoch: 3 step: 272, loss is 3.0730128288269043\n",
      "epoch: 3 step: 292, loss is 3.386103630065918\n",
      "epoch: 3 step: 312, loss is 1.9067223072052002\n",
      "epoch: 3 step: 332, loss is 2.4119949340820312\n",
      "epoch: 3 step: 352, loss is 3.313817262649536\n",
      "epoch: 3 step: 372, loss is 3.7992334365844727\n",
      "epoch: 3 step: 392, loss is 3.1055102348327637\n",
      "epoch: 3 step: 412, loss is 4.131277561187744\n",
      "epoch: 3 step: 432, loss is 2.5144782066345215\n",
      "epoch: 3 step: 452, loss is 2.669318437576294\n",
      "epoch: 3 step: 472, loss is 4.017714977264404\n",
      "epoch: 3 step: 492, loss is 4.362669944763184\n",
      "epoch: 3 step: 512, loss is 3.722585678100586\n",
      "epoch: 3 step: 532, loss is 3.215291976928711\n",
      "epoch: 3 step: 552, loss is 2.3319764137268066\n",
      "epoch: 3 step: 572, loss is 3.729297161102295\n",
      "epoch: 3 step: 592, loss is 2.837831497192383\n",
      "epoch: 3 step: 612, loss is 2.1874160766601562\n",
      "epoch: 3 step: 632, loss is 2.071779727935791\n",
      "epoch: 4 step: 8, loss is 3.0104117393493652\n",
      "epoch: 4 step: 28, loss is 2.9055802822113037\n",
      "epoch: 4 step: 48, loss is 2.9004955291748047\n",
      "epoch: 4 step: 68, loss is 2.443234443664551\n",
      "epoch: 4 step: 88, loss is 2.1387345790863037\n",
      "epoch: 4 step: 108, loss is 3.5231099128723145\n",
      "epoch: 4 step: 128, loss is 2.9000027179718018\n",
      "epoch: 4 step: 148, loss is 3.2900643348693848\n",
      "epoch: 4 step: 168, loss is 2.0737645626068115\n",
      "epoch: 4 step: 188, loss is 2.5844388008117676\n",
      "epoch: 4 step: 208, loss is 1.9210245609283447\n",
      "epoch: 4 step: 228, loss is 2.635148048400879\n",
      "epoch: 4 step: 248, loss is 1.4970192909240723\n",
      "epoch: 4 step: 268, loss is 2.9738388061523438\n",
      "epoch: 4 step: 288, loss is 2.7900772094726562\n",
      "epoch: 4 step: 308, loss is 3.3290576934814453\n",
      "epoch: 4 step: 328, loss is 3.462557792663574\n",
      "epoch: 4 step: 348, loss is 1.7892876863479614\n",
      "epoch: 4 step: 368, loss is 2.2486319541931152\n",
      "epoch: 4 step: 388, loss is 2.828738212585449\n",
      "epoch: 4 step: 408, loss is 2.2924647331237793\n",
      "epoch: 4 step: 428, loss is 2.8065271377563477\n",
      "epoch: 4 step: 448, loss is 2.259221076965332\n",
      "epoch: 4 step: 468, loss is 2.582808256149292\n",
      "epoch: 4 step: 488, loss is 1.9168462753295898\n",
      "epoch: 4 step: 508, loss is 4.007031440734863\n",
      "epoch: 4 step: 528, loss is 1.7469854354858398\n",
      "epoch: 4 step: 548, loss is 2.6824769973754883\n",
      "epoch: 4 step: 568, loss is 1.875719428062439\n",
      "epoch: 4 step: 588, loss is 1.969812035560608\n",
      "epoch: 4 step: 608, loss is 3.17946720123291\n",
      "epoch: 4 step: 628, loss is 2.7661614418029785\n",
      "epoch: 5 step: 4, loss is 2.523751735687256\n",
      "epoch: 5 step: 24, loss is 2.3886477947235107\n",
      "epoch: 5 step: 44, loss is 1.919185757637024\n",
      "epoch: 5 step: 64, loss is 1.7921665906906128\n",
      "epoch: 5 step: 84, loss is 2.4005136489868164\n",
      "epoch: 5 step: 104, loss is 2.5917580127716064\n",
      "epoch: 5 step: 124, loss is 2.585157871246338\n",
      "epoch: 5 step: 144, loss is 1.755359411239624\n",
      "epoch: 5 step: 164, loss is 2.233250141143799\n",
      "epoch: 5 step: 184, loss is 1.904883861541748\n",
      "epoch: 5 step: 204, loss is 1.946573257446289\n",
      "epoch: 5 step: 224, loss is 2.7659082412719727\n",
      "epoch: 5 step: 244, loss is 1.4595675468444824\n",
      "epoch: 5 step: 264, loss is 1.5994787216186523\n",
      "epoch: 5 step: 284, loss is 2.2924604415893555\n",
      "epoch: 5 step: 304, loss is 1.6804041862487793\n",
      "epoch: 5 step: 324, loss is 2.389014720916748\n",
      "epoch: 5 step: 344, loss is 2.06150484085083\n",
      "epoch: 5 step: 364, loss is 1.805682897567749\n",
      "epoch: 5 step: 384, loss is 1.776914358139038\n",
      "epoch: 5 step: 404, loss is 2.6060380935668945\n",
      "epoch: 5 step: 424, loss is 2.179489850997925\n",
      "epoch: 5 step: 444, loss is 2.280202627182007\n",
      "epoch: 5 step: 464, loss is 2.011141300201416\n",
      "epoch: 5 step: 484, loss is 1.9689762592315674\n",
      "epoch: 5 step: 504, loss is 2.1524579524993896\n",
      "epoch: 5 step: 524, loss is 2.060880661010742\n",
      "epoch: 5 step: 544, loss is 1.2024720907211304\n",
      "epoch: 5 step: 564, loss is 1.5560758113861084\n",
      "epoch: 5 step: 584, loss is 1.4365780353546143\n",
      "epoch: 5 step: 604, loss is 1.9588669538497925\n",
      "epoch: 5 step: 624, loss is 2.7911720275878906\n",
      "epoch: 5 step: 644, loss is 1.373628854751587\n",
      "epoch: 6 step: 20, loss is 1.4348046779632568\n",
      "epoch: 6 step: 40, loss is 2.010711669921875\n",
      "epoch: 6 step: 60, loss is 2.013392448425293\n",
      "epoch: 6 step: 80, loss is 3.1393356323242188\n",
      "epoch: 6 step: 100, loss is 2.4816999435424805\n",
      "epoch: 6 step: 120, loss is 1.628434419631958\n",
      "epoch: 6 step: 140, loss is 1.2590198516845703\n",
      "epoch: 6 step: 160, loss is 1.7877893447875977\n",
      "epoch: 6 step: 180, loss is 1.1161350011825562\n",
      "epoch: 6 step: 200, loss is 3.4058923721313477\n",
      "epoch: 6 step: 220, loss is 3.419649362564087\n",
      "epoch: 6 step: 240, loss is 1.5487377643585205\n",
      "epoch: 6 step: 260, loss is 2.421776294708252\n",
      "epoch: 6 step: 280, loss is 2.037935256958008\n",
      "epoch: 6 step: 300, loss is 2.9964237213134766\n",
      "epoch: 6 step: 320, loss is 2.3063747882843018\n",
      "epoch: 6 step: 340, loss is 1.1831600666046143\n",
      "epoch: 6 step: 360, loss is 3.576791763305664\n",
      "epoch: 6 step: 380, loss is 1.5037198066711426\n",
      "epoch: 6 step: 400, loss is 1.4172978401184082\n",
      "epoch: 6 step: 420, loss is 1.4200693368911743\n",
      "epoch: 6 step: 440, loss is 2.3608405590057373\n",
      "epoch: 6 step: 460, loss is 1.766852617263794\n",
      "epoch: 6 step: 480, loss is 1.3734683990478516\n",
      "epoch: 6 step: 500, loss is 1.7062031030654907\n",
      "epoch: 6 step: 520, loss is 1.5756396055221558\n",
      "epoch: 6 step: 540, loss is 1.2079463005065918\n",
      "epoch: 6 step: 560, loss is 1.5454519987106323\n",
      "epoch: 6 step: 580, loss is 2.0412344932556152\n",
      "epoch: 6 step: 600, loss is 2.1661996841430664\n",
      "epoch: 6 step: 620, loss is 3.312932014465332\n",
      "epoch: 6 step: 640, loss is 1.8731002807617188\n",
      "epoch: 7 step: 16, loss is 2.6235973834991455\n",
      "epoch: 7 step: 36, loss is 1.4346354007720947\n",
      "epoch: 7 step: 56, loss is 2.383777141571045\n",
      "epoch: 7 step: 76, loss is 1.6781511306762695\n",
      "epoch: 7 step: 96, loss is 1.3462518453598022\n",
      "epoch: 7 step: 116, loss is 1.0901249647140503\n",
      "epoch: 7 step: 136, loss is 2.2039430141448975\n",
      "epoch: 7 step: 156, loss is 1.5700615644454956\n",
      "epoch: 7 step: 176, loss is 1.4599721431732178\n",
      "epoch: 7 step: 196, loss is 1.5516393184661865\n",
      "epoch: 7 step: 216, loss is 2.137852191925049\n",
      "epoch: 7 step: 236, loss is 2.263735771179199\n",
      "epoch: 7 step: 256, loss is 1.6424782276153564\n",
      "epoch: 7 step: 276, loss is 1.9480115175247192\n",
      "epoch: 7 step: 296, loss is 1.3170287609100342\n",
      "epoch: 7 step: 316, loss is 1.3832958936691284\n",
      "epoch: 7 step: 336, loss is 1.5730464458465576\n",
      "epoch: 7 step: 356, loss is 1.9286653995513916\n",
      "epoch: 7 step: 376, loss is 1.6494535207748413\n",
      "epoch: 7 step: 396, loss is 1.5600543022155762\n",
      "epoch: 7 step: 416, loss is 1.3718957901000977\n",
      "epoch: 7 step: 436, loss is 2.6293869018554688\n",
      "epoch: 7 step: 456, loss is 1.3868427276611328\n",
      "epoch: 7 step: 476, loss is 0.5766772627830505\n",
      "epoch: 7 step: 496, loss is 1.7358911037445068\n",
      "epoch: 7 step: 516, loss is 2.4043397903442383\n",
      "epoch: 7 step: 536, loss is 2.4920754432678223\n",
      "epoch: 7 step: 556, loss is 0.85036301612854\n",
      "epoch: 7 step: 576, loss is 1.299971342086792\n",
      "epoch: 7 step: 596, loss is 1.0907796621322632\n",
      "epoch: 7 step: 616, loss is 1.6143438816070557\n",
      "epoch: 7 step: 636, loss is 2.1532504558563232\n",
      "epoch: 8 step: 12, loss is 1.4176342487335205\n",
      "epoch: 8 step: 32, loss is 3.0913825035095215\n",
      "epoch: 8 step: 52, loss is 1.9606592655181885\n",
      "epoch: 8 step: 72, loss is 1.3666939735412598\n",
      "epoch: 8 step: 92, loss is 2.052290201187134\n",
      "epoch: 8 step: 112, loss is 0.6709403991699219\n",
      "epoch: 8 step: 132, loss is 1.5466159582138062\n",
      "epoch: 8 step: 152, loss is 1.5029985904693604\n",
      "epoch: 8 step: 172, loss is 1.0565171241760254\n",
      "epoch: 8 step: 192, loss is 1.9531667232513428\n",
      "epoch: 8 step: 212, loss is 2.6530275344848633\n",
      "epoch: 8 step: 232, loss is 1.6327738761901855\n",
      "epoch: 8 step: 252, loss is 1.4624935388565063\n",
      "epoch: 8 step: 272, loss is 1.5161161422729492\n",
      "epoch: 8 step: 292, loss is 0.8646835088729858\n",
      "epoch: 8 step: 312, loss is 0.8723614811897278\n",
      "epoch: 8 step: 332, loss is 1.9397202730178833\n",
      "epoch: 8 step: 352, loss is 1.9297473430633545\n",
      "epoch: 8 step: 372, loss is 1.4110469818115234\n",
      "epoch: 8 step: 392, loss is 1.8912793397903442\n",
      "epoch: 8 step: 412, loss is 1.4723105430603027\n",
      "epoch: 8 step: 432, loss is 1.2734867334365845\n",
      "epoch: 8 step: 452, loss is 1.3273872137069702\n",
      "epoch: 8 step: 472, loss is 2.0162153244018555\n",
      "epoch: 8 step: 492, loss is 1.3804352283477783\n",
      "epoch: 8 step: 512, loss is 1.1367878913879395\n",
      "epoch: 8 step: 532, loss is 1.7851961851119995\n",
      "epoch: 8 step: 552, loss is 1.9957739114761353\n",
      "epoch: 8 step: 572, loss is 1.6635520458221436\n",
      "epoch: 8 step: 592, loss is 1.841933250427246\n",
      "epoch: 8 step: 612, loss is 1.3039408922195435\n",
      "epoch: 8 step: 632, loss is 1.6319398880004883\n",
      "epoch: 9 step: 8, loss is 1.326233148574829\n",
      "epoch: 9 step: 28, loss is 1.9598957300186157\n",
      "epoch: 9 step: 48, loss is 1.5219130516052246\n",
      "epoch: 9 step: 68, loss is 2.3265137672424316\n",
      "epoch: 9 step: 88, loss is 2.0686488151550293\n",
      "epoch: 9 step: 108, loss is 1.6300996541976929\n",
      "epoch: 9 step: 128, loss is 1.600329041481018\n",
      "epoch: 9 step: 148, loss is 1.636958360671997\n",
      "epoch: 9 step: 168, loss is 2.157217502593994\n",
      "epoch: 9 step: 188, loss is 1.3073712587356567\n",
      "epoch: 9 step: 208, loss is 3.039062738418579\n",
      "epoch: 9 step: 228, loss is 2.0941288471221924\n",
      "epoch: 9 step: 248, loss is 2.146470785140991\n",
      "epoch: 9 step: 268, loss is 1.5992016792297363\n",
      "epoch: 9 step: 288, loss is 0.6483487486839294\n",
      "epoch: 9 step: 308, loss is 1.4520267248153687\n",
      "epoch: 9 step: 328, loss is 1.2461742162704468\n",
      "epoch: 9 step: 348, loss is 2.013761281967163\n",
      "epoch: 9 step: 368, loss is 2.315614700317383\n",
      "epoch: 9 step: 388, loss is 1.5696340799331665\n",
      "epoch: 9 step: 408, loss is 2.2333593368530273\n",
      "epoch: 9 step: 428, loss is 1.643688440322876\n",
      "epoch: 9 step: 448, loss is 1.0612813234329224\n",
      "epoch: 9 step: 468, loss is 1.243777871131897\n",
      "epoch: 9 step: 488, loss is 1.8283790349960327\n",
      "epoch: 9 step: 508, loss is 1.6945421695709229\n",
      "epoch: 9 step: 528, loss is 1.0575480461120605\n",
      "epoch: 9 step: 548, loss is 1.9805073738098145\n",
      "epoch: 9 step: 568, loss is 1.4449446201324463\n",
      "epoch: 9 step: 588, loss is 1.525897741317749\n",
      "epoch: 9 step: 608, loss is 0.8673492074012756\n",
      "epoch: 9 step: 628, loss is 1.0945733785629272\n",
      "epoch: 10 step: 4, loss is 1.70728600025177\n",
      "epoch: 10 step: 24, loss is 2.0018296241760254\n",
      "epoch: 10 step: 44, loss is 0.720720648765564\n",
      "epoch: 10 step: 64, loss is 1.8261256217956543\n",
      "epoch: 10 step: 84, loss is 1.306488037109375\n",
      "epoch: 10 step: 104, loss is 1.5362086296081543\n",
      "epoch: 10 step: 124, loss is 1.8844903707504272\n",
      "epoch: 10 step: 144, loss is 1.468966007232666\n",
      "epoch: 10 step: 164, loss is 0.8768717050552368\n",
      "epoch: 10 step: 184, loss is 1.7320644855499268\n",
      "epoch: 10 step: 204, loss is 1.4779095649719238\n",
      "epoch: 10 step: 224, loss is 1.4246379137039185\n",
      "epoch: 10 step: 244, loss is 1.3520692586898804\n",
      "epoch: 10 step: 264, loss is 1.6395111083984375\n",
      "epoch: 10 step: 284, loss is 2.4612789154052734\n",
      "epoch: 10 step: 304, loss is 1.6445372104644775\n",
      "epoch: 10 step: 324, loss is 0.9874206781387329\n",
      "epoch: 10 step: 344, loss is 0.9605167508125305\n",
      "epoch: 10 step: 364, loss is 2.159125804901123\n",
      "epoch: 10 step: 384, loss is 1.386296033859253\n",
      "epoch: 10 step: 404, loss is 1.8003463745117188\n",
      "epoch: 10 step: 424, loss is 1.447635531425476\n",
      "epoch: 10 step: 444, loss is 1.4405122995376587\n",
      "epoch: 10 step: 464, loss is 1.0281262397766113\n",
      "epoch: 10 step: 484, loss is 1.0012381076812744\n",
      "epoch: 10 step: 504, loss is 1.1986010074615479\n",
      "epoch: 10 step: 524, loss is 1.0298668146133423\n",
      "epoch: 10 step: 544, loss is 1.3184356689453125\n",
      "epoch: 10 step: 564, loss is 1.8874071836471558\n",
      "epoch: 10 step: 584, loss is 1.6954177618026733\n",
      "epoch: 10 step: 604, loss is 0.9048141241073608\n",
      "epoch: 10 step: 624, loss is 0.5382093787193298\n",
      "epoch: 10 step: 644, loss is 0.9498060941696167\n",
      "epoch: 11 step: 20, loss is 1.0583151578903198\n",
      "epoch: 11 step: 40, loss is 1.3173657655715942\n",
      "epoch: 11 step: 60, loss is 1.3100507259368896\n",
      "epoch: 11 step: 80, loss is 1.8976972103118896\n",
      "epoch: 11 step: 100, loss is 2.800771951675415\n",
      "epoch: 11 step: 120, loss is 1.012507677078247\n",
      "epoch: 11 step: 140, loss is 1.3572455644607544\n",
      "epoch: 11 step: 160, loss is 1.7329771518707275\n",
      "epoch: 11 step: 180, loss is 1.6348543167114258\n",
      "epoch: 11 step: 200, loss is 1.2956056594848633\n",
      "epoch: 11 step: 220, loss is 1.5621604919433594\n",
      "epoch: 11 step: 240, loss is 1.4317755699157715\n",
      "epoch: 11 step: 260, loss is 2.4528751373291016\n",
      "epoch: 11 step: 280, loss is 0.8876857757568359\n",
      "epoch: 11 step: 300, loss is 2.073775291442871\n",
      "epoch: 11 step: 320, loss is 1.871262550354004\n",
      "epoch: 11 step: 340, loss is 1.445719838142395\n",
      "epoch: 11 step: 360, loss is 1.4405450820922852\n",
      "epoch: 11 step: 380, loss is 2.1316545009613037\n",
      "epoch: 11 step: 400, loss is 3.2869157791137695\n",
      "epoch: 11 step: 420, loss is 1.6525102853775024\n",
      "epoch: 11 step: 440, loss is 2.7192459106445312\n",
      "epoch: 11 step: 460, loss is 0.9788044691085815\n",
      "epoch: 11 step: 480, loss is 1.3582966327667236\n",
      "epoch: 11 step: 500, loss is 1.5183305740356445\n",
      "epoch: 11 step: 520, loss is 1.3147412538528442\n",
      "epoch: 11 step: 540, loss is 0.9027664065361023\n",
      "epoch: 11 step: 560, loss is 0.8756932020187378\n",
      "epoch: 11 step: 580, loss is 1.4676603078842163\n",
      "epoch: 11 step: 600, loss is 1.6922866106033325\n",
      "epoch: 11 step: 620, loss is 1.4850447177886963\n",
      "epoch: 11 step: 640, loss is 1.7801779508590698\n",
      "epoch: 12 step: 16, loss is 1.1046414375305176\n",
      "epoch: 12 step: 36, loss is 1.9310214519500732\n",
      "epoch: 12 step: 56, loss is 0.36862480640411377\n",
      "epoch: 12 step: 76, loss is 1.8850746154785156\n",
      "epoch: 12 step: 96, loss is 0.7834808826446533\n",
      "epoch: 12 step: 116, loss is 2.242021083831787\n",
      "epoch: 12 step: 136, loss is 2.025269031524658\n",
      "epoch: 12 step: 156, loss is 0.63016676902771\n",
      "epoch: 12 step: 176, loss is 0.3991768956184387\n",
      "epoch: 12 step: 196, loss is 1.5917757749557495\n",
      "epoch: 12 step: 216, loss is 0.8067003488540649\n",
      "epoch: 12 step: 236, loss is 0.9091920852661133\n",
      "epoch: 12 step: 256, loss is 1.2338659763336182\n",
      "epoch: 12 step: 276, loss is 1.4359544515609741\n",
      "epoch: 12 step: 296, loss is 2.0635414123535156\n",
      "epoch: 12 step: 316, loss is 1.3336588144302368\n",
      "epoch: 12 step: 336, loss is 1.7335591316223145\n",
      "epoch: 12 step: 356, loss is 0.8121359348297119\n",
      "epoch: 12 step: 376, loss is 1.4966366291046143\n",
      "epoch: 12 step: 396, loss is 1.7381246089935303\n",
      "epoch: 12 step: 416, loss is 1.7755699157714844\n",
      "epoch: 12 step: 436, loss is 1.723572015762329\n",
      "epoch: 12 step: 456, loss is 1.008500576019287\n",
      "epoch: 12 step: 476, loss is 1.7383551597595215\n",
      "epoch: 12 step: 496, loss is 0.5002912282943726\n",
      "epoch: 12 step: 516, loss is 0.6714131236076355\n",
      "epoch: 12 step: 536, loss is 1.3352956771850586\n",
      "epoch: 12 step: 556, loss is 1.7121831178665161\n",
      "epoch: 12 step: 576, loss is 1.411611557006836\n",
      "epoch: 12 step: 596, loss is 2.131006956100464\n",
      "epoch: 12 step: 616, loss is 1.4157497882843018\n",
      "epoch: 12 step: 636, loss is 1.962529182434082\n",
      "epoch: 13 step: 12, loss is 1.0079333782196045\n",
      "epoch: 13 step: 32, loss is 1.6376681327819824\n",
      "epoch: 13 step: 52, loss is 1.4337553977966309\n",
      "epoch: 13 step: 72, loss is 2.131706953048706\n",
      "epoch: 13 step: 92, loss is 3.1893506050109863\n",
      "epoch: 13 step: 112, loss is 0.8662774562835693\n",
      "epoch: 13 step: 132, loss is 0.5923596620559692\n",
      "epoch: 13 step: 152, loss is 0.7677662372589111\n",
      "epoch: 13 step: 172, loss is 1.341264247894287\n",
      "epoch: 13 step: 192, loss is 1.1919492483139038\n",
      "epoch: 13 step: 212, loss is 1.5959606170654297\n",
      "epoch: 13 step: 232, loss is 1.051809310913086\n",
      "epoch: 13 step: 252, loss is 1.3054304122924805\n",
      "epoch: 13 step: 272, loss is 1.8873865604400635\n",
      "epoch: 13 step: 292, loss is 1.3987525701522827\n",
      "epoch: 13 step: 312, loss is 0.6654103994369507\n",
      "epoch: 13 step: 332, loss is 0.9910561442375183\n",
      "epoch: 13 step: 352, loss is 0.7855793833732605\n",
      "epoch: 13 step: 372, loss is 1.7263662815093994\n",
      "epoch: 13 step: 392, loss is 2.2014431953430176\n",
      "epoch: 13 step: 412, loss is 1.6801289319992065\n",
      "epoch: 13 step: 432, loss is 0.5593265295028687\n",
      "epoch: 13 step: 452, loss is 1.3757706880569458\n",
      "epoch: 13 step: 472, loss is 1.3524653911590576\n",
      "epoch: 13 step: 492, loss is 1.8683674335479736\n",
      "epoch: 13 step: 512, loss is 2.1713767051696777\n",
      "epoch: 13 step: 532, loss is 0.9984009265899658\n",
      "epoch: 13 step: 552, loss is 1.2154816389083862\n",
      "epoch: 13 step: 572, loss is 0.9770654439926147\n",
      "epoch: 13 step: 592, loss is 2.148245334625244\n",
      "epoch: 13 step: 612, loss is 0.7538137435913086\n",
      "epoch: 13 step: 632, loss is 0.3465551733970642\n",
      "epoch: 14 step: 8, loss is 1.8119993209838867\n",
      "epoch: 14 step: 28, loss is 0.7220839262008667\n",
      "epoch: 14 step: 48, loss is 1.028082251548767\n",
      "epoch: 14 step: 68, loss is 0.36351144313812256\n",
      "epoch: 14 step: 88, loss is 1.339735746383667\n",
      "epoch: 14 step: 108, loss is 1.0551378726959229\n",
      "epoch: 14 step: 128, loss is 0.737848162651062\n",
      "epoch: 14 step: 148, loss is 0.6778227090835571\n",
      "epoch: 14 step: 168, loss is 1.6055105924606323\n",
      "epoch: 14 step: 188, loss is 0.6802518367767334\n",
      "epoch: 14 step: 208, loss is 1.00095796585083\n",
      "epoch: 14 step: 228, loss is 1.15718674659729\n",
      "epoch: 14 step: 248, loss is 1.4540423154830933\n",
      "epoch: 14 step: 268, loss is 1.6307618618011475\n",
      "epoch: 14 step: 288, loss is 1.1997326612472534\n",
      "epoch: 14 step: 308, loss is 1.9571869373321533\n",
      "epoch: 14 step: 328, loss is 0.588616132736206\n",
      "epoch: 14 step: 348, loss is 1.3144478797912598\n",
      "epoch: 14 step: 368, loss is 2.311875343322754\n",
      "epoch: 14 step: 388, loss is 1.1462523937225342\n",
      "epoch: 14 step: 408, loss is 1.5895129442214966\n",
      "epoch: 14 step: 428, loss is 0.7340607643127441\n",
      "epoch: 14 step: 448, loss is 1.027085304260254\n",
      "epoch: 14 step: 468, loss is 1.3209500312805176\n",
      "epoch: 14 step: 488, loss is 0.20508617162704468\n",
      "epoch: 14 step: 508, loss is 0.5738251209259033\n",
      "epoch: 14 step: 528, loss is 0.5941346883773804\n",
      "epoch: 14 step: 548, loss is 1.1505706310272217\n",
      "epoch: 14 step: 568, loss is 0.7579870820045471\n",
      "epoch: 14 step: 588, loss is 0.290268212556839\n",
      "epoch: 14 step: 608, loss is 0.1620335578918457\n",
      "epoch: 14 step: 628, loss is 1.012864112854004\n",
      "epoch: 15 step: 4, loss is 1.095604658126831\n",
      "epoch: 15 step: 24, loss is 0.5617169141769409\n",
      "epoch: 15 step: 44, loss is 0.6630169153213501\n",
      "epoch: 15 step: 64, loss is 0.42918455600738525\n",
      "epoch: 15 step: 84, loss is 0.5918524265289307\n",
      "epoch: 15 step: 104, loss is 1.5730044841766357\n",
      "epoch: 15 step: 124, loss is 0.2928904891014099\n",
      "epoch: 15 step: 144, loss is 0.26495975255966187\n",
      "epoch: 15 step: 164, loss is 0.7751787304878235\n",
      "epoch: 15 step: 184, loss is 0.289165735244751\n",
      "epoch: 15 step: 204, loss is 1.0493520498275757\n",
      "epoch: 15 step: 224, loss is 1.0951558351516724\n",
      "epoch: 15 step: 244, loss is 0.8846170902252197\n",
      "epoch: 15 step: 264, loss is 0.9796821475028992\n",
      "epoch: 15 step: 284, loss is 0.4998597502708435\n",
      "epoch: 15 step: 304, loss is 0.7075047492980957\n",
      "epoch: 15 step: 324, loss is 0.8617863059043884\n",
      "epoch: 15 step: 344, loss is 0.8464143872261047\n",
      "epoch: 15 step: 364, loss is 0.9448611736297607\n",
      "epoch: 15 step: 384, loss is 0.5141619443893433\n",
      "epoch: 15 step: 404, loss is 0.7482223510742188\n",
      "epoch: 15 step: 424, loss is 2.2686502933502197\n",
      "epoch: 15 step: 444, loss is 0.7541457414627075\n",
      "epoch: 15 step: 464, loss is 0.666462779045105\n",
      "epoch: 15 step: 484, loss is 0.7898057699203491\n",
      "epoch: 15 step: 504, loss is 0.6323609948158264\n",
      "epoch: 15 step: 524, loss is 0.484919011592865\n",
      "epoch: 15 step: 544, loss is 0.7236605286598206\n",
      "epoch: 15 step: 564, loss is 1.3733655214309692\n",
      "epoch: 15 step: 584, loss is 0.8424955606460571\n",
      "epoch: 15 step: 604, loss is 0.9845132231712341\n",
      "epoch: 15 step: 624, loss is 0.6474192142486572\n",
      "epoch: 15 step: 644, loss is 0.4505113959312439\n",
      "epoch: 16 step: 20, loss is 0.652280330657959\n",
      "epoch: 16 step: 40, loss is 1.2234820127487183\n",
      "epoch: 16 step: 60, loss is 0.59885174036026\n",
      "epoch: 16 step: 80, loss is 1.1686761379241943\n",
      "epoch: 16 step: 100, loss is 0.7849249243736267\n",
      "epoch: 16 step: 120, loss is 0.41550445556640625\n",
      "epoch: 16 step: 140, loss is 1.7203660011291504\n",
      "epoch: 16 step: 160, loss is 0.9750475883483887\n",
      "epoch: 16 step: 180, loss is 1.0436259508132935\n",
      "epoch: 16 step: 200, loss is 1.081822395324707\n",
      "epoch: 16 step: 220, loss is 0.3173430562019348\n",
      "epoch: 16 step: 240, loss is 0.6352086663246155\n",
      "epoch: 16 step: 260, loss is 0.4508392810821533\n",
      "epoch: 16 step: 280, loss is 1.0194320678710938\n",
      "epoch: 16 step: 300, loss is 0.44106754660606384\n",
      "epoch: 16 step: 320, loss is 0.36078065633773804\n",
      "epoch: 16 step: 340, loss is 0.5731016397476196\n",
      "epoch: 16 step: 360, loss is 0.7801119089126587\n",
      "epoch: 16 step: 380, loss is 2.0184903144836426\n",
      "epoch: 16 step: 400, loss is 0.8130784034729004\n",
      "epoch: 16 step: 420, loss is 0.9182175397872925\n",
      "epoch: 16 step: 440, loss is 1.062186598777771\n",
      "epoch: 16 step: 460, loss is 1.3410146236419678\n",
      "epoch: 16 step: 480, loss is 0.3540511429309845\n",
      "epoch: 16 step: 500, loss is 0.879030704498291\n",
      "epoch: 16 step: 520, loss is 0.5394832491874695\n",
      "epoch: 16 step: 540, loss is 0.6586525440216064\n",
      "epoch: 16 step: 560, loss is 0.3614182472229004\n",
      "epoch: 16 step: 580, loss is 0.7403373718261719\n",
      "epoch: 16 step: 600, loss is 1.5936440229415894\n",
      "epoch: 16 step: 620, loss is 0.8255724906921387\n",
      "epoch: 16 step: 640, loss is 0.4531637728214264\n",
      "epoch: 17 step: 16, loss is 0.6655391454696655\n",
      "epoch: 17 step: 36, loss is 0.8234655857086182\n",
      "epoch: 17 step: 56, loss is 0.27471354603767395\n",
      "epoch: 17 step: 76, loss is 1.5717997550964355\n",
      "epoch: 17 step: 96, loss is 0.13317281007766724\n",
      "epoch: 17 step: 116, loss is 0.7714890241622925\n",
      "epoch: 17 step: 136, loss is 0.26515161991119385\n",
      "epoch: 17 step: 156, loss is 0.5123022794723511\n",
      "epoch: 17 step: 176, loss is 0.5456663370132446\n",
      "epoch: 17 step: 196, loss is 1.2731493711471558\n",
      "epoch: 17 step: 216, loss is 1.791410207748413\n",
      "epoch: 17 step: 236, loss is 0.7391328811645508\n",
      "epoch: 17 step: 256, loss is 1.1908633708953857\n",
      "epoch: 17 step: 276, loss is 0.5448155403137207\n",
      "epoch: 17 step: 296, loss is 1.0635323524475098\n",
      "epoch: 17 step: 316, loss is 0.2594693899154663\n",
      "epoch: 17 step: 336, loss is 0.41366004943847656\n",
      "epoch: 17 step: 356, loss is 1.1410796642303467\n",
      "epoch: 17 step: 376, loss is 0.3255597651004791\n",
      "epoch: 17 step: 396, loss is 0.6087820529937744\n",
      "epoch: 17 step: 416, loss is 0.4454842805862427\n",
      "epoch: 17 step: 436, loss is 0.2933064103126526\n",
      "epoch: 17 step: 456, loss is 0.7514063119888306\n",
      "epoch: 17 step: 476, loss is 0.29496121406555176\n",
      "epoch: 17 step: 496, loss is 0.28233587741851807\n",
      "epoch: 17 step: 516, loss is 0.6859861612319946\n",
      "epoch: 17 step: 536, loss is 0.72813880443573\n",
      "epoch: 17 step: 556, loss is 0.9202365875244141\n",
      "epoch: 17 step: 576, loss is 1.0652775764465332\n",
      "epoch: 17 step: 596, loss is 0.4909213185310364\n",
      "epoch: 17 step: 616, loss is 0.2269400954246521\n",
      "epoch: 17 step: 636, loss is 1.9432988166809082\n",
      "epoch: 18 step: 12, loss is 0.12471377849578857\n",
      "epoch: 18 step: 32, loss is 0.6870617866516113\n",
      "epoch: 18 step: 52, loss is 0.554256021976471\n",
      "epoch: 18 step: 72, loss is 0.6496404409408569\n",
      "epoch: 18 step: 92, loss is 1.9361430406570435\n",
      "epoch: 18 step: 112, loss is 0.2993617057800293\n",
      "epoch: 18 step: 132, loss is 0.8078664541244507\n",
      "epoch: 18 step: 152, loss is 1.1459864377975464\n",
      "epoch: 18 step: 172, loss is 0.49173325300216675\n",
      "epoch: 18 step: 192, loss is 0.6378870010375977\n",
      "epoch: 18 step: 212, loss is 0.5649080872535706\n",
      "epoch: 18 step: 232, loss is 0.4847838282585144\n",
      "epoch: 18 step: 252, loss is 0.479457288980484\n",
      "epoch: 18 step: 272, loss is 0.5904800891876221\n",
      "epoch: 18 step: 292, loss is 0.5301405787467957\n",
      "epoch: 18 step: 312, loss is 0.9689311981201172\n",
      "epoch: 18 step: 332, loss is 0.37710708379745483\n",
      "epoch: 18 step: 352, loss is 0.2865782380104065\n",
      "epoch: 18 step: 372, loss is 0.20644555985927582\n",
      "epoch: 18 step: 392, loss is 0.5548571348190308\n",
      "epoch: 18 step: 412, loss is 0.912866473197937\n",
      "epoch: 18 step: 432, loss is 0.530142605304718\n",
      "epoch: 18 step: 452, loss is 0.8173048496246338\n",
      "epoch: 18 step: 472, loss is 1.2516210079193115\n",
      "epoch: 18 step: 492, loss is 0.24047373235225677\n",
      "epoch: 18 step: 512, loss is 0.3322042226791382\n",
      "epoch: 18 step: 532, loss is 0.30128616094589233\n",
      "epoch: 18 step: 552, loss is 0.14212217926979065\n",
      "epoch: 18 step: 572, loss is 1.165787935256958\n",
      "epoch: 18 step: 592, loss is 1.2294714450836182\n",
      "epoch: 18 step: 612, loss is 0.4365244507789612\n",
      "epoch: 18 step: 632, loss is 0.6790670156478882\n",
      "epoch: 19 step: 8, loss is 0.7941328287124634\n",
      "epoch: 19 step: 28, loss is 0.8983414173126221\n",
      "epoch: 19 step: 48, loss is 0.7592026591300964\n",
      "epoch: 19 step: 68, loss is 1.258764624595642\n",
      "epoch: 19 step: 88, loss is 0.9930226802825928\n",
      "epoch: 19 step: 108, loss is 0.6015121936798096\n",
      "epoch: 19 step: 128, loss is 0.886481523513794\n",
      "epoch: 19 step: 148, loss is 1.9162728786468506\n",
      "epoch: 19 step: 168, loss is 0.43231022357940674\n",
      "epoch: 19 step: 188, loss is 1.6728250980377197\n",
      "epoch: 19 step: 208, loss is 0.34614312648773193\n",
      "epoch: 19 step: 228, loss is 0.43233513832092285\n",
      "epoch: 19 step: 248, loss is 0.18335048854351044\n",
      "epoch: 19 step: 268, loss is 0.5323336124420166\n",
      "epoch: 19 step: 288, loss is 1.0256998538970947\n",
      "epoch: 19 step: 308, loss is 0.9085396528244019\n",
      "epoch: 19 step: 328, loss is 0.6055328845977783\n",
      "epoch: 19 step: 348, loss is 1.8764872550964355\n",
      "epoch: 19 step: 368, loss is 1.066469430923462\n",
      "epoch: 19 step: 388, loss is 1.2675138711929321\n",
      "epoch: 19 step: 408, loss is 1.5187456607818604\n",
      "epoch: 19 step: 428, loss is 0.6513700485229492\n",
      "epoch: 19 step: 448, loss is 1.1688023805618286\n",
      "epoch: 19 step: 468, loss is 0.17954397201538086\n",
      "epoch: 19 step: 488, loss is 0.4048639237880707\n",
      "epoch: 19 step: 508, loss is 0.3110013008117676\n",
      "epoch: 19 step: 528, loss is 0.9547141790390015\n",
      "epoch: 19 step: 548, loss is 0.9697200655937195\n",
      "epoch: 19 step: 568, loss is 0.15955066680908203\n",
      "epoch: 19 step: 588, loss is 0.17716370522975922\n",
      "epoch: 19 step: 608, loss is 0.9964013695716858\n",
      "epoch: 19 step: 628, loss is 0.08896639943122864\n",
      "epoch: 20 step: 4, loss is 0.7369649410247803\n",
      "epoch: 20 step: 24, loss is 0.21427255868911743\n",
      "epoch: 20 step: 44, loss is 0.251228392124176\n",
      "epoch: 20 step: 64, loss is 0.8215402364730835\n",
      "epoch: 20 step: 84, loss is 1.1626954078674316\n",
      "epoch: 20 step: 104, loss is 0.33143630623817444\n",
      "epoch: 20 step: 124, loss is 0.8082230687141418\n",
      "epoch: 20 step: 144, loss is 1.034249186515808\n",
      "epoch: 20 step: 164, loss is 0.899483859539032\n",
      "epoch: 20 step: 184, loss is 0.5465563535690308\n",
      "epoch: 20 step: 204, loss is 0.32465875148773193\n",
      "epoch: 20 step: 224, loss is 0.5491271018981934\n",
      "epoch: 20 step: 244, loss is 0.25207579135894775\n",
      "epoch: 20 step: 264, loss is 0.8084590435028076\n",
      "epoch: 20 step: 284, loss is 0.7010613679885864\n",
      "epoch: 20 step: 304, loss is 1.1788443326950073\n",
      "epoch: 20 step: 324, loss is 1.4318382740020752\n",
      "epoch: 20 step: 344, loss is 0.8871144652366638\n",
      "epoch: 20 step: 364, loss is 0.4041329622268677\n",
      "epoch: 20 step: 384, loss is 0.5624322891235352\n",
      "epoch: 20 step: 404, loss is 1.4273875951766968\n",
      "epoch: 20 step: 424, loss is 0.06075916439294815\n",
      "epoch: 20 step: 444, loss is 0.43996912240982056\n",
      "epoch: 20 step: 464, loss is 0.5321958661079407\n",
      "epoch: 20 step: 484, loss is 0.3917096257209778\n",
      "epoch: 20 step: 504, loss is 1.4956715106964111\n",
      "epoch: 20 step: 524, loss is 0.3512282371520996\n",
      "epoch: 20 step: 544, loss is 0.46414482593536377\n",
      "epoch: 20 step: 564, loss is 1.2933942079544067\n",
      "epoch: 20 step: 584, loss is 0.2333594560623169\n",
      "epoch: 20 step: 604, loss is 0.1896713525056839\n",
      "epoch: 20 step: 624, loss is 0.7202919721603394\n",
      "epoch: 20 step: 644, loss is 0.6412459015846252\n",
      "epoch: 21 step: 20, loss is 0.33645132184028625\n",
      "epoch: 21 step: 40, loss is 0.37011799216270447\n",
      "epoch: 21 step: 60, loss is 0.29220831394195557\n",
      "epoch: 21 step: 80, loss is 0.533099889755249\n",
      "epoch: 21 step: 100, loss is 0.8874159455299377\n",
      "epoch: 21 step: 120, loss is 0.6169638633728027\n",
      "epoch: 21 step: 140, loss is 1.8929262161254883\n",
      "epoch: 21 step: 160, loss is 0.49265751242637634\n",
      "epoch: 21 step: 180, loss is 1.0844080448150635\n",
      "epoch: 21 step: 200, loss is 0.4652942419052124\n",
      "epoch: 21 step: 220, loss is 0.3491199314594269\n",
      "epoch: 21 step: 240, loss is 1.1012325286865234\n",
      "epoch: 21 step: 260, loss is 1.1042358875274658\n",
      "epoch: 21 step: 280, loss is 0.9622070789337158\n",
      "epoch: 21 step: 300, loss is 0.5465341806411743\n",
      "epoch: 21 step: 320, loss is 0.8402570486068726\n",
      "epoch: 21 step: 340, loss is 0.20567312836647034\n",
      "epoch: 21 step: 360, loss is 0.2143910825252533\n",
      "epoch: 21 step: 380, loss is 1.7796474695205688\n",
      "epoch: 21 step: 400, loss is 1.9347963333129883\n",
      "epoch: 21 step: 420, loss is 0.16005271673202515\n",
      "epoch: 21 step: 440, loss is 0.47697633504867554\n",
      "epoch: 21 step: 460, loss is 0.5591856241226196\n",
      "epoch: 21 step: 480, loss is 0.8945000767707825\n",
      "epoch: 21 step: 500, loss is 0.29789504408836365\n",
      "epoch: 21 step: 520, loss is 0.9448103904724121\n",
      "epoch: 21 step: 540, loss is 0.6419051885604858\n",
      "epoch: 21 step: 560, loss is 0.13577939569950104\n",
      "epoch: 21 step: 580, loss is 0.3494647741317749\n",
      "epoch: 21 step: 600, loss is 0.20396187901496887\n",
      "epoch: 21 step: 620, loss is 1.8836466073989868\n",
      "epoch: 21 step: 640, loss is 0.43944042921066284\n",
      "epoch: 22 step: 16, loss is 0.170399010181427\n",
      "epoch: 22 step: 36, loss is 0.35767242312431335\n",
      "epoch: 22 step: 56, loss is 0.1317787617444992\n",
      "epoch: 22 step: 76, loss is 0.1630413830280304\n",
      "epoch: 22 step: 96, loss is 0.36754536628723145\n",
      "epoch: 22 step: 116, loss is 0.26919811964035034\n",
      "epoch: 22 step: 136, loss is 0.3160053491592407\n",
      "epoch: 22 step: 156, loss is 0.536644458770752\n",
      "epoch: 22 step: 176, loss is 0.1963856816291809\n",
      "epoch: 22 step: 196, loss is 1.0423049926757812\n",
      "epoch: 22 step: 216, loss is 0.1767541468143463\n",
      "epoch: 22 step: 236, loss is 0.19404016435146332\n",
      "epoch: 22 step: 256, loss is 0.24653466045856476\n",
      "epoch: 22 step: 276, loss is 0.5460947155952454\n",
      "epoch: 22 step: 296, loss is 0.18673697113990784\n",
      "epoch: 22 step: 316, loss is 0.6992709040641785\n",
      "epoch: 22 step: 336, loss is 0.40710264444351196\n",
      "epoch: 22 step: 356, loss is 0.08439282327890396\n",
      "epoch: 22 step: 376, loss is 0.8348665237426758\n",
      "epoch: 22 step: 396, loss is 0.3850824236869812\n",
      "epoch: 22 step: 416, loss is 0.415886789560318\n",
      "epoch: 22 step: 436, loss is 0.47750258445739746\n",
      "epoch: 22 step: 456, loss is 0.9138714075088501\n",
      "epoch: 22 step: 476, loss is 0.40202367305755615\n",
      "epoch: 22 step: 496, loss is 0.4576965272426605\n",
      "epoch: 22 step: 516, loss is 0.6249617338180542\n",
      "epoch: 22 step: 536, loss is 0.5169787406921387\n",
      "epoch: 22 step: 556, loss is 0.9310171008110046\n",
      "epoch: 22 step: 576, loss is 0.4333032965660095\n",
      "epoch: 22 step: 596, loss is 0.24275633692741394\n",
      "epoch: 22 step: 616, loss is 0.6705260276794434\n",
      "epoch: 22 step: 636, loss is 0.8879790902137756\n",
      "epoch: 23 step: 12, loss is 1.8324722051620483\n",
      "epoch: 23 step: 32, loss is 0.1734505593776703\n",
      "epoch: 23 step: 52, loss is 0.5987938642501831\n",
      "epoch: 23 step: 72, loss is 0.3674972653388977\n",
      "epoch: 23 step: 92, loss is 0.11664199829101562\n",
      "epoch: 23 step: 112, loss is 0.077203668653965\n",
      "epoch: 23 step: 132, loss is 1.2589259147644043\n",
      "epoch: 23 step: 152, loss is 0.18238413333892822\n",
      "epoch: 23 step: 172, loss is 0.2165234237909317\n",
      "epoch: 23 step: 192, loss is 0.4050714373588562\n",
      "epoch: 23 step: 212, loss is 0.494726300239563\n",
      "epoch: 23 step: 232, loss is 0.30066540837287903\n",
      "epoch: 23 step: 252, loss is 0.44830039143562317\n",
      "epoch: 23 step: 272, loss is 0.03535059839487076\n",
      "epoch: 23 step: 292, loss is 0.48290061950683594\n",
      "epoch: 23 step: 312, loss is 0.49515867233276367\n",
      "epoch: 23 step: 332, loss is 0.11372770369052887\n",
      "epoch: 23 step: 352, loss is 0.5145947933197021\n",
      "epoch: 23 step: 372, loss is 0.2270568162202835\n",
      "epoch: 23 step: 392, loss is 0.20884397625923157\n",
      "epoch: 23 step: 412, loss is 0.2833927869796753\n",
      "epoch: 23 step: 432, loss is 0.44657889008522034\n",
      "epoch: 23 step: 452, loss is 0.8217039108276367\n",
      "epoch: 23 step: 472, loss is 0.5293501615524292\n",
      "epoch: 23 step: 492, loss is 0.34128034114837646\n",
      "epoch: 23 step: 512, loss is 0.44020625948905945\n",
      "epoch: 23 step: 532, loss is 0.7925037741661072\n",
      "epoch: 23 step: 552, loss is 0.9443270564079285\n",
      "epoch: 23 step: 572, loss is 0.2875439524650574\n",
      "epoch: 23 step: 592, loss is 0.13834533095359802\n",
      "epoch: 23 step: 612, loss is 1.0120935440063477\n",
      "epoch: 23 step: 632, loss is 0.05348697304725647\n",
      "epoch: 24 step: 8, loss is 0.019638145342469215\n",
      "epoch: 24 step: 28, loss is 0.6307274699211121\n",
      "epoch: 24 step: 48, loss is 0.668438196182251\n",
      "epoch: 24 step: 68, loss is 0.5250872373580933\n",
      "epoch: 24 step: 88, loss is 0.3856600522994995\n",
      "epoch: 24 step: 108, loss is 0.44039201736450195\n",
      "epoch: 24 step: 128, loss is 1.0357294082641602\n",
      "epoch: 24 step: 148, loss is 0.21022440493106842\n",
      "epoch: 24 step: 168, loss is 0.15886232256889343\n",
      "epoch: 24 step: 188, loss is 1.0301673412322998\n",
      "epoch: 24 step: 208, loss is 0.2181876003742218\n",
      "epoch: 24 step: 228, loss is 0.30546051263809204\n",
      "epoch: 24 step: 248, loss is 0.17693838477134705\n",
      "epoch: 24 step: 268, loss is 0.2688223123550415\n",
      "epoch: 24 step: 288, loss is 0.2719205617904663\n",
      "epoch: 24 step: 308, loss is 0.05306773632764816\n",
      "epoch: 24 step: 328, loss is 1.223482370376587\n",
      "epoch: 24 step: 348, loss is 0.13534294068813324\n",
      "epoch: 24 step: 368, loss is 0.4773838222026825\n",
      "epoch: 24 step: 388, loss is 0.1494586318731308\n",
      "epoch: 24 step: 408, loss is 0.19162863492965698\n",
      "epoch: 24 step: 428, loss is 0.061350926756858826\n",
      "epoch: 24 step: 448, loss is 1.1186408996582031\n",
      "epoch: 24 step: 468, loss is 0.6110461950302124\n",
      "epoch: 24 step: 488, loss is 0.34694212675094604\n",
      "epoch: 24 step: 508, loss is 0.4482060670852661\n",
      "epoch: 24 step: 528, loss is 0.5898587703704834\n",
      "epoch: 24 step: 548, loss is 0.42203906178474426\n",
      "epoch: 24 step: 568, loss is 0.2849928140640259\n",
      "epoch: 24 step: 588, loss is 0.6138379573822021\n",
      "epoch: 24 step: 608, loss is 0.1813475787639618\n",
      "epoch: 24 step: 628, loss is 0.7852131724357605\n",
      "epoch: 25 step: 4, loss is 0.37063729763031006\n",
      "epoch: 25 step: 24, loss is 0.6979368329048157\n",
      "epoch: 25 step: 44, loss is 0.8493701219558716\n",
      "epoch: 25 step: 64, loss is 0.9652935862541199\n",
      "epoch: 25 step: 84, loss is 0.33137378096580505\n",
      "epoch: 25 step: 104, loss is 0.5335137248039246\n",
      "epoch: 25 step: 124, loss is 0.17878668010234833\n",
      "epoch: 25 step: 144, loss is 0.37106460332870483\n",
      "epoch: 25 step: 164, loss is 1.4990154504776\n",
      "epoch: 25 step: 184, loss is 0.3330490291118622\n",
      "epoch: 25 step: 204, loss is 0.6232205629348755\n",
      "epoch: 25 step: 224, loss is 0.7057129144668579\n",
      "epoch: 25 step: 244, loss is 0.20035037398338318\n",
      "epoch: 25 step: 264, loss is 0.7455989122390747\n",
      "epoch: 25 step: 284, loss is 0.3252946734428406\n",
      "epoch: 25 step: 304, loss is 0.20037733018398285\n",
      "epoch: 25 step: 324, loss is 0.32327020168304443\n",
      "epoch: 25 step: 344, loss is 0.03749881684780121\n",
      "epoch: 25 step: 364, loss is 0.04032690078020096\n",
      "epoch: 25 step: 384, loss is 1.0708752870559692\n",
      "epoch: 25 step: 404, loss is 0.5922589898109436\n",
      "epoch: 25 step: 424, loss is 0.814495325088501\n",
      "epoch: 25 step: 444, loss is 0.5164997577667236\n",
      "epoch: 25 step: 464, loss is 0.394950807094574\n",
      "epoch: 25 step: 484, loss is 0.5740600228309631\n",
      "epoch: 25 step: 504, loss is 0.27749788761138916\n",
      "epoch: 25 step: 524, loss is 0.2262544482946396\n",
      "epoch: 25 step: 544, loss is 0.3997160792350769\n",
      "epoch: 25 step: 564, loss is 1.8268580436706543\n",
      "epoch: 25 step: 584, loss is 2.08073353767395\n",
      "epoch: 25 step: 604, loss is 0.7096734046936035\n",
      "epoch: 25 step: 624, loss is 0.09065496921539307\n",
      "epoch: 25 step: 644, loss is 0.10762758553028107\n",
      "epoch: 26 step: 20, loss is 0.07773997634649277\n",
      "epoch: 26 step: 40, loss is 0.48322397470474243\n",
      "epoch: 26 step: 60, loss is 0.48469042778015137\n",
      "epoch: 26 step: 80, loss is 0.03181992471218109\n",
      "epoch: 26 step: 100, loss is 0.13826334476470947\n",
      "epoch: 26 step: 120, loss is 0.16656027734279633\n",
      "epoch: 26 step: 140, loss is 0.05261441320180893\n",
      "epoch: 26 step: 160, loss is 0.46163368225097656\n",
      "epoch: 26 step: 180, loss is 1.4995651245117188\n",
      "epoch: 26 step: 200, loss is 0.34093961119651794\n",
      "epoch: 26 step: 220, loss is 0.9732261300086975\n",
      "epoch: 26 step: 240, loss is 0.6331762075424194\n",
      "epoch: 26 step: 260, loss is 0.7888714671134949\n",
      "epoch: 26 step: 280, loss is 0.23425893485546112\n",
      "epoch: 26 step: 300, loss is 0.4043894410133362\n",
      "epoch: 26 step: 320, loss is 0.9205327033996582\n",
      "epoch: 26 step: 340, loss is 0.03968852758407593\n",
      "epoch: 26 step: 360, loss is 0.29993048310279846\n",
      "epoch: 26 step: 380, loss is 0.2428816854953766\n",
      "epoch: 26 step: 400, loss is 0.6913646459579468\n",
      "epoch: 26 step: 420, loss is 0.29893359541893005\n",
      "epoch: 26 step: 440, loss is 0.5452821850776672\n",
      "epoch: 26 step: 460, loss is 0.051816362887620926\n",
      "epoch: 26 step: 480, loss is 1.1341032981872559\n",
      "epoch: 26 step: 500, loss is 0.9771886467933655\n",
      "epoch: 26 step: 520, loss is 0.42579618096351624\n",
      "epoch: 26 step: 540, loss is 0.08605745434761047\n",
      "epoch: 26 step: 560, loss is 0.14944833517074585\n",
      "epoch: 26 step: 580, loss is 0.9353640079498291\n",
      "epoch: 26 step: 600, loss is 1.4522064924240112\n",
      "epoch: 26 step: 620, loss is 0.6066356897354126\n",
      "epoch: 26 step: 640, loss is 0.12182162702083588\n",
      "epoch: 27 step: 16, loss is 0.03982978314161301\n",
      "epoch: 27 step: 36, loss is 0.7340368032455444\n",
      "epoch: 27 step: 56, loss is 0.595859169960022\n",
      "epoch: 27 step: 76, loss is 0.25434166193008423\n",
      "epoch: 27 step: 96, loss is 0.4950389564037323\n",
      "epoch: 27 step: 116, loss is 1.075486183166504\n",
      "epoch: 27 step: 136, loss is 0.09020894765853882\n",
      "epoch: 27 step: 156, loss is 0.6428298950195312\n",
      "epoch: 27 step: 176, loss is 0.17278622090816498\n",
      "epoch: 27 step: 196, loss is 0.1328270435333252\n",
      "epoch: 27 step: 216, loss is 0.7671843767166138\n",
      "epoch: 27 step: 236, loss is 0.17561697959899902\n",
      "epoch: 27 step: 256, loss is 0.6505058407783508\n",
      "epoch: 27 step: 276, loss is 0.8266726136207581\n",
      "epoch: 27 step: 296, loss is 0.5669166445732117\n",
      "epoch: 27 step: 316, loss is 0.8922984600067139\n",
      "epoch: 27 step: 336, loss is 0.7553054094314575\n",
      "epoch: 27 step: 356, loss is 1.116859793663025\n",
      "epoch: 27 step: 376, loss is 0.5115839838981628\n",
      "epoch: 27 step: 396, loss is 0.14311078190803528\n",
      "epoch: 27 step: 416, loss is 0.17442992329597473\n",
      "epoch: 27 step: 436, loss is 1.152942419052124\n",
      "epoch: 27 step: 456, loss is 0.14370176196098328\n",
      "epoch: 27 step: 476, loss is 0.3634757101535797\n",
      "epoch: 27 step: 496, loss is 0.46462705731391907\n",
      "epoch: 27 step: 516, loss is 0.03195933997631073\n",
      "epoch: 27 step: 536, loss is 0.8413581252098083\n",
      "epoch: 27 step: 556, loss is 0.6000186800956726\n",
      "epoch: 27 step: 576, loss is 0.7018091678619385\n",
      "epoch: 27 step: 596, loss is 0.48320209980010986\n",
      "epoch: 27 step: 616, loss is 0.048192255198955536\n",
      "epoch: 27 step: 636, loss is 0.7483428716659546\n",
      "epoch: 28 step: 12, loss is 0.2515410780906677\n",
      "epoch: 28 step: 32, loss is 0.05427020788192749\n",
      "epoch: 28 step: 52, loss is 0.142697274684906\n",
      "epoch: 28 step: 72, loss is 0.6646154522895813\n",
      "epoch: 28 step: 92, loss is 0.5467788577079773\n",
      "epoch: 28 step: 112, loss is 0.28663313388824463\n",
      "epoch: 28 step: 132, loss is 0.9696770906448364\n",
      "epoch: 28 step: 152, loss is 0.10032118856906891\n",
      "epoch: 28 step: 172, loss is 0.30613261461257935\n",
      "epoch: 28 step: 192, loss is 0.14450567960739136\n",
      "epoch: 28 step: 212, loss is 1.2640010118484497\n",
      "epoch: 28 step: 232, loss is 1.5075221061706543\n",
      "epoch: 28 step: 252, loss is 0.6341180801391602\n",
      "epoch: 28 step: 272, loss is 1.0902878046035767\n",
      "epoch: 28 step: 292, loss is 0.7184292078018188\n",
      "epoch: 28 step: 312, loss is 0.1067415326833725\n",
      "epoch: 28 step: 332, loss is 1.3102082014083862\n",
      "epoch: 28 step: 352, loss is 0.14469057321548462\n",
      "epoch: 28 step: 372, loss is 1.0933500528335571\n",
      "epoch: 28 step: 392, loss is 0.15794752538204193\n",
      "epoch: 28 step: 412, loss is 0.06696868687868118\n",
      "epoch: 28 step: 432, loss is 0.3882443904876709\n",
      "epoch: 28 step: 452, loss is 0.3193930387496948\n",
      "epoch: 28 step: 472, loss is 2.6190943717956543\n",
      "epoch: 28 step: 492, loss is 0.22684365510940552\n",
      "epoch: 28 step: 512, loss is 0.11579511314630508\n",
      "epoch: 28 step: 532, loss is 0.5695791244506836\n",
      "epoch: 28 step: 552, loss is 0.12945277988910675\n",
      "epoch: 28 step: 572, loss is 0.6399567127227783\n",
      "epoch: 28 step: 592, loss is 0.40167760848999023\n",
      "epoch: 28 step: 612, loss is 0.19155080616474152\n",
      "epoch: 28 step: 632, loss is 0.024616971611976624\n",
      "epoch: 29 step: 8, loss is 0.2520994544029236\n",
      "epoch: 29 step: 28, loss is 0.13349083065986633\n",
      "epoch: 29 step: 48, loss is 0.08016563951969147\n",
      "epoch: 29 step: 68, loss is 0.5396062731742859\n",
      "epoch: 29 step: 88, loss is 0.66817307472229\n",
      "epoch: 29 step: 108, loss is 0.0770513117313385\n",
      "epoch: 29 step: 128, loss is 0.26249203085899353\n",
      "epoch: 29 step: 148, loss is 0.3425979018211365\n",
      "epoch: 29 step: 168, loss is 0.4053422808647156\n",
      "epoch: 29 step: 188, loss is 0.4065818190574646\n",
      "epoch: 29 step: 208, loss is 0.8436069488525391\n",
      "epoch: 29 step: 228, loss is 0.7077059745788574\n",
      "epoch: 29 step: 248, loss is 0.11558286845684052\n",
      "epoch: 29 step: 268, loss is 0.22742874920368195\n",
      "epoch: 29 step: 288, loss is 0.3336602449417114\n",
      "epoch: 29 step: 308, loss is 0.5759804248809814\n",
      "epoch: 29 step: 328, loss is 0.4559602737426758\n",
      "epoch: 29 step: 348, loss is 0.05328977108001709\n",
      "epoch: 29 step: 368, loss is 0.027320723980665207\n",
      "epoch: 29 step: 388, loss is 0.24968792498111725\n",
      "epoch: 29 step: 408, loss is 0.18831248581409454\n",
      "epoch: 29 step: 428, loss is 0.07740633189678192\n",
      "epoch: 29 step: 448, loss is 0.26540523767471313\n",
      "epoch: 29 step: 468, loss is 0.03828234225511551\n",
      "epoch: 29 step: 488, loss is 1.0457735061645508\n",
      "epoch: 29 step: 508, loss is 0.6490185260772705\n",
      "epoch: 29 step: 528, loss is 0.7926676869392395\n",
      "epoch: 29 step: 548, loss is 1.0535138845443726\n",
      "epoch: 29 step: 568, loss is 0.04029252380132675\n",
      "epoch: 29 step: 588, loss is 0.2857048511505127\n",
      "epoch: 29 step: 608, loss is 1.2475759983062744\n",
      "epoch: 29 step: 628, loss is 0.143353670835495\n",
      "epoch: 30 step: 4, loss is 0.4015801250934601\n",
      "epoch: 30 step: 24, loss is 0.016325559467077255\n",
      "epoch: 30 step: 44, loss is 0.16330544650554657\n",
      "epoch: 30 step: 64, loss is 0.06303137540817261\n",
      "epoch: 30 step: 84, loss is 0.338131844997406\n",
      "epoch: 30 step: 104, loss is 0.8414392471313477\n",
      "epoch: 30 step: 124, loss is 0.3527812361717224\n",
      "epoch: 30 step: 144, loss is 0.05532168969511986\n",
      "epoch: 30 step: 164, loss is 0.027330461889505386\n",
      "epoch: 30 step: 184, loss is 0.04661601781845093\n",
      "epoch: 30 step: 204, loss is 0.22454127669334412\n",
      "epoch: 30 step: 224, loss is 0.206223726272583\n",
      "epoch: 30 step: 244, loss is 0.3657858073711395\n",
      "epoch: 30 step: 264, loss is 0.09077397733926773\n",
      "epoch: 30 step: 284, loss is 0.2806132435798645\n",
      "epoch: 30 step: 304, loss is 1.1017011404037476\n",
      "epoch: 30 step: 324, loss is 0.14979493618011475\n",
      "epoch: 30 step: 344, loss is 0.6026135683059692\n",
      "epoch: 30 step: 364, loss is 0.15723347663879395\n",
      "epoch: 30 step: 384, loss is 0.2898576855659485\n",
      "epoch: 30 step: 404, loss is 0.07057607918977737\n",
      "epoch: 30 step: 424, loss is 0.284640908241272\n",
      "epoch: 30 step: 444, loss is 0.051339827477931976\n",
      "epoch: 30 step: 464, loss is 0.41112619638442993\n",
      "epoch: 30 step: 484, loss is 0.10671362280845642\n",
      "epoch: 30 step: 504, loss is 0.3147532641887665\n",
      "epoch: 30 step: 524, loss is 0.1662684977054596\n",
      "epoch: 30 step: 544, loss is 0.17984315752983093\n",
      "epoch: 30 step: 564, loss is 0.21031305193901062\n",
      "epoch: 30 step: 584, loss is 1.0544908046722412\n",
      "epoch: 30 step: 604, loss is 1.1263731718063354\n",
      "epoch: 30 step: 624, loss is 0.44869768619537354\n",
      "epoch: 30 step: 644, loss is 0.2658633887767792\n"
     ]
    }
   ],
   "source": [
    "trainer.train(30,train_loader,callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a83d24c-e603-4ec1-98f3-54f9ea7ee67f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T12:12:47.129722Z",
     "iopub.status.busy": "2024-06-26T12:12:47.129246Z",
     "iopub.status.idle": "2024-06-26T12:22:41.383942Z",
     "shell.execute_reply": "2024-06-26T12:22:41.382440Z",
     "shell.execute_reply.started": "2024-06-26T12:12:47.129679Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9483262188272298}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.load_param_into_net(model, ms.load_checkpoint(ckpt_cb.latest_ckpt_file_name))\n",
    "trainer.eval(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ms22]",
   "language": "python",
   "name": "conda-env-ms22-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
